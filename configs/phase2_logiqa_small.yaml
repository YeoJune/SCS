# configs/phas# íƒœìŠ¤í¬ ì„¤ì •
task:
  name: "LogiQA"
  type: "reasoning" # DataProcessorê°€ process_reasoningì„ í˜¸ì¶œí•˜ë„ë¡ ì§€ì‹œ
  dataset_name: "datatune/LogiQA2.0" # ğŸ”§ FIX: ì‘ë™í•˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€ê²½
  max_samples:
    train: 500 # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì œí•œ
    validation: 100
    test: 100_small.yaml
# Phase 2: LogiQA ì¶”ë¡  ëŠ¥ë ¥ ê²€ì¦ (Small-Scale)

# ê¸°ë³¸ ì„¤ì • ìƒì†
defaults:
  - base_model

# ì‹¤í—˜ ë©”íƒ€ì •ë³´
experiment:
  name: "Phase2_LogiQA_Small"
  description: "LogiQA ë°ì´í„°ì…‹ì„ í†µí•œ ê¸°ë³¸ ì¶”ë¡  ëŠ¥ë ¥ ê²€ì¦ (ì‘ì€ ê·œëª¨)"
  phase: 2
  expected_runtime: "30-60ë¶„"

# íƒœìŠ¤í¬ ì„¤ì •
task:
  name: "LogiQA"
  type: "reasoning" # DataProcessorê°€ process_reasoningì„ í˜¸ì¶œí•˜ë„ë¡ ì§€ì‹œ
  dataset_name: "datatune/LogiQA2.0" # Hugging Face ë°ì´í„°ì…‹ ID
  max_samples:
    train: 500 # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì œí•œ
    validation: 100
    test: 100

# ëª¨ë¸ ì„¤ì • (Small êµ¬ì„±ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ)
model:
  brain_regions:
    PFC:
      grid_height: 16
      grid_width: 8
      total_neurons: 128
      decay_rate: 0.95
      distance_tau: 1.5
    ACC:
      grid_height: 8
      grid_width: 8
      total_neurons: 64
      decay_rate: 0.88
      distance_tau: 2.0
    IPL:
      grid_height: 12
      grid_width: 8
      total_neurons: 96
      decay_rate: 0.92
      distance_tau: 2.5
    MTL:
      grid_height: 8
      grid_width: 8
      total_neurons: 64
      decay_rate: 0.97
      distance_tau: 3.0

# í•™ìŠµ ì„¤ì •
training:
  epochs: 20 # ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•´ ë‹¨ì¶•
  learning_rate: 5e-4
  batch_size: 4 # QA/ì¶”ë¡  íƒœìŠ¤í¬ëŠ” ì‘ì€ ë°°ì¹˜ê°€ ì•ˆì •ì 
  max_clk_training: 150 # ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ CLK ìˆ˜ë¥¼ ì•½ê°„ ëŠ˜ë ¤ì¤Œ

  # ìµœì í™” ì„¤ì •
  optimizer: "adamw"
  weight_decay: 0.01
  warmup_steps: 100

  # í‰ê°€ ì„¤ì •
  eval_every_n_epochs: 5
  save_every_n_epochs: 10
  early_stopping_patience: 5

# ë°ì´í„° ì„¤ì •
data:
  tokenizer:
    name: "bert-base-uncased"
    max_length: 256 # LogiQAëŠ” ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ ìˆ˜ ìˆìŒ
    padding: true
    truncation: true

  # ë°ì´í„° ì „ì²˜ë¦¬ ì„¤ì •
  preprocessing:
    lowercase: true
    remove_punctuation: false # ë…¼ë¦¬ ê´€ê³„ì— ì¤‘ìš”
    max_context_length: 200
    max_question_length: 50

# í‰ê°€ ë©”íŠ¸ë¦­
metrics:
  primary: "accuracy"
  additional: ["f1", "precision", "recall"]

# ì‹¤í—˜ ì¶”ì  ì„¤ì •
tracking:
  use_wandb: true
  wandb_project: "SCS_LogiQA"
  wandb_tags: ["phase2", "logiqa", "small", "reasoning"]

  # ì €ì¥í•  ë©”íŠ¸ë¦­
  log_metrics: ["loss", "accuracy", "f1"]
  log_frequency: 10 # ë§¤ 10 ìŠ¤í…ë§ˆë‹¤

# ë¶„ì„ ì„¤ì •
analysis:
  dynamics_analysis:
    enabled: true
    save_spike_patterns: true
    analyze_connectivity: true

  representation_analysis:
    enabled: false # Phase 2ì—ì„œëŠ” ë¹„í™œì„±í™”

  ablation_study:
    enabled: false # Phase 2ì—ì„œëŠ” ë¹„í™œì„±í™”
