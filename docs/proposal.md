# 연구 제안서: 의미론적 연산을 위한 뇌 모방 동적 컴퓨팅 아키텍처

## Abstract

기존 대규모 언어 모델(LLM)은 방대한 텍스트의 구문적 패턴 학습에는 탁월하지만, 동적인 의미론적 추론 과정 구현에는 근본적인 한계를 보인다. Dziri et al. (2023)의 연구는 트랜스포머가 다단계 추론을 진정한 의미론적 해석이 아닌 선형화된 서브그래프 매칭으로 수행함을 실증했다. 본 연구는 이러한 정적 처리 방식의 한계를 극복하기 위해, 지역적 및 원거리 신경 간섭의 동적 상호작용을 핵심 원리로 하는 새로운 스파이킹 인지 아키텍처(Spike-Based Cognitive System, SCS)를 제안한다.

SCS의 핵심 가설은 "정적인 가중치 행렬에 의존하는 트랜스포머와 달리, 시간에 따라 변화하는 SCS의 내적 상태와 스파이크 패턴 자체가 의미 정보를 인코딩하고 처리하는 동적인 연산자의 역할을 수행할 수 있다"는 것이다. 이를 검증하기 위해, 기초 논리 연산부터 복잡한 의미 추론까지 단계적 실험을 수행하고, Ablation Study를 통해 각 구조적 요소가 의미론적 연산에 기여하는 방식을 규명한다.

## 1. Introduction

### 1.1. 연구 배경 및 문제 제기

트랜스포머 기반 LLM의 성공에도 불구하고, 이들의 '추론' 능력은 본질적으로 정적이다. Dziri et al. (2023)의 연구는 이 한계를 명확히 드러냈다. LLM은 복잡해 보이는 다단계 추론 문제를 의미론적으로 이해해서 푸는 것이 아니라 선형화된 서브그래프 매칭이라는 패턴 인식 방식으로 해결한다. 이는 예측 불가능한 상황에 대한 유연성 부재와, 진정한 의미의 맥락 의존적 추론이 불가능함을 시사한다.

이러한 정적 패턴 매칭의 한계를 넘어서기 위해서는, 연산 과정 자체가 시간에 따라 동적으로 변화하며 입력의 의미에 따라 다른 처리 경로를 형성하는 아키텍처가 요구된다. 스파이킹 신경망의 시공간적 동역학이 이러한 동적 의미 처리를 위한 이상적인 기질을 제공한다고 제안한다.

### 1.2. 연구 가설

본 연구의 핵심 가설은 다음과 같다:

> **"단순한 스파이크 동역학으로 작동하는 네트워크는, (1) 각 모듈에 부여된 기능적 편향(inductive bias)과 (2) 태스크 기반 종단간 학습 신호에 의해 유도될 때, 중앙 통제 장치 없이도 상호작용을 통해 스스로를 조직화하여 복잡한 NLP 문제를 해결하는 데 필요한 연산 구조를 형성할 것이다."**

이 가설은 무작정 스스로 조직화된다는 순진한 주장이 아니라, 두 가지 핵심적인 제약 조건 아래에서 유용한 구조로 조직화된다는 정교하고 검증 가능한 과학적 가설이다.

### 1.3. 연구 질문

다음 두 가지 핵심 연구 질문에 답하고자 한다:

- **RQ1:** 지역적/원거리 간섭이라는 단순한 스파이크 동역학 원리가 복합적인 NLP 추론 능력을 창발시킬 수 있는가?
- **RQ2:** 특정 기능을 수행하도록 초기화된 모듈이 종단간 학습 과정 속에서 정말로 그 기능적 역할을 담당하도록 분화되는가?

## 2. Design Philosophy & Architecture

### 2.1. 설계 철학: 뉴런 간섭을 통한 동적 의미 연산

SCS의 설계는 세 가지 계층적 원리에 기반한다. 가장 근본적인 것은 시변하는 스파이크 패턴 자체가 연산자가 되는 **동적 컴퓨팅(원리 1)**이다. 이러한 동적 컴퓨팅은 뉴런 간의 **다중 스케일 간섭(원리 2)**을 통해 구체적인 연산을 수행한다. 마지막으로, 이 연산의 효율성과 목적성을 높이기 위해 **기능적 특화(원리 3)**라는 구조적 편향을 시스템에 부여한다.

#### 원리 1: 동적 컴퓨팅 (Dynamic Computing)

트랜스포머의 연산은 상태가 없는 순수 함수와 같아 동일 입력에 항상 동일 출력을 반환한다. 반면 SCS는 이전의 모든 계산 기록이 막전위 $V_i(t)$에 누적되어 있는 상태 의존적 시스템이다. 이는 의미론적 중의성 해소에 결정적이다.

각 뉴런은 막전위 $V_i(t)$, 이진 스파이크 출력 $s_i(t) \in \{0,1\}$, 휴지기 $R_i(t)$를 유지하며, 다음 식으로 업데이트된다:

$$V_i(t+1) = \lambda \cdot (V_i(t) + I_{ext,i}(t) + I_{internal,i}(t) + I_{axon,i}(t))$$

여기서 $I_{ext}$는 외부 입력 신호, $I_{internal}$은 같은 영역 내 뉴런들로부터의 신호, $I_{axon}$은 다른 영역으로부터의 축삭 신호이다.

$$s_i(t) = H(V_i(t) - \theta) \cdot \mathbb{1}[R_i(t) = 0]$$

#### 원리 2: 다중 스케일 간섭 (Multi-Scale Interference)

의미는 단일 뉴런이 아닌, 뉴런들 간의 상호작용 속에 존재한다. SCS는 두 가지 규모의 간섭을 통해 정적인 '개념'과 동적인 '관계'를 동시에 처리함으로써, 구성적 의미론을 구현한다.

**지역적 간섭: 표상 형성 및 안정화**

인접 뉴런들 사이의 상호작용은 입력 신호에 대한 안정적인 내부 표상을 형성한다:

$$W_{internal}(i,j) = w_0 \exp(-|i-j|/\tau) \quad \text{for } |i-j| \leq 5$$

**원거리 간섭: 관계 결속 및 추론**

원거리 간섭은 분리된 표상들을 동적으로 연결하여 관계를 형성한다:

$$I_{axon}^{(target)}(t) = (A^{source \to target})^T \cdot [E \odot s^{source}(t) - 0.5 \cdot (1-E) \odot s^{source}(t)]$$

**간섭의 창발적 특성**: 이 두 간섭의 동시 작동은 개별 구성요소의 합을 넘어서는 의미 처리 능력을 창발시킨다. 구체적으로, 이는 **개념 표상과 관계 결속 과정의 상호적이고 동시적인 재조정**을 의미한다. 예를 들어, '사과'라는 개념 표상은 '빨갛다'는 속성과 관계를 맺는 과정에서 더욱 명확해지고, 동시에 '빨갛다'는 표상 또한 '사과'와 결속되면서 그 의미가 구체화된다. 이러한 **지속적인 피드백 루프**는 각 층의 연산이 단방향으로 끝나는 기존 딥러닝 아키텍처에서는 나타나기 어려운 연산 방식이다.

#### 원리 3: 기능적 특화 (Functional Specialization)

무작위 네트워크가 아닌, 의미 처리에 필요한 기능적 편향을 가진 모듈 구조가 효율적이다. 각 모듈은 고유한 동역학적 특성으로 초기화된다.

| 모듈 (가설적 역할) | Decay Rate (λ) | Distance Tau (τ) | 초기 동역학 특성        |
| :----------------- | :------------- | :--------------- | :---------------------- |
| PFC-inspired       | 0.95           | 1.5              | 긴 지속성 (작업 기억)   |
| ACC-inspired       | 0.88           | 2.0              | 빠른 반응 (갈등 감지)   |
| IPL-inspired       | 0.92           | 2.5              | 중간 지속성 (관계 연합) |
| MTL-inspired       | 0.97           | 3.0              | 장기 기억 (의미 저장)   |

**중요한 점은 이것이 '설계 가설'이라는 것이다.** 이 기능 분화가 실제로 학습을 통해 창발하는지 여부는 실험적 검증이 필요한 핵심 연구 질문이다.

### 2.2. 시스템 아키텍처 및 동작

#### 전역 동기화 및 분산 처리

SCS의 핵심은 CLK 신호에 맞춰 모든 뉴런이 동시에 상태를 업데이트한다는 점이다. 이 전역적 동기화는 전체 네트워크 상태의 일관성을 매 시간 단계마다 보장하며, 모든 뉴런이 동일한 시간적 맥락 아래에서 상호작용할 수 있게 한다.

#### 정보 처리의 흐름

1. **인코딩**: Input Node가 어텐션을 통해 토큰을 다수 뉴런의 동시 활성 스파이크 패턴으로 변환한다.
2. **분산 처리**: 지역적/원거리 간섭이 일어나면서 네트워크는 스스로 안정적인 해석을 찾아간다.
3. **적응적 출력**: 시스템 스스로 수렴을 판단하고 출력을 생성한다.
4. **디코딩**: Output Node가 최종 안정화된 스파이크 패턴을 토큰 확률 분포로 변환한다.

#### 계층적 학습 전략

복잡한 동역학 시스템의 학습은 안정성(안정적인 지식 유지)과 가소성(새로운 지식 학습) 사이의 균형을 요구한다. SCS는 이 문제를 해결하기 위해, 각기 다른 시공간적 스케일에서 작동하는 계층적 학습 메커니즘을 채택한다:

- **입출력 노드**: Backpropagation (시스템 전체의 안정적인 수렴 보장)
- **내부 연결**: Surrogate gradient 기반 학습
- **축삭 연결**: K-hop 제한 backpropagation + 신경조절 피드백 (국소적이고 맥락 의존적인 가소성 제공)

**K-hop 제한 편미분 기반 신경조절**:

$$\frac{\partial L}{\partial s_i} = \sum_{j \in \text{children}_2(i)} A_{ij} \cdot \frac{\partial L}{\partial s_j}$$

**Trace-based STDP (선택적 활성화)**: 기본적으로 비활성화되어 있으며 실험적 목적으로만 활성화 가능하다.

도파민 신호: $D_i(t) = \tanh\left(2.0 \cdot \frac{\partial L}{\partial s_i} \cdot \Delta s_i\right)$

아세틸콜린 신호: $\text{ACh}_i(t) = \sigma\left(3.0 \cdot \left|\frac{\partial L}{\partial s_i}\right|\right)$

## 3. Rationale and Related Works

### 3.1. 동적 신경망과 동적 컴퓨팅 원리

동적 신경망은 입력에 따라 구조나 연산을 적응적으로 조절하는 신경망으로, 계산 효율성 향상을 주목적으로 발전해왔다. Zhou et al. (2023)의 Spikformer는 스파이킹 자기주의 메커니즘을 통해 기존 트랜스포머의 softmax 연산을 제거하고 계산 복잡도를 감소시켰다. 이러한 접근법들은 생체 모방 동역학의 계산적 유효성을 입증하지만, 주로 계산 자원 최적화에 국한되어 있다.

우리의 동적 컴퓨팅 원리는 단순한 효율성 향상을 넘어 동적 구조 변화를 통해 의미론적 처리 능력 자체를 향상시키고자 하는 점에서 차별화된다.

### 3.2. 적응적 추론 구조와 다중 스케일 간섭

Zhou et al. (2024)의 SELF-DISCOVER 프레임워크는 LLM이 39개 원자적 추론 모듈에서 태스크별로 적절한 모듈을 선택, 적응, 구현하는 3단계 과정을 통해 추론 구조를 동적으로 구성한다. 25개 추론 태스크에서 Chain-of-Thought 대비 평균 20-32% 성능 향상과 10-40배 계산량 감소를 달성했다.

SELF-DISCOVER의 핵심 기여는 태스크별 최적 추론 구조의 존재를 실증적으로 입증한 것이다. 이는 우리의 다중 스케일 간섭 원리와 기본 가정을 공유한다. 그러나 SELF-DISCOVER는 태스크 수준에서의 일회성 구조 결정에 머무른다. 우리는 이를 확장하여 추론 과정 중 지속적인 구조 적응을 가능하게 한다.

### 3.3. 신경생물학적 근거와 기능적 특화

Benisty et al. (2024)의 연구는 우리의 기능적 특화 원리에 대한 강력한 신경생물학적 근거를 제공한다. wide-field mesoscopic calcium imaging을 사용하여 마우스 대뇌피질 전체의 자발적 행동 중 신경 활동을 관찰한 결과, 기능적 연결성의 빠른 변화(서브초 단위)가 행동을 인코딩한다는 것을 발견했다. 구체적으로, 인접 뉴런 간 상관관계(지역적)와 뇌 영역 간 상관관계(원거리) 모두가 행동 정보를 담고 있으며, 이러한 동적 연결성 변화가 정적 활성도 패턴보다 행동 예측에 더 유효하다는 점을 확인했다.

이는 우리 SCS의 핵심 설계 원리를 직접적으로 뒷받침한다. SCS의 지역적 간섭과 원거리 간섭이 실제 뇌에서 관찰되는 메커니즘과 일치함을 의미한다.

### 3.4. 그래프 신경망과의 관계

SCS는 동적으로 변화하는 그래프 구조 위에서 정보를 처리한다는 점에서 GNN, 특히 동적 GNN과 개념적 유사성을 공유한다. 그러나 결정적인 차이가 있다. GNN은 일반적으로 사전에 정의되거나 명시적으로 추론된 그래프 구조 위에서 메시지 패싱을 수행한다. 반면, SCS는 **어떠한 명시적인 그래프 구조도 없이, 오직 뉴런 간의 시공간적 스파이크 상호작용 그 자체를 통해 기능적 연결성 그래프가 실시간으로 창발한다.** 이는 사전 정의된 관계에 의존하지 않는, 보다 유연하고 근본적인 정보 처리 방식을 가능하게 한다.

## 4. Validation Plan

우리의 핵심 가설을 체계적으로 검증하고 SCS의 능력을 입증하기 위해, 기초 연산 능력의 증명부터 고차원적 논리 추론까지 포괄하는 3단계 검증 계획을 제안한다.

### 4.1. Phase 1: Foundational Capability Verification

**목표**: 제안된 스파이크 동역학이 원리적으로 기본적인 논리 및 순차 연산을 구현할 수 있는지 증명한다.

**모델**: 핵심 동역학을 유지하되, 노드 및 모듈 수를 축소한 경량 SCS 모델.

**태스크**:

1. Logical Operations: XOR, AND 등 기초 논리 게이트 기능의 학습 및 재현
2. Sequential Operations: Sequence Copying/Reversal 등 시간적 순서 정보의 단기 기억 및 처리

**성공 기준**: 목표 출력을 높은 정확도로 재현하여, SCS 동역학이 기본적인 비선형 연산 및 순차 처리를 위한 충분한 표현력을 가짐을 입증한다.

### 4.2. Phase 2: Core Semantic Reasoning Validation

**목표**: 완전한 SCS 아키텍처가 우리의 설계 철학에 명시된 핵심 의미론적 연산을 효과적으로 수행하는지 평가한다.

**모델**: 4개 모듈 기반의 완전한 SCS 아키텍처.

**태스크**:

1. Relational Binding (PIQA, SocialIQA): 개념 간의 암묵적 관계를 추론하는 능력 검증
2. Compositional Reasoning (CLUTRR, ProofWriter): 여러 의미 단위를 논리적으로 조합하여 결론을 도출하는 능력 검증
3. Conflict Resolution & Integration (HotpotQA): 불확실하거나 상충하는 정보 속에서 일관된 해석을 찾아내는 능력 검증

**평가**: Transformer 및 SNN 베이스라인과의 정량적 성능 비교를 통해 SCS의 효과성(RQ1)을 평가한다.

### 4.3. Phase 3: High-Level Reasoning via Pre-training and Fine-tuning

**목표**: SCS의 확장성과, 대규모 데이터로부터 일반적인 세계 지식을 학습한 후 복잡하고 새로운 문제에 적응하는 능력을 평가한다.

**방법론**: Phase 2의 모델을 대규모 텍스트 코퍼스에 사전 학습시킨 후, 고차원적이고 다단계 추론을 요구하는 벤치마크에 미세 조정한다.

**태스크**:

1. Multi-hop Logical Reasoning (StrategyQA)
2. Mathematical & Algorithmic Reasoning (AQuA-RAT, GSM8K)

### 4.4. 가설 기반 분석 및 잠재적 결과 해석

**가설 기반 분석**: 우리의 분석은 사전에 설정된 가설을 검증하는 데 초점을 맞출 것이다. 예를 들어, 우리는 PFC 모듈 제거가 Task 2의 성능을 다른 모듈 제거 시보다 유의미하게 더 저하시킬 것이라 예측한다.

**잠재적 결과 해석**: 우리는 **가설과 모순되는 결과 또한 중요한 과학적 발견으로 간주할 것이다.** 예를 들어, 예상과 다른 모듈이 특정 기능에 결정적인 역할을 하는 것으로 밝혀진다면, 이는 제안된 아키텍처 내에서 뇌와는 다른 방식으로 기능 분화가 일어남을 시사하는 결과가 될 것이다. 이는 우리의 초기 '설계 가설'을 데이터 기반으로 수정하고, 지능의 구현에 대한 새로운 통찰을 제공할 기회이다.

**분석 방법**: Ablation Study를 통해 각 모듈을 비활성화했을 때, 의도했던 기능과 관련된 태스크에서만 선택적인 성능 저하가 나타나는지를 확인함으로써, 모듈이 성공적으로 분화되었는지 검증한다(RQ2). Transfer Entropy와 표상 유사도 분석을 도입하여, 모델의 내적 동역학과 표상 공간을 심층 분석하고 해석 가능성을 확보할 것이다.

## 5. Conclusion, Limitations, and Future Work

본 연구는 정적 패턴 매칭의 한계를 넘어, 동적 의미론적 연산을 위한 새로운 인지 아키텍처 패러다임을 제시한다. SCS는 생물학적으로 타당한 원리를 바탕으로, 분산된 모듈 간의 상호작용을 통해 복잡한 추론이 창발하는 과정을 모델링한다.

본 연구의 한계는 명확하다. 4개 모듈의 구성은 가설에 기반한 최소 단위이며, 대규모 확장 시의 계산 복잡성 문제도 중요한 도전 과제이다. 또한, 제안된 학습 메커니즘의 수렴성과 안정성에 대한 이론적 보장이 부족하다.

향후 연구는 이러한 한계를 극복하는 데 초점을 맞출 것이다. 태스크에 따라 모듈의 수와 연결성을 동적으로 조절하는 메타 학습 접근법을 개발하고, 분산 학습 및 뉴로모픽 하드웨어 최적화를 통해 대규모 스케일링 문제를 해결하는 방향으로 나아가고자 한다.

## References

Benisty, H., Barson, D., Moberly, A. H., Lohani, S., Tang, L., Coifman, R. R., Crair, M. C., Mishne, G., Cardin, J. A., & Higley, M. J. (2024). Rapid fluctuations in functional connectivity of cortical networks encode spontaneous behavior. _Nature Neuroscience_, 27(1), 148-158.

Dziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., & Choi, Y. (2023). Faith and fate: Limits of transformers on compositionality. _Advances in Neural Information Processing Systems_, 36.

Huang, G. (2024). Dynamic neural networks: Advantages and challenges. _National Science Review_, 11(8), nwae088.

Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le, Q. V., Chi, E. H., Zhou, D., Mishra, S., & Zheng, H. S. (2024). SELF-DISCOVER: Large language models self-compose reasoning structures. _Advances in Neural Information Processing Systems_, 37.

Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., & Yuan, L. (2023). Spikformer: When spiking neural network meets transformer. _International Conference on Learning Representations_.
