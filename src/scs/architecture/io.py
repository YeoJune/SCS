# src/scs/architecture/io.py
"""
ì…ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ v5.0 (T5 ìŠ¤ì¼€ì¼ ì •í•©ì„± ë³´ì •)
ì…ë ¥: í† í° ìœˆë„ìš° â†’ ë‹¨ì¼ ë¬¸ë§¥ ë²¡í„° â†’ ê³µê°„ ë¶„ì‚°
ì¶œë ¥: ê³µê°„ ì§‘ì¤‘ â†’ ë‹¨ì¼ íˆë“  ë²¡í„° â†’ CLK ìœˆë„ìš° ëˆ„ì 

ì£¼ìš” ë³€ê²½ì‚¬í•­:
1. InputInterface: T5 ì •ê·œí™” ìˆœì„œ ë§ì¶¤ (norm_first=True, ì‚¬ì „ ì •ê·œí™” ì œê±°)
2. OutputInterface: compressor_power ì´ˆê¸°ê°’ ì¶•ì†Œ (T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer
from transformers import T5ForConditionalGeneration
from typing import Optional, Tuple, Dict, Any, List
import math
import warnings


class RMSNorm(nn.Module):
    """RMS Normalization (T5 ìŠ¤íƒ€ì¼)"""
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(dim))
        self.eps = eps
    
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)
        return self.scale * x / rms


def load_t5_embeddings(model_name: str = "t5-base"):
    """T5 ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„ë² ë”© ë¡œë“œ"""
    print(f"Loading T5 embeddings from {model_name}...")
    t5_model = T5ForConditionalGeneration.from_pretrained(model_name)
    t5_config = t5_model.config
    
    return {
        'token_embedding_weights': t5_model.shared.weight.data.clone(),
        'lm_head_weights': t5_model.lm_head.weight.data.clone() if hasattr(t5_model, 'lm_head') else t5_model.shared.weight.data.clone(),
        'vocab_size': t5_config.vocab_size,
        'd_model': t5_config.d_model,
        'model_name': model_name,
        'full_model': t5_model
    }


def transplant_encoder_layer(scs_layer, t5_layer):
    """T5 encoder ë ˆì´ì–´ë¥¼ PyTorch TransformerEncoderLayerë¡œ ì´ì‹"""
    t5_self_attn = t5_layer.layer[0].SelfAttention
    t5_ff = t5_layer.layer[1].DenseReluDense
    
    with torch.no_grad():
        # Self-Attention weights
        q_weight = t5_self_attn.q.weight.data
        k_weight = t5_self_attn.k.weight.data  
        v_weight = t5_self_attn.v.weight.data
        in_proj_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)
        
        scs_layer.self_attn.in_proj_weight.copy_(in_proj_weight)
        scs_layer.self_attn.out_proj.weight.copy_(t5_self_attn.o.weight.data)
        
        # LayerNorms
        scs_layer.norm1.weight.copy_(t5_layer.layer[0].layer_norm.weight.data)
        scs_layer.norm2.weight.copy_(t5_layer.layer[1].layer_norm.weight.data)
        
        # Feed Forward
        scs_layer.linear1.weight.copy_(t5_ff.wi.weight.data)
        scs_layer.linear2.weight.copy_(t5_ff.wo.weight.data)


def transplant_decoder_layer(scs_layer, t5_layer, include_cross_attention=False):
    """T5 decoder ë ˆì´ì–´ë¥¼ PyTorch TransformerDecoderLayerë¡œ ì´ì‹"""
    t5_self_attn = t5_layer.layer[0].SelfAttention
    t5_cross_attn = t5_layer.layer[1].EncDecAttention
    t5_ff = t5_layer.layer[2].DenseReluDense
    
    with torch.no_grad():
        # Self-Attention
        q_weight = t5_self_attn.q.weight.data
        k_weight = t5_self_attn.k.weight.data  
        v_weight = t5_self_attn.v.weight.data
        in_proj_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)
        
        scs_layer.self_attn.in_proj_weight.copy_(in_proj_weight)
        scs_layer.self_attn.out_proj.weight.copy_(t5_self_attn.o.weight.data)
        scs_layer.norm1.weight.copy_(t5_layer.layer[0].layer_norm.weight.data)
        
        # Cross-Attention (optional)
        if include_cross_attention:
            q_weight = t5_cross_attn.q.weight.data
            k_weight = t5_cross_attn.k.weight.data
            v_weight = t5_cross_attn.v.weight.data
            in_proj_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)
            
            scs_layer.multihead_attn.in_proj_weight.copy_(in_proj_weight)
            scs_layer.multihead_attn.out_proj.weight.copy_(t5_cross_attn.o.weight.data)
            scs_layer.norm2.weight.copy_(t5_layer.layer[1].layer_norm.weight.data)
        
        # Feed Forward
        scs_layer.linear1.weight.copy_(t5_ff.wi.weight.data)
        scs_layer.linear2.weight.copy_(t5_ff.wo.weight.data)
        scs_layer.norm3.weight.copy_(t5_layer.layer[2].layer_norm.weight.data)


class InputInterface(nn.Module):
    """
    ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ v5.0: T5 ì •ê·œí™” ìˆœì„œ ë§ì¶¤
    
    ì£¼ìš” ë³€ê²½ì‚¬í•­:
    - norm_first=Trueë¡œ T5 ë¸”ë¡ ìˆœì„œì™€ ì¼ì¹˜ (Input â†’ Norm â†’ Attention)
    - ì‚¬ì „ ì •ê·œí™”(layer_norm) ì œê±°í•˜ì—¬ T5 ê°€ì¤‘ì¹˜ê°€ ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¼ ì…ë ¥ ë°›ë„ë¡ í•¨
    """
    
    def __init__(
        self,
        vocab_size: int,
        grid_height: int,
        grid_width: int,
        embedding_dim: int = 512,
        window_size: int = 31,
        encoder_layers: int = 6,
        encoder_heads: int = 8,
        encoder_dropout: float = 0.1,
        dim_feedforward: int = 2048,
        input_power: float = 0.5,
        softmax_temperature: float = 1.0,
        use_positional_encoding: bool = True,
        t5_model_name: Optional[str] = None,
        device: str = "cuda"
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.grid_height = grid_height
        self.grid_width = grid_width
        self.embedding_dim = embedding_dim
        self.window_size = window_size
        self.input_power = input_power
        self.softmax_temperature = softmax_temperature
        self.use_positional_encoding = use_positional_encoding
        self.device = device
        
        # T5 ì„ë² ë”© ë¡œë“œ ë° ì´ˆê¸°í™”
        if t5_model_name is not None:
            t5_data = load_t5_embeddings(t5_model_name)
            self.vocab_size = t5_data['vocab_size']
            self.embedding_dim = t5_data['d_model']
            
            self.token_embedding = nn.Embedding.from_pretrained(
                t5_data['token_embedding_weights'], 
                freeze=False
            )
            print(f"Loaded T5 token embedding: {self.token_embedding.weight.shape}")
        else:
            self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # [CLS] í† í°
        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim) * 0.02)
        
        # ìœ„ì¹˜ ì„ë² ë”©
        if self.use_positional_encoding:
            self.position_embedding = nn.Embedding(window_size + 1, self.embedding_dim)
        
        # Transformer Encoder (T5 ìˆœì„œ ë§ì¶¤)
        encoder_layer = TransformerEncoderLayer(
            d_model=self.embedding_dim,
            nhead=encoder_heads,
            dim_feedforward=dim_feedforward,
            dropout=encoder_dropout,
            norm_first=True,  # ğŸ”¥ T5ì™€ ë™ì¼í•œ ìˆœì„œ: Input â†’ Norm â†’ Attention
            batch_first=True
        )
        self.transformer_encoder = TransformerEncoder(
            encoder_layer,
            num_layers=encoder_layers
        )
        
        # T5 Encoder ì´ì‹
        if t5_model_name is not None:
            self._transplant_t5_encoder(t5_data['full_model'])
        
        # Linear ë§¤í•‘ ë ˆì´ì–´
        self.pattern_mapper = nn.Linear(
            self.embedding_dim,
            self.grid_height * self.grid_width
        )
        self._initialize_mapper()
        
        # Dropout (T5 ìŠ¤íƒ€ì¼, ì •ê·œí™” ëŒ€ì‹  ì‚¬ìš©)
        self.dropout = nn.Dropout(encoder_dropout)
    
    def _transplant_t5_encoder(self, t5_model):
        """T5 encoder ê°€ì¤‘ì¹˜ ì´ì‹"""
        try:
            t5_encoder = t5_model.encoder
            min_layers = min(len(self.transformer_encoder.layers), len(t5_encoder.block))
            
            for i in range(min_layers):
                transplant_encoder_layer(
                    self.transformer_encoder.layers[i],
                    t5_encoder.block[i]
                )
            
            print(f"Transplanted {min_layers} encoder layers from T5")
            
        except Exception as e:
            print(f"Failed to transplant T5 encoder: {e}")
            warnings.warn(f"T5 encoder transplant failed: {e}")
    
    def _initialize_mapper(self):
        """íŒ¨í„´ ë§¤í•‘ ë ˆì´ì–´ ì§êµ ì´ˆê¸°í™”"""
        torch.nn.init.orthogonal_(self.pattern_mapper.weight)
        if self.pattern_mapper.bias is not None:
            torch.nn.init.constant_(self.pattern_mapper.bias, 0.0)
        
    def forward(self, token_window: torch.Tensor) -> torch.Tensor:
        """
        ìœˆë„ìš° ê¸°ë°˜ ì¼ê´„ ì²˜ë¦¬ (T5 ìŠ¤ì¼€ì¼ ë§ì¶¤)
        
        Args:
            token_window: [B, window_size] í† í° ìœˆë„ìš°
            
        Returns:
            membrane_pattern: [B, H, W] ë§‰ì „ìœ„ íŒ¨í„´
        """
        if token_window is None or token_window.numel() == 0:
            return None
            
        batch_size, seq_len = token_window.shape
        
        # í† í° ì„ë² ë”©
        token_embeds = self.token_embedding(token_window)
        
        # [CLS] í† í° ì¶”ê°€
        cls_tokens = self.cls_token.expand(batch_size, 1, -1)
        windowed_input = torch.cat([cls_tokens, token_embeds], dim=1)
        
        # ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        if self.use_positional_encoding:
            positions = torch.arange(seq_len + 1, device=self.device).unsqueeze(0).expand(batch_size, -1)
            position_embeds = self.position_embedding(positions)
            windowed_input = windowed_input + position_embeds
        
        # Dropout ì ìš© (T5 ìŠ¤íƒ€ì¼)
        windowed_input = self.dropout(windowed_input)
        
        # ğŸ”¥ ì‚¬ì „ ì •ê·œí™” ì œê±°! T5 encoderê°€ ë‚´ë¶€ì—ì„œ ì •ê·œí™” ìˆ˜í–‰
        # windowed_input = self.layer_norm(windowed_input)  # â† ì´ ë¼ì¸ ì œê±°ë¨
        
        # Transformer Encoder (T5ì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ì˜ ì…ë ¥)
        encoder_output = self.transformer_encoder(windowed_input)
        context_vector = encoder_output[:, 0, :]  # [CLS] í† í°
        
        # Linear ë§¤í•‘ ë° Softmax
        membrane_logits = self.pattern_mapper(context_vector)
        pattern_probs = F.softmax(membrane_logits / self.softmax_temperature, dim=-1)
        
        # Scaling
        total_energy = self.grid_height * self.grid_width * self.input_power
        scaled_pattern = pattern_probs * total_energy
        
        # 2D ê·¸ë¦¬ë“œë¡œ ë³€í™˜
        membrane_pattern = scaled_pattern.view(
            batch_size, self.grid_height, self.grid_width
        )
        
        return membrane_pattern


class OutputInterface(nn.Module):
    """
    ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤ v5.0: T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤
    
    ì£¼ìš” ë³€ê²½ì‚¬í•­:
    - compressor_power ì´ˆê¸°ê°’ì„ 0.1ë¡œ ì¶•ì†Œ (T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ê³¼ ë§ì¶¤)
    - ìì—°ìŠ¤ëŸ¬ìš´ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ìœ ë„ (ì–¸ì–´ëª¨ë¸ë§ â†’ SNN ì •ë³´ í™œìš©)
    """
    
    def __init__(
        self,
        vocab_size: int,
        grid_height: int,
        grid_width: int,
        pad_token_id: int,
        embedding_dim: int = 512,
        window_size: int = 31,
        decoder_layers: int = 6,
        decoder_heads: int = 8,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        use_positional_encoding: bool = True,
        use_clk_position_encoding: bool = True,
        t5_model_name: Optional[str] = None,
        transplant_cross_attention: bool = False,
        device: str = "cuda"
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.grid_height = grid_height
        self.grid_width = grid_width
        self.pad_token_id = pad_token_id
        self.embedding_dim = embedding_dim
        self.window_size = window_size
        self.use_positional_encoding = use_positional_encoding
        self.use_clk_position_encoding = use_clk_position_encoding
        self.transplant_cross_attention = transplant_cross_attention
        self.device = device
        
        # T5 ì„ë² ë”© ë¡œë“œ ë° ì´ˆê¸°í™”
        if t5_model_name is not None:
            t5_data = load_t5_embeddings(t5_model_name)
            self.vocab_size = t5_data['vocab_size']
            self.embedding_dim = t5_data['d_model']
            
            self.token_embedding = nn.Embedding.from_pretrained(
                t5_data['token_embedding_weights'], 
                freeze=False,
                padding_idx=self.pad_token_id
            )
            print(f"Loaded T5 token embedding: {self.token_embedding.weight.shape}")
        else:
            self.token_embedding = nn.Embedding(
                vocab_size, 
                embedding_dim, 
                padding_idx=self.pad_token_id
            )
        
        # Linear ê³µê°„ ì••ì¶•
        self.spatial_compressor = nn.Linear(
            self.grid_height * self.grid_width, 
            self.embedding_dim
        )
        # ğŸ”¥ T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤: 0.1ë¡œ ì´ˆê¸°ê°’ ì¶•ì†Œ
        # T5 Encoder Final LayerNorm: mean=0.24, std=0.079
        # ì´ ì‘ì€ ìŠ¤ì¼€ì¼ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì§„ì  í•™ìŠµ ìœ ë„
        self.compressor_power = nn.Parameter(torch.tensor(0.1, dtype=torch.float32), requires_grad=True)
        self._initialize_compressor()
        
        # ìœ„ì¹˜ ì„ë² ë”©
        if self.use_clk_position_encoding:
            self.clk_position_embedding = nn.Embedding(window_size, self.embedding_dim)
        
        if self.use_positional_encoding:
            self.position_embedding = nn.Embedding(window_size, self.embedding_dim)
        
        # Transformer Decoder
        decoder_layer = TransformerDecoderLayer(
            d_model=self.embedding_dim,
            nhead=decoder_heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_decoder = TransformerDecoder(
            decoder_layer,
            num_layers=decoder_layers
        )
        
        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´
        self.final_projection = nn.Linear(self.embedding_dim, self.vocab_size)
        
        # T5 ê°€ì¤‘ì¹˜ ì´ì‹
        if t5_model_name is not None:
            self._transplant_t5_decoder(t5_data['full_model'])
            with torch.no_grad():
                self.final_projection.weight.copy_(t5_data['lm_head_weights'])
            print(f"Loaded T5 LM head: {self.final_projection.weight.shape}")
        
        self.layer_norm = RMSNorm(self.embedding_dim)
    
    def _transplant_t5_decoder(self, t5_model):
        """T5 decoder ê°€ì¤‘ì¹˜ ì´ì‹"""
        try:
            t5_decoder = t5_model.decoder
            min_layers = min(len(self.transformer_decoder.layers), len(t5_decoder.block))
            
            for i in range(min_layers):
                transplant_decoder_layer(
                    self.transformer_decoder.layers[i],
                    t5_decoder.block[i],
                    include_cross_attention=self.transplant_cross_attention
                )
            
            cross_status = "with" if self.transplant_cross_attention else "without"
            print(f"Transplanted {min_layers} decoder layers from T5 ({cross_status} cross-attention)")
            
        except Exception as e:
            print(f"Failed to transplant T5 decoder: {e}")
            warnings.warn(f"T5 decoder transplant failed: {e}")
    
    def _initialize_compressor(self):
        """ê³µê°„ ì••ì¶• ë ˆì´ì–´ ì§êµ ì´ˆê¸°í™”"""
        torch.nn.init.orthogonal_(self.spatial_compressor.weight)
        if self.spatial_compressor.bias is not None:
            torch.nn.init.constant_(self.spatial_compressor.bias, 0.0)
    
    def forward(
        self,
        grid_spikes: torch.Tensor,
        decoder_input_ids: torch.Tensor,
        hidden_states_history: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìŠ¤íŒŒì´í¬ ê²©ìì™€ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œë¶€í„° ë¡œì§“ ìƒì„± (T5 ìŠ¤ì¼€ì¼ ë§ì¶¤)
        
        Args:
            grid_spikes: [B, H, W] ìŠ¤íŒŒì´í¬ ê·¸ë¦¬ë“œ
            decoder_input_ids: [B, seq_len] ë””ì½”ë” ì…ë ¥ í† í°ë“¤
            hidden_states_history: [B, clk_count, D] ì´ì „ CLKë“¤ì˜ íˆë“  ë²¡í„°ë“¤
            
        Returns:
            output_logits: [B, window_len, vocab_size] ì¶œë ¥ ë¡œì§“
            updated_history: [B, new_clk_count, D] ì—…ë°ì´íŠ¸ëœ íˆë“  ë²¡í„° íˆìŠ¤í† ë¦¬
        """
        if grid_spikes.dim() == 2:
            grid_spikes = grid_spikes.unsqueeze(0)
        
        if decoder_input_ids.dim() == 1:
            decoder_input_ids = decoder_input_ids.unsqueeze(0)
        
        batch_size = decoder_input_ids.shape[0]
        
        # ìœˆë„ìš° í¬ê¸°ë¡œ ì œí•œ
        if decoder_input_ids.shape[1] > self.window_size:
            decoder_window = decoder_input_ids[:, -self.window_size:]
        else:
            decoder_window = decoder_input_ids
        
        window_len = decoder_window.shape[1]
        
        # í˜„ì¬ CLKì˜ íˆë“  ë²¡í„° ìƒì„± (T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤)
        current_hidden_vector = self._create_current_hidden_vector(grid_spikes)
        
        # CLK íˆë“  ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        memory_sequence = self._update_hidden_history(
            hidden_states_history, 
            current_hidden_vector
        )
        
        # ë””ì½”ë” ì…ë ¥ ì„ë² ë”©
        target_embeds = self._prepare_target_embeddings(decoder_window)
        
        # Causal mask ìƒì„±
        tgt_mask = self._generate_causal_mask(window_len)
        
        # Transformer ë””ì½”ë” ì‹¤í–‰
        decoder_output = self.transformer_decoder(
            tgt=target_embeds,
            memory=memory_sequence,
            tgt_mask=tgt_mask
        )
        
        # ìµœì¢… ë¡œì§“ ê³„ì‚°
        output_logits = self.final_projection(decoder_output)
        
        return output_logits, memory_sequence
    
    def _create_current_hidden_vector(self, grid_spikes: torch.Tensor) -> torch.Tensor:
        """
        ìŠ¤íŒŒì´í¬ ê²©ìë¥¼ ë‹¨ì¼ íˆë“  ë²¡í„°ë¡œ ì••ì¶• (T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤)
        
        ë³€ê²½ì‚¬í•­:
        - compressor_power ì´ˆê¸°ê°’ 0.1 â†’ T5 ì¸ì½”ë” ì¶œë ¥ê³¼ ìœ ì‚¬í•œ ìŠ¤ì¼€ì¼ (stdâ‰ˆ0.08)
        - ì ì§„ì  í•™ìŠµ: ì–¸ì–´ëª¨ë¸ë§ë¶€í„° ì‹œì‘ â†’ SNN ì •ë³´ ì ì°¨ í™œìš©
        """
        batch_size = grid_spikes.shape[0]
        
        # ìŠ¤íŒŒì´í¬ ê°’ í‰íƒ„í™”
        spikes_input = grid_spikes.view(batch_size, -1)
        
        # Linear ì••ì¶• (ì§êµ ì´ˆê¸°í™”ë¡œ ë¶„ì‚° ë³´ì¡´)
        hidden_vector = self.spatial_compressor(spikes_input)
        
        # ì •ê·œí™” (std=1.0)
        hidden_vector = self.layer_norm(hidden_vector)
        
        # T5 ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ ë§ì¶¤ (ì´ˆê¸°ê°’ 0.1 â†’ stdâ‰ˆ0.1, T5ì˜ stdâ‰ˆ0.08ê³¼ ìœ ì‚¬)
        return hidden_vector * self.compressor_power
    
    def _update_hidden_history(
        self, 
        hidden_states_history: Optional[torch.Tensor], 
        current_hidden_vector: torch.Tensor
    ) -> torch.Tensor:
        """CLK íˆë“  ë²¡í„° íˆìŠ¤í† ë¦¬ë¥¼ ìœˆë„ìš° í¬ê¸°ë¡œ ì œí•œí•˜ì—¬ ì—…ë°ì´íŠ¸"""
        
        # í˜„ì¬ íˆë“  ë²¡í„°ë¥¼ ì‹œí€€ìŠ¤ ì°¨ì› ì¶”ê°€
        current_hidden_seq = current_hidden_vector.unsqueeze(1)
        
        if hidden_states_history is None:
            memory_sequence = current_hidden_seq
        else:
            # ìœˆë„ìš° í¬ê¸° ì œí•œ
            if hidden_states_history.shape[1] >= self.window_size:
                trimmed_history = hidden_states_history[:, -(self.window_size-1):]
                memory_sequence = torch.cat([trimmed_history, current_hidden_seq], dim=1)
            else:
                memory_sequence = torch.cat([hidden_states_history, current_hidden_seq], dim=1)
        
        # CLK ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        if self.use_clk_position_encoding:
            batch_size, clk_count, embed_dim = memory_sequence.shape
            clk_positions = torch.arange(clk_count, device=self.device).unsqueeze(0).expand(batch_size, -1)
            clk_embeds = self.clk_position_embedding(clk_positions)
            memory_sequence = memory_sequence + clk_embeds
        
        return memory_sequence
    
    def _prepare_target_embeddings(self, decoder_window: torch.Tensor) -> torch.Tensor:
        """ë””ì½”ë” ìœˆë„ìš° í† í°ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (T5 ìŠ¤íƒ€ì¼ ìœ ì§€)"""
        batch_size, window_len = decoder_window.shape
        
        # í† í° ì„ë² ë”©
        token_embeds = self.token_embedding(decoder_window)
        
        # ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        if self.use_positional_encoding:
            positions = torch.arange(window_len, device=self.device).unsqueeze(0).expand(batch_size, -1)
            position_embeds = self.position_embedding(positions)
            combined_embeds = token_embeds + position_embeds
        else:
            combined_embeds = token_embeds
        
        # ì •ê·œí™” (T5 ë””ì½”ë” ë¸”ë¡ ì§„ì… ì „ ì •ê·œí™”ì™€ ë™ì¼)
        return self.layer_norm(combined_embeds)
    
    def _generate_causal_mask(self, size: int) -> torch.Tensor:
        """ìê¸°íšŒê·€ë¥¼ ìœ„í•œ causal mask ìƒì„±"""
        mask = torch.triu(torch.ones(size, size, device=self.device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask