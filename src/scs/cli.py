# src/scs/cli.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SCS (Spike-Based Cognitive System) ê³µì‹ CLI ì‹¤í–‰ ì§„ì…ì 
"""

import argparse
import sys
from pathlib import Path
import logging
from datetime import datetime
from typing import Dict, Any, List, Tuple
import torch

# --- í”„ë¡œì íŠ¸ ëª¨ë“ˆ Import ---
# ì´ íŒŒì¼ì€ íŒ¨í‚¤ì§€ ë‚´ë¶€ì— ìˆìœ¼ë¯€ë¡œ, ìƒëŒ€/ì ˆëŒ€ ê²½ë¡œë¡œ ëª¨ë“ˆì„ import í•©ë‹ˆë‹¤.
try:
    from scs.architecture import SCSSystem
    from scs.training import SCSTrainer, TrainingConfig, MultiObjectiveLoss, TimingLoss, OptimizerFactory
    from scs.data import create_dataloader, SCSTokenizer
    from scs.utils import (
        setup_logging, load_config, save_config, set_random_seed,
        get_device, ModelBuilder
    )
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì˜¤ë¥˜: {e}. íŒ¨í‚¤ì§€ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ('pip install -e .')")
    sys.exit(1)


# --- ëª…ë ¹í–‰ ì¸ì ë° ìœ íš¨ì„± ê²€ì‚¬ ---
def setup_args() -> argparse.ArgumentParser:
    """CLI ì¸ì ì„¤ì •"""
    parser = argparse.ArgumentParser(
        description="SCS ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ì„ ì–¸ì  ì¡°ë¦½ êµ¬ì¡° ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # í•™ìŠµ ëª¨ë“œ
  scs --mode train --config configs/phase2_logiqa_small.yaml
  
  # í‰ê°€ ëª¨ë“œ  
  scs --mode evaluate --experiment_dir experiments/phase2_20241201_1430
  
  # ì„¤ì • íŒŒì¼ ê²€ì¦
  scs --mode validate --config configs/my_experiment.yaml
        """
    )
    parser.add_argument("--mode", type=str, required=True, 
                       choices=["train", "evaluate", "validate"], 
                       help="ì‹¤í–‰ ëª¨ë“œ")
    parser.add_argument("--config", type=str, 
                       help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (train/validate ëª¨ë“œ í•„ìˆ˜)")
    parser.add_argument("--experiment_dir", type=str, 
                       help="ì‹¤í—˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ (evaluate ëª¨ë“œ í•„ìˆ˜)")
    parser.add_argument("--device", type=str, default="auto", 
                       help="ì—°ì‚° ì¥ì¹˜ ì„ íƒ (cuda, cpu, mps)")
    parser.add_argument("--seed", type=int, default=42, 
                       help="ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ")
    parser.add_argument("--debug", action="store_true", 
                       help="ë””ë²„ê·¸ ëª¨ë“œ (ìƒì„¸ ë¡œê¹…)")
    return parser

def validate_args(args: argparse.Namespace):
    """CLI ì¸ì ìœ íš¨ì„± ê²€ì‚¬"""
    if args.mode in ["train", "validate"] and not args.config:
        raise ValueError(f"{args.mode} ëª¨ë“œì—ì„œëŠ” --config ì¸ìê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
    if args.mode == "evaluate" and not args.experiment_dir:
        raise ValueError("evaluate ëª¨ë“œì—ì„œëŠ” --experiment_dir ì¸ìê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
    if args.config and not Path(args.config).exists():
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
    if args.experiment_dir and not Path(args.experiment_dir).exists():
        raise FileNotFoundError(f"ì‹¤í—˜ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.experiment_dir}")


# --- Conv2d ì¶œë ¥ ì°¨ì› ê³„ì‚° í•¨ìˆ˜ ---
def calculate_conv2d_output_size(input_size: int, kernel_size: int, stride: int = 1, 
                                padding: int = 0, dilation: int = 1) -> int:
    """Conv2d ì¶œë ¥ í¬ê¸° ê³„ì‚°"""
    return (input_size + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1


def validate_axonal_connections(config: Dict[str, Any]) -> List[str]:
    """ì¶•ì‚­ ì—°ê²°ì˜ ì°¨ì› í˜¸í™˜ì„± ê²€ì¦"""
    errors = []
    
    if "brain_regions" not in config or "axonal_connections" not in config:
        return errors
    
    brain_regions = config["brain_regions"]
    connections = config["axonal_connections"].get("connections", [])
    
    # ê° íƒ€ê²Ÿ ë…¸ë“œë³„ë¡œ ì—°ê²°ë“¤ì„ ê·¸ë£¹í™”
    target_connections = {}
    for conn in connections:
        source = conn.get("source")
        target = conn.get("target")
        
        if not source or not target:
            continue
            
        if target not in target_connections:
            target_connections[target] = []
        target_connections[target].append(conn)
    
    # ê° íƒ€ê²Ÿ ë…¸ë“œì— ëŒ€í•´ ì°¨ì› ê²€ì¦
    for target, target_conns in target_connections.items():
        if target not in brain_regions:
            continue
            
        target_grid_size = brain_regions[target].get("grid_size")
        if not target_grid_size or len(target_grid_size) != 2:
            continue
            
        target_h, target_w = target_grid_size
        
        for conn in target_conns:
            source = conn["source"]
            if source not in brain_regions:
                continue
                
            source_grid_size = brain_regions[source].get("grid_size")
            if not source_grid_size or len(source_grid_size) != 2:
                continue
                
            source_h, source_w = source_grid_size
            
            # Conv2d íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            kernel_size = conn.get("kernel_size", 1)
            stride = conn.get("stride", 1)
            padding = conn.get("padding", 0)
            dilation = conn.get("dilation", 1)
            
            # ì¶œë ¥ í¬ê¸° ê³„ì‚°
            try:
                output_h = calculate_conv2d_output_size(source_h, kernel_size, stride, padding, dilation)
                output_w = calculate_conv2d_output_size(source_w, kernel_size, stride, padding, dilation)
                
                # íƒ€ê²Ÿ í¬ê¸°ì™€ ë¹„êµ
                if output_h != target_h or output_w != target_w:
                    errors.append(
                        f"ì¶•ì‚¥ ì—°ê²° ì°¨ì› ë¶ˆì¼ì¹˜: {source}â†’{target}\n"
                        f"   ì†ŒìŠ¤ í¬ê¸°: {source_grid_size}, íƒ€ê²Ÿ í¬ê¸°: {target_grid_size}\n"
                        f"   Conv2d íŒŒë¼ë¯¸í„°: kernel_size={kernel_size}, stride={stride}, padding={padding}, dilation={dilation}\n"
                        f"   ê³„ì‚°ëœ ì¶œë ¥ í¬ê¸°: [{output_h}, {output_w}] (ì˜ˆìƒ: [{target_h}, {target_w}])\n"
                        f"   í•´ê²°ì±…: paddingì„ ì¡°ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ Conv2d íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”."
                    )
                    
            except Exception as calc_error:
                errors.append(
                    f"ì¶•ì‚¥ ì—°ê²° {source}â†’{target}ì˜ ì°¨ì› ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {calc_error}"
                )
    
    return errors


# --- ì„¤ì • íŒŒì¼ ê²€ì¦ ëª¨ë“œ ---
def validate_mode(args: argparse.Namespace):
    """ì„¤ì • íŒŒì¼ êµ¬ì¡° ê²€ì¦ ëª¨ë“œ"""
    print("ğŸ” ì„¤ì • íŒŒì¼ êµ¬ì¡° ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        config_path = Path(args.config)
        if not config_path.is_absolute():
            config_path = Path.cwd() / config_path
        config = load_config(config_path)
        
        # ModelBuilderë¥¼ í†µí•œ ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        validation_errors = ModelBuilder.validate_config_structure(config)
        
        # ì¶•ì‚¥ ì—°ê²° ì°¨ì› ê²€ì¦ ì¶”ê°€
        dimension_errors = validate_axonal_connections(config)
        
        all_errors = validation_errors + dimension_errors
        
        if not all_errors:
            print("âœ… ì„¤ì • íŒŒì¼ êµ¬ì¡°ê°€ ì˜¬ë°”ë¦…ë‹ˆë‹¤!")
            
            # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì•ˆí•¨)
            try:
                model = ModelBuilder.build_scs_from_config(config, device="cpu")
                total_params = sum(p.numel() for p in model.parameters())
                print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
                print(f"   - ì´ ë§¤ê°œë³€ìˆ˜: {total_params:,}")
                print(f"   - ë‡Œ ì˜ì—­ ìˆ˜: {len(config['brain_regions'])}")
                print(f"   - ì¶•ì‚­ ì—°ê²° ìˆ˜: {len(config['axonal_connections']['connections'])}")
                print(f"   - ì…ë ¥ ë…¸ë“œ: {config['system_roles']['input_node']}")
                print(f"   - ì¶œë ¥ ë…¸ë“œ: {config['system_roles']['output_node']}")
                
                # ì¶•ë³„ ì—°ê²° ì°¨ì› ì •ë³´ ì¶œë ¥
                print(f"ğŸ“ ì¶•ì‚­ ì—°ê²° ì°¨ì› ê²€ì¦:")
                for conn in config['axonal_connections']['connections']:
                    source = conn['source']
                    target = conn['target']
                    source_size = config['brain_regions'][source]['grid_size']
                    target_size = config['brain_regions'][target]['grid_size']
                    
                    kernel_size = conn.get('kernel_size', 1)
                    stride = conn.get('stride', 1)
                    padding = conn.get('padding', 0)
                    dilation = conn.get('dilation', 1)
                    
                    output_h = calculate_conv2d_output_size(source_size[0], kernel_size, stride, padding, dilation)
                    output_w = calculate_conv2d_output_size(source_size[1], kernel_size, stride, padding, dilation)
                    
                    print(f"   - {source}â†’{target}: {source_size} â†’ [{output_h}, {output_w}] (íƒ€ê²Ÿ: {target_size})")
                
                print("âœ… ëª¨ë¸ ìƒì„± ë° ì°¨ì› ê²€ì¦ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            except Exception as model_error:
                print(f"âš ï¸  ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {model_error}")
                return False
                
        else:
            print("âŒ ì„¤ì • íŒŒì¼ì—ì„œ ë‹¤ìŒ ì˜¤ë¥˜ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for i, error in enumerate(all_errors, 1):
                print(f"   {i}. {error}")
            
            # ì°¨ì› ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° ë„ì›€ë§ ì œê³µ
            if dimension_errors:
                print("\nğŸ’¡ ì¶•ì‚­ ì—°ê²° ì°¨ì› ë¬¸ì œ í•´ê²° ê°€ì´ë“œ:")
                print("   1. Conv2d ì¶œë ¥ í¬ê¸° ê³µì‹: floor((Input + 2*Padding - Dilation*(Kernel-1) - 1) / Stride + 1)")
                print("   2. ë™ì¼í•œ í¬ê¸° ìœ ì§€: stride=1ì¼ ë•Œ padding = (kernel_size-1)/2")
                print("   3. í¬ê¸° ì¶•ì†Œ: stride > 1 ë˜ëŠ” paddingì„ ì¤„ì´ì„¸ìš”")
                print("   4. ì˜¨ë¼ì¸ ê³„ì‚°ê¸°: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html")
            
            return False
            
        return True
        
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


# --- ë°ì´í„°ì…‹ ì´ë¦„ ì¶”ì¶œ í—¬í¼ ---
def get_dataset_name_from_config(config: Dict[str, Any], logger) -> str:
    """ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ì´ë¦„ ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
    dataset_name = None
    
    # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë°ì´í„°ì…‹ ì´ë¦„ íƒìƒ‰ (ìˆœì„œ ì¤‘ìš”)
    if "task" in config and "dataset_name" in config["task"]:
        dataset_name = config["task"]["dataset_name"]
    elif "data_loading" in config and "dataset_name" in config["data_loading"]:
        dataset_name = config["data_loading"]["dataset_name"]
    elif "data" in config and "dataset_name" in config["data"]:
        dataset_name = config["data"]["dataset_name"]  
    elif "dataset_name" in config:
        dataset_name = config["dataset_name"]
    else:
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        dataset_name = "datatune/LogiQA2.0"
        logger.warning(f"dataset_name not found in config, using default: {dataset_name}")
    
    return dataset_name


# --- í•™ìŠµ ì„¤ì • ì¶”ì¶œ ë° ì •ê·œí™” í—¬í¼ ---
def extract_and_normalize_training_config(config: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """ì„¤ì •ì—ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„° ì¶”ì¶œ ë° ì •ê·œí™”"""
    raw_config = config.get("learning", config.get("training", {})).copy()
    
    # gradual_unfreezing ë³„ë„ ì¶”ì¶œ
    unfreezing_config = raw_config.pop("gradual_unfreezing", {})
    
    # íŒŒë¼ë¯¸í„° ì´ë¦„ ì •ê·œí™”
    param_mapping = {
        "base_learning_rate": "learning_rate",
        "max_grad_norm": "gradient_clip_norm", 
        "eval_every_n_epochs": "eval_every",
        "save_every_n_epochs": "save_every",
        "use_schedule_sampling": "use_scheduled_sampling",
        "scheduled_sampling_start": "ss_start_prob",
        "scheduled_sampling_end": "ss_end_prob",
        "scheduled_sampling_decay": "ss_decay_epochs",
    }
    
    for old_name, new_name in param_mapping.items():
        if old_name in raw_config:
            raw_config[new_name] = raw_config.pop(old_name)
    
    # TrainingConfigê°€ í—ˆìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°
    valid_params = {
        "epochs", "learning_rate", "weight_decay", "gradient_clip_norm",
        "eval_every", "save_every", "early_stopping_patience", "max_clk_training",
        "use_scheduled_sampling", "ss_start_prob", "ss_end_prob", "ss_decay_epochs"
    }
    filtered_config = {k: v for k, v in raw_config.items() if k in valid_params}
    
    # íƒ€ì… ë³€í™˜
    float_params = ["learning_rate", "weight_decay", "gradient_clip_norm", "ss_start_prob", "ss_end_prob"]
    int_params = ["epochs", "eval_every", "save_every", "early_stopping_patience", "max_clk_training", "ss_decay_epochs"]
    bool_params = ["use_scheduled_sampling"]
    
    for param in float_params:
        if param in filtered_config:
            filtered_config[param] = float(filtered_config[param])
    
    for param in int_params:
        if param in filtered_config:
            filtered_config[param] = int(filtered_config[param])
    
    for param in bool_params:
        if param in filtered_config:
            filtered_config[param] = bool(filtered_config[param])
    
    return filtered_config, raw_config, unfreezing_config


# --- ëª¨ë“œë³„ ì‹¤í–‰ í•¨ìˆ˜ ---
def train_mode(args: argparse.Namespace, config: Dict[str, Any]):
    """í•™ìŠµ ëª¨ë“œ ì‹¤í–‰ (ìƒˆë¡œìš´ ì„ ì–¸ì  ì¡°ë¦½ êµ¬ì¡° ì§€ì›)"""
    # 1. ì‹¤í—˜ í™˜ê²½ ì„¤ì •
    experiment_name = f"{Path(args.config).stem}_{datetime.now().strftime('%Y%m%d_%H%M')}"
    experiment_dir = Path("experiments") / experiment_name
    experiment_dir.mkdir(parents=True, exist_ok=True)
    save_config(config, experiment_dir / "config.yaml")
    setup_logging(log_dir=experiment_dir / "logs", level=logging.DEBUG if args.debug else logging.INFO)
    logger = logging.getLogger(__name__)
    set_random_seed(args.seed)
    device = get_device(args.device)
    logger.info(f"ğŸš€ ì‹¤í—˜ '{experiment_name}' ì‹œì‘ | ë””ë°”ì´ìŠ¤: {device}")

    try:
        # 2. ì„¤ì • íŒŒì¼ ì‚¬ì „ ê²€ì¦ (ì°¨ì› ê²€ì¦ í¬í•¨)
        logger.info("ğŸ“‹ ì„¤ì • íŒŒì¼ êµ¬ì¡° ë° ì°¨ì› ê²€ì¦ ì¤‘...")
        validation_errors = ModelBuilder.validate_config_structure(config)
        dimension_errors = validate_axonal_connections(config)
        all_errors = validation_errors + dimension_errors
        
        if all_errors:
            logger.error("âŒ ì„¤ì • íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨:")
            for error in all_errors:
                logger.error(f"   - {error}")
            raise ValueError("ì„¤ì • íŒŒì¼ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        logger.info("âœ… ì„¤ì • íŒŒì¼ êµ¬ì¡° ë° ì°¨ì› ê²€ì¦ ì™„ë£Œ")

        # 3. ë°ì´í„° ë¡œë” ìƒì„±
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
        tokenizer = SCSTokenizer(config["data_loading"]["tokenizer"]["name"])

        # í† í¬ë‚˜ì´ì € ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
        tokenizer_config = config["data_loading"]["tokenizer"]
        tokenizer_config["pad_token_id"] = getattr(tokenizer.tokenizer, 'pad_token_id', 0)
        tokenizer_config["eos_token_id"] = getattr(tokenizer.tokenizer, 'eos_token_id', 1)
        tokenizer_config["bos_token_id"] = getattr(tokenizer.tokenizer, 'bos_token_id', 2)
        tokenizer_config["unk_token_id"] = getattr(tokenizer.tokenizer, 'unk_token_id', 3)
        pad_token_id = tokenizer_config["pad_token_id"]

        dataset_name = get_dataset_name_from_config(config, logger)
        
        # ìƒˆë¡œ ì¶”ê°€: learning_styleê³¼ BERT ì„¤ì • ì¶”ì¶œ
        task_config = config.get("task", {})
        learning_style = task_config.get("learning_style", "generative")
        bert_config = task_config.get("bert_config", None)
        
        # ë¡œê¹…
        if learning_style == "bert":
            logger.info(f"ğŸ­ BERT ìŠ¤íƒ€ì¼ í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
            if bert_config:
                logger.info(f"ğŸ“ BERT ì„¤ì •: {bert_config}")
        else:
            logger.info(f"ğŸ¯ ê¸°ì¡´ ìƒì„±í˜•(Generative) í•™ìŠµ ëª¨ë“œ")

        # ë°ì´í„° ì„¤ì • ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
        data_config = config.get("data", {})
        
        train_samples = data_config.get("train_samples", -1)
        val_samples = data_config.get("val_samples", -1)
        test_samples = data_config.get("test_samples", -1)
        task_id = task_config.get("task_id", 1)
        
        # í›ˆë ¨ ë°ì´í„° ë¡œë” ìƒì„± (ìƒˆ íŒŒë¼ë¯¸í„° ì „ë‹¬)
        train_loader = create_dataloader(
            dataset_name=dataset_name, 
            split="train", 
            batch_size=config["data_loading"]["batch_size"], 
            max_length=config["data_loading"]["tokenizer"]["max_length"], 
            tokenizer=tokenizer,
            num_samples=train_samples,
            task_id=task_id,
            learning_style=learning_style,  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
            bert_config=bert_config  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
        )

        # ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„± (ìƒˆ íŒŒë¼ë¯¸í„° ì „ë‹¬)
        val_loader = create_dataloader(
            dataset_name=dataset_name, 
            split="validation", 
            batch_size=1, 
            max_length=config["data_loading"]["tokenizer"]["max_length"], 
            tokenizer=tokenizer,
            num_samples=val_samples,
            task_id=task_id,
            learning_style=learning_style,  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
            bert_config=bert_config  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
        )
        
        logger.info(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (ë°ì´í„°ì…‹: {dataset_name}, ìŠ¤íƒ€ì¼: {learning_style})")

        # 4. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™” (ìƒˆë¡œìš´ ì„ ì–¸ì  ì¡°ë¦½ ë°©ì‹)
        logger.info("ğŸ§  SCS ëª¨ë¸ ìƒì„± ì¤‘...")
        config["io_system"]["input_interface"]["vocab_size"] = tokenizer.vocab_size
        config["io_system"]["output_interface"]["vocab_size"] = tokenizer.vocab_size

        model = ModelBuilder.build_scs_from_config(config, device=device)
        
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        logger.info(f"   - ì´ ë§¤ê°œë³€ìˆ˜: {total_params:,}")
        logger.info(f"   - ë‡Œ ì˜ì—­: {list(config['brain_regions'].keys())}")
        logger.info(f"   - ì…ë ¥â†’ì¶œë ¥: {config['system_roles']['input_node']} â†’ {config['system_roles']['output_node']}")

        # 5. í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì„±
        logger.info("âš™ï¸ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì„± ì¤‘...")
        
        filtered_config, raw_config, unfreezing_config = extract_and_normalize_training_config(config)
        
        training_config = TrainingConfig(pad_token_id=pad_token_id, device=device, **filtered_config)
        
        # TimingLoss ì‚¬ìš© (ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° êµ¬ì¡°)
        loss_fn = TimingLoss(
            pad_token_id=pad_token_id,
            # SCSLoss ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë“¤
            spike_reg_weight=raw_config.get("spike_reg_weight", 0.0),
            temporal_weight=raw_config.get("temporal_weight", 0.0),
            length_penalty_weight=raw_config.get("length_penalty_weight", 0.0),
            target_spike_rate=raw_config.get("target_spike_rate", 0.1),
            # TimingLoss ì „ìš© íŒŒë¼ë¯¸í„°ë“¤
            timing_weight=raw_config.get("timing_weight", 1.0),
            sync_target_start=raw_config.get("sync_target_start", 1.0),
            sync_target_end=raw_config.get("sync_target_end", 0.0)
        )
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„±
        optimizer_type = raw_config.get("optimizer", "adamw").lower()
        optimizer = OptimizerFactory.create(optimizer_type=optimizer_type, model=model, config=training_config)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=training_config.epochs)
        
        logger.info(f"âœ… í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì„± ì™„ë£Œ (ì˜µí‹°ë§ˆì´ì €: {optimizer_type})")

        # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
        logger.info("ğŸ¯ í•™ìŠµ ì‹œì‘...")
        
        trainer = SCSTrainer(
            model=model, 
            config=training_config, 
            loss_fn=loss_fn, 
            optimizer=optimizer, 
            scheduler=scheduler, 
            tokenizer=tokenizer,
            unfreezing_config=unfreezing_config
        )
        trainer.train(train_loader, val_loader, save_path=str(experiment_dir / "checkpoints"))

        # 7. ìµœì¢… í‰ê°€
        logger.info("ğŸ“ˆ ìµœì¢… í‰ê°€ ì‹œì‘...")
        test_loader = create_dataloader(
            dataset_name=dataset_name, 
            split="test", 
            batch_size=1, 
            max_length=config["data_loading"]["tokenizer"]["max_length"], 
            tokenizer=tokenizer,
            num_samples=test_samples,
            task_id=task_id,
            learning_style=learning_style,
            bert_config=bert_config
        )
        
        # ì˜ˆì‹œ ì €ì¥ ê°œìˆ˜ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ 10)
        save_examples = config.get('evaluation', {}).get('save_examples', 10)
        
        test_results = trainer.evaluate(test_loader, save_examples=save_examples)
        
        # ê²°ê³¼ ì €ì¥ (evaluate_modeì™€ ë™ì¼í•œ í˜•ì‹)
        results_path = experiment_dir / f"final_results_{datetime.now().strftime('%Y%m%d_%H%M')}.yaml"
        save_config(test_results, results_path)
        
        logger.info("ğŸ‰ í•™ìŠµ ë° í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
        for key, value in test_results.items():
            if key not in ['examples']:  # ì˜ˆì‹œëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸
                if isinstance(value, float):
                    logger.info(f"   - {key}: {value:.4f}")
                else:
                    logger.info(f"   - {key}: {value}")
        
        logger.info(f"ğŸ’¾ ì €ì¥ëœ ì˜ˆì‹œ ê°œìˆ˜: {test_results['num_examples_saved']}")
        logger.info(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {experiment_dir}")
        logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼ íŒŒì¼: {results_path}")

        # ê¸°ì¡´ results.yamlë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (ê°„ë‹¨í•œ ë²„ì „)
        simple_results = {k: v for k, v in test_results.items() if k not in ['examples']}
        save_config(simple_results, experiment_dir / "results.yaml")

    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise

def find_best_checkpoint(experiment_dir: Path) -> Path:
    """ê°€ì¥ ì í•©í•œ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
    checkpoint_dir = experiment_dir / "checkpoints"
    
    # 1ìˆœìœ„: best_model.pt
    best_model_path = checkpoint_dir / "best_model.pt"
    if best_model_path.exists():
        return best_model_path
    
    # 2ìˆœìœ„: ê°€ì¥ ìµœê·¼ ì—í¬í¬ ì²´í¬í¬ì¸íŠ¸
    checkpoint_files = list(checkpoint_dir.glob("checkpoint_epoch_*.pt"))
    if checkpoint_files:
        # ì—í¬í¬ ë²ˆí˜¸ë¡œ ì •ë ¬í•´ì„œ ê°€ì¥ ìµœê·¼ ê²ƒ ì„ íƒ
        def extract_epoch(path):
            try:
                return int(path.stem.split('_')[-1])
            except (ValueError, IndexError):
                return -1
        
        latest_checkpoint = max(checkpoint_files, key=extract_epoch)
        return latest_checkpoint
    
    raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {checkpoint_dir}ì—ì„œ 'best_model.pt' ë˜ëŠ” 'checkpoint_epoch_*.pt' íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

def load_model_with_checkpoint(config: Dict[str, Any], checkpoint_path: Path, device: str, logger) -> torch.nn.Module:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (ì„¤ì • í˜¸í™˜ì„± ê²€ì¦ í¬í•¨)"""
    try:
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (PyTorch 2.6+ í˜¸í™˜ì„±)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¡œê¹…
        epoch = checkpoint.get('epoch', 'unknown')
        best_loss = checkpoint.get('best_loss', 'unknown')
        save_timestamp = checkpoint.get('save_timestamp', 'unknown')
        logger.info(f"ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ ì •ë³´: ì—í¬í¬={epoch}, ìµœê³  ì†ì‹¤={best_loss}, ì €ì¥ ì‹œê°„={save_timestamp}")
        
        # ì„¤ì • í˜¸í™˜ì„± ê²€ì¦
        saved_training_config = checkpoint.get('training_config_dict', {})
        saved_model_config = checkpoint.get('model_config', {})
        saved_vocab_size = checkpoint.get('tokenizer_vocab_size')
        
        # ì–´íœ˜ í¬ê¸° í˜¸í™˜ì„± ê²€ì¦
        current_vocab_size = config.get("io_system", {}).get("input_interface", {}).get("vocab_size")
        if saved_vocab_size and current_vocab_size and saved_vocab_size != current_vocab_size:
            logger.warning(f"ì–´íœ˜ í¬ê¸° ë¶ˆì¼ì¹˜: ì €ì¥ëœ={saved_vocab_size}, í˜„ì¬={current_vocab_size}")
            logger.warning("ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆ í† í¬ë‚˜ì´ì €ë¡œ ì¬í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # í•™ìŠµ ì„¤ì • ë¹„êµ (ê²½ê³ ë§Œ)
        if saved_training_config:
            current_max_clk = config.get("learning", {}).get("max_clk_training") or config.get("training", {}).get("max_clk_training")
            saved_max_clk = saved_training_config.get('max_clk_training')
            if current_max_clk and saved_max_clk and current_max_clk != saved_max_clk:
                logger.warning(f"max_clk_training ë¶ˆì¼ì¹˜: ì €ì¥ëœ={saved_max_clk}, í˜„ì¬={current_max_clk}")
        
        # ëª¨ë¸ ìƒì„± (í˜„ì¬ ì„¤ì • ì‚¬ìš©)
        logger.info("ëª¨ë¸ êµ¬ì¡° ìƒì„± ì¤‘...")
        model = ModelBuilder.build_scs_from_config(config, device=device)
        
        # ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
        model_state_dict = checkpoint['model_state_dict']
        missing, unexpected = model.load_state_dict(model_state_dict, strict=False)
        
        if missing or unexpected:
            logger.warning("ì¼ë¶€ íŒŒë¼ë¯¸í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            if missing:
                logger.warning(f"ëˆ„ë½ëœ í‚¤ë“¤: {list(missing)[:5]}...")
            if unexpected:
                logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ í‚¤ë“¤: {list(unexpected)[:5]}...")
        else:
            logger.info("âœ… ëª¨ë¸ ìƒíƒœ ì™„ì „íˆ ë¡œë“œë¨")
        
        return model
        
    except Exception as e:
        logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        logger.info("ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
        return ModelBuilder.build_scs_from_config(config, device=device)
    
def evaluate_mode(args: argparse.Namespace):
    """í‰ê°€ ëª¨ë“œ ì‹¤í–‰ (BERT ìŠ¤íƒ€ì¼ ì§€ì› ì¶”ê°€)"""
    # 1. í™˜ê²½ ì„¤ì •
    experiment_dir = Path(args.experiment_dir)
    config_path = experiment_dir / "config.yaml"

    if not config_path.exists():
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")

    config = load_config(config_path)
    setup_logging(log_dir=experiment_dir / "logs" / "eval", level=logging.DEBUG if args.debug else logging.INFO)
    logger = logging.getLogger(__name__)
    device = get_device(args.device)
    logger.info(f"ğŸ“Š í‰ê°€ ëª¨ë“œ ì‹œì‘ | ë””ë°”ì´ìŠ¤: {device}")
    
    try:
        # 2. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
        checkpoint_path = find_best_checkpoint(experiment_dir)
        logger.info(f"ì‚¬ìš©í•  ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
        
        # 3. ì„¤ì • íŒŒì¼ ê²€ì¦ (ê²½ê³ ë§Œ, í‰ê°€ëŠ” ê³„ì†)
        logger.info("ğŸ“‹ ì €ì¥ëœ ì„¤ì • íŒŒì¼ ê²€ì¦ ì¤‘...")
        validation_errors = ModelBuilder.validate_config_structure(config)
        dimension_errors = validate_axonal_connections(config)
        all_errors = validation_errors + dimension_errors
        
        if all_errors:
            logger.warning("âš ï¸ ì €ì¥ëœ ì„¤ì • íŒŒì¼ì— ì¼ë¶€ ë¬¸ì œê°€ ìˆì§€ë§Œ í‰ê°€ë¥¼ ê³„ì†í•©ë‹ˆë‹¤:")
            for error in all_errors[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                logger.warning(f"   - {error}")

        # 4. ë°ì´í„° ë¡œë” ìƒì„± (BERT ìŠ¤íƒ€ì¼ ì§€ì› ì¶”ê°€)
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
        tokenizer = SCSTokenizer(config["data_loading"]["tokenizer"]["name"])

        # í† í¬ë‚˜ì´ì € ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
        tokenizer_config = config["data_loading"]["tokenizer"]
        tokenizer_config["pad_token_id"] = getattr(tokenizer.tokenizer, 'pad_token_id', 0)
        tokenizer_config["eos_token_id"] = getattr(tokenizer.tokenizer, 'eos_token_id', 1)
        tokenizer_config["bos_token_id"] = getattr(tokenizer.tokenizer, 'bos_token_id', 2)
        tokenizer_config["unk_token_id"] = getattr(tokenizer.tokenizer, 'unk_token_id', 3)
        pad_token_id = tokenizer_config["pad_token_id"]
        dataset_name = get_dataset_name_from_config(config, logger)

        # ìƒˆë¡œ ì¶”ê°€: learning_styleê³¼ BERT ì„¤ì • ì¶”ì¶œ
        task_config = config.get("task", {})
        learning_style = task_config.get("learning_style", "generative")
        bert_config = task_config.get("bert_config", None)
        
        # ë¡œê¹…
        if learning_style == "bert":
            logger.info(f"ğŸ­ BERT ìŠ¤íƒ€ì¼ í‰ê°€ ëª¨ë“œ")
            if bert_config:
                logger.info(f"ğŸ“ BERT ì„¤ì •: {bert_config}")
        else:
            logger.info(f"ğŸ¯ ê¸°ì¡´ ìƒì„±í˜•(Generative) í‰ê°€ ëª¨ë“œ")

        # ë°ì´í„° ì„¤ì • ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
        data_config = config.get("data", {})
        
        test_samples = data_config.get("test_samples", -1)
        task_id = task_config.get("task_id", 1)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± (ìƒˆ íŒŒë¼ë¯¸í„° ì „ë‹¬)
        test_loader = create_dataloader(
            dataset_name=dataset_name, 
            split="test", 
            batch_size=1, 
            max_length=config["data_loading"]["tokenizer"]["max_length"], 
            tokenizer=tokenizer,
            num_samples=test_samples,
            task_id=task_id,
            learning_style=learning_style,  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
            bert_config=bert_config  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
        )
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (ìŠ¤íƒ€ì¼: {learning_style})")
        
        # 5. ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œ)
        logger.info("ğŸ§  ëª¨ë¸ ë³µì› ì¤‘...")
        config["io_system"]["input_interface"]["vocab_size"] = tokenizer.vocab_size
        config["io_system"]["output_interface"]["vocab_size"] = tokenizer.vocab_size
        
        model = load_model_with_checkpoint(config, checkpoint_path, device, logger)
        logger.info("âœ… ëª¨ë¸ ë³µì› ì™„ë£Œ")

        # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í‰ê°€ (ê¸°ì¡´ ì½”ë“œ)
        logger.info("ğŸ“ˆ í‰ê°€ ì‹¤í–‰ ì¤‘...")
        
        filtered_config, _, _ = extract_and_normalize_training_config(config)
        
        training_config = TrainingConfig(pad_token_id=pad_token_id, device=device, **filtered_config)
        trainer = SCSTrainer(model=model, config=training_config, tokenizer=tokenizer)
        
        # ì˜ˆì‹œ ì €ì¥ ê°œìˆ˜ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ 10)
        save_examples = config.get('evaluation', {}).get('save_examples', 10)
        
        results = trainer.evaluate(test_loader, save_examples=save_examples)
        
        # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥ (ê¸°ì¡´ ì½”ë“œ)
        results_path = experiment_dir / f"eval_results_{datetime.now().strftime('%Y%m%d_%H%M')}.yaml"
        save_config(results, results_path)
        
        logger.info("ğŸ‰ í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("ğŸ“Š í‰ê°€ ê²°ê³¼:")
        for key, value in results.items():
            if key not in ['examples']:  # ì˜ˆì‹œëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸
                if isinstance(value, float):
                    logger.info(f"   - {key}: {value:.4f}")
                else:
                    logger.info(f"   - {key}: {value}")
        
        logger.info(f"ğŸ’¾ ì €ì¥ëœ ì˜ˆì‹œ ê°œìˆ˜: {results['num_examples_saved']}")
        logger.info(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_path}")

    except Exception as e:
        logger.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise


def main():
    """ë©”ì¸ CLI í•¨ìˆ˜ (ìƒˆë¡œìš´ ì„ ì–¸ì  ì¡°ë¦½ êµ¬ì¡° ì§€ì›)"""
    parser = setup_args()
    args = parser.parse_args()
    
    try:
        validate_args(args)
        
        if args.mode == "validate":
            # ì„¤ì • íŒŒì¼ ê²€ì¦ ëª¨ë“œ
            success = validate_mode(args)
            sys.exit(0 if success else 1)
            
        elif args.mode == "train":
            # í•™ìŠµ ëª¨ë“œ
            config_path = Path(args.config)
            if not config_path.is_absolute():
                config_path = Path.cwd() / config_path
            config = load_config(config_path)
            train_mode(args, config)
            
        elif args.mode == "evaluate":
            # í‰ê°€ ëª¨ë“œ
            evaluate_mode(args)
            
    except (ValueError, FileNotFoundError) as e:
        print(f"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        logging.getLogger(__name__).critical(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    # ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•ŒëŠ” ì•„ë¬´ ì¼ë„ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ.
    # scs ëª…ë ¹ì–´ ë˜ëŠ” run.pyë¥¼ í†µí•´ main()ì´ í˜¸ì¶œë˜ì–´ì•¼ í•¨.
    pass