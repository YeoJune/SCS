# src/scs/data/dataset.py
"""
ë²”ìš© SCS ë°ì´í„°ì…‹ ëª¨ë“ˆ
"""

import torch
from torch.utils.data import Dataset
from typing import Dict, List, Any, Union, Optional
from datasets import load_dataset
import logging

from .tokenizer import SCSTokenizer
from .bert_dataset import BERTStyleDataset

logger = logging.getLogger(__name__)

class BaseDataset(Dataset):
    """ë²”ìš© ë² ì´ìŠ¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        dataset_name: str,
        tokenizer: SCSTokenizer,
        split: str = "train",
        max_length: int = 256,
        num_samples: int = -1  # max_samples â†’ num_samplesë¡œ ë³€ê²½, -1ì€ ì „ì²´
    ):
        self.dataset_name = dataset_name
        self.tokenizer = tokenizer
        self.split = split
        self.max_length = max_length
        self.num_samples = num_samples  # ë³€ê²½
        
        logger.info(f"ğŸ“¦ Loading {dataset_name} ({split})...")
        self.data = self._load_and_process_data()
        logger.info(f"âœ… Loaded {len(self.data)} examples")
        
    def _load_and_process_data(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ì…‹ ë¡œë”©
            raw_dataset = load_dataset(self.dataset_name, split=self.split)
            
            # ìƒ˜í”Œ ê°œìˆ˜ ì œí•œ (í‘œì¤€ì ì¸ ë°©ì‹)
            if self.num_samples > 0 and len(raw_dataset) > self.num_samples:
                raw_dataset = raw_dataset.select(range(self.num_samples))
                logger.info(f"Dataset truncated to {self.num_samples} samples")
            else:
                logger.info(f"Using full dataset: {len(raw_dataset)} samples")
            
            # ë°ì´í„° ì²˜ë¦¬
            processed_data = []
            for idx, item in enumerate(raw_dataset):
                try:
                    processed_item = self._process_item(item, idx)
                    if processed_item:
                        processed_data.append(processed_item)
                except Exception as e:
                    logger.warning(f"Failed to process item {idx}: {e}")
                    continue
                    
            return processed_data
            
        except Exception as e:
            logger.error(f"Failed to load dataset {self.dataset_name}: {e}")
            return []
    
    def _process_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì•„ì´í…œ ì²˜ë¦¬ - ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ"""
        return {
            'input_text': str(item),
            'target_text': "unknown",
            'metadata': {'index': idx}
        }
    
    def _tokenize_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """í† í°í™”"""
        input_tokens = self.tokenizer.tokenize(item['input_text'], self.max_length)
        target_tokens = self.tokenizer.tokenize(item['target_text'], self.max_length // 4)
        
        return {
            'input_tokens': input_tokens,
            'target_tokens': target_tokens,
            'input_text': item['input_text'],
            'target_text': item['target_text'],
            'metadata': item.get('metadata', {})
        }
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        try:
            return self._tokenize_item(self.data[idx])
        except Exception as e:
            logger.warning(f"Error in __getitem__[{idx}]: {e}")
            # í´ë°± ì•„ì´í…œ ë°˜í™˜
            return {
                'input_tokens': [0] * 10,  # ê¸°ë³¸ í† í°
                'target_tokens': [0] * 5,
                'input_text': "error",
                'target_text': "error",
                'metadata': {'index': idx, 'error': True}
            }


class LogiQADataset(BaseDataset):
    """LogiQA ì „ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, tokenizer: SCSTokenizer, split: str = "train", num_samples: Optional[int] = None):
        super().__init__("datatune/LogiQA2.0", tokenizer, split, max_length=256, num_samples=num_samples)

    def _process_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """LogiQA ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            import json
            
            # text í•„ë“œì—ì„œ JSON íŒŒì‹±
            raw_text = item.get('text', '').strip()
            if not raw_text:
                return None
            
            # JSON íŒŒì‹±
            data = json.loads(raw_text)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            context = data.get('text', '').strip()
            question = data.get('question', '').strip()
            options = data.get('options', [])
            answer = data.get('answer', 0)
            
            if not question or not options or len(options) < 2:
                return None
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
            input_parts = []
            if context:
                input_parts.append(f"Context: {context}")
            input_parts.append(f"Question: {question}")
            
            # ì„ íƒì§€ ì¶”ê°€
            # options_text = " ".join([f"{chr(65+i)}) {opt.strip()}" 
            #                        for i, opt in enumerate(options)])
            # input_parts.append(f"Options: {options_text}")
            target_text = options[answer].strip()  # ì‹¤ì œ ë‹µ í…ìŠ¤íŠ¸
            
            return {
                'input_text': " ".join(input_parts),
                'target_text': target_text,
                'metadata': {
                    'id': data.get('id', idx),
                    'context': context,
                    'question': question,
                    'options': options,
                    'answer': answer,
                    'index': idx
                }
            }
            
        except Exception as e:
            logger.warning(f"Failed to process LogiQA item {idx}: {e}")
            return None

class bAbIDataset(BaseDataset):
    """
    bAbI ì „ìš© ë°ì´í„°ì…‹ ('Muennighoff/babi' ë²„ì „ ì‚¬ìš©)
    """
    def __init__(self, tokenizer: SCSTokenizer, task_id: int = 1, split: str = "train", num_samples: int = -1):
        assert 1 <= task_id <= 20, "task_idëŠ” 1ê³¼ 20 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤."
        self.task_id = task_id
        
        super().__init__(
            dataset_name="Muennighoff/babi",
            tokenizer=tokenizer, 
            split=split, 
            max_length=256,
            num_samples=num_samples
        )

    def _load_and_process_data(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ - task_idë¡œ í•„í„°ë§"""
        try:
            # 1. ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
            raw_dataset = load_dataset(self.dataset_name, split=self.split)
            
            # 2. ì›í•˜ëŠ” íƒœìŠ¤í¬ ë²ˆí˜¸ë¡œ í•„í„°ë§
            filtered_dataset = raw_dataset.filter(lambda example: example['task'] == self.task_id)
            logger.info(f"Task {self.task_id} í•„í„°ë§ ì™„ë£Œ: {len(filtered_dataset)}ê°œ ìƒ˜í”Œ")
            
            # 3. ìƒ˜í”Œ ìˆ˜ ì œí•œ (í‘œì¤€ì ì¸ ë°©ì‹)
            if self.num_samples > 0 and len(filtered_dataset) > self.num_samples:
                final_dataset = filtered_dataset.select(range(self.num_samples))
                logger.info(f"Dataset truncated to {self.num_samples} samples")
            else:
                final_dataset = filtered_dataset
                logger.info(f"Using full filtered dataset: {len(final_dataset)} samples")

            # 4. ê° ì•„ì´í…œ ì²˜ë¦¬
            processed_data = []
            for idx, item in enumerate(final_dataset):
                try:
                    processed_item = self._process_item(item, idx)
                    if processed_item:
                        processed_data.append(processed_item)
                except Exception as e:
                    logger.warning(f"bAbI ì•„ì´í…œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            return processed_data

        except Exception as e:
            logger.error(f"bAbI ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _process_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """bAbI ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # í•„ë“œ ì´ë¦„: story -> passageë¡œ ë³€ê²½ë¨
            passage_text = item.get('passage', '').strip().replace('\n', ' ')
            question_text = item.get('question', '').strip()
            answer_text = item.get('answer', '').strip()
            
            if not passage_text or not question_text or not answer_text:
                return None
            
            # ì…ë ¥ í˜•ì‹: "Context: [ì§€ë¬¸] Question: [ì§ˆë¬¸]"
            input_text = f"Context: {passage_text} Question: {question_text}"
            
            return {
                'input_text': input_text,
                'target_text': f"{question_text} Answer: {answer_text}",
                'metadata': {
                    'index': idx, 
                    'task': item.get('task', self.task_id),
                    'task_type': 'reasoning'
                }
            }
            
        except Exception as e:
            logger.warning(f"bAbI ì•„ì´í…œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None


class SQuADDataset(BaseDataset):
    """SQuAD (Stanford Question Answering Dataset) ì „ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, tokenizer: SCSTokenizer, split: str = "train", num_samples: int = -1):
        super().__init__(
            dataset_name="rajpurkar/squad",
            tokenizer=tokenizer, 
            split=split, 
            max_length=512,  # SQuADëŠ” ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ
            num_samples=num_samples
        )

    def _process_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """SQuAD ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            context = item.get('context', '').strip()
            question = item.get('question', '').strip()
            answers = item.get('answers', {})
            title = item.get('title', '').strip()
            item_id = item.get('id', str(idx))
            
            if not context or not question:
                return None
            
            # ë‹µë³€ ì²˜ë¦¬
            answer_texts = answers.get('text', [])
            answer_starts = answers.get('answer_start', [])
            
            # ì²« ë²ˆì§¸ ë‹µë³€ì„ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš© (SQuADëŠ” ì—¬ëŸ¬ ë‹µë³€ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            if answer_texts and len(answer_texts) > 0:
                target_text = answer_texts[0].strip()
            else:
                # ë‹µë³€ì´ ì—†ëŠ” ê²½ìš° (SQuAD 2.0ì˜ unanswerable questions)
                target_text = "unanswerable"
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
            input_parts = []
            
            if title:
                input_parts.append(f"Title: {title}")
            
            input_parts.extend([
                f"Context: {context}",
                f"Question: {question}"
            ])
            
            input_text = " ".join(input_parts)
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'metadata': {
                    'id': item_id,
                    'title': title,
                    'context': context,
                    'question': question,
                    'all_answers': answer_texts,  # ëª¨ë“  ë‹µë³€ ë³´ì¡´
                    'answer_starts': answer_starts,
                    'task_type': 'reading_comprehension',
                    'index': idx,
                    'is_answerable': len(answer_texts) > 0
                }
            }
            
        except Exception as e:
            logger.warning(f"Failed to process SQuAD item {idx}: {e}")
            return None


class MultiDataset(BaseDataset):
    """ë‹¤ì¤‘ íƒœìŠ¤í¬ ì§€ì› ë°ì´í„°ì…‹"""
    
    def __init__(
        self,
        dataset_name: str, 
        tokenizer: SCSTokenizer, 
        split: str = "train",
        task_type: str = "auto",
        num_samples: int = -1
    ):
        self.task_type = task_type
        super().__init__(dataset_name, tokenizer, split, num_samples=num_samples)
    
    def _process_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """ë‹¤ì¤‘ íƒœìŠ¤í¬ ì•„ì´í…œ ì²˜ë¦¬"""
        
        # LogiQA ì²˜ë¦¬
        if "logiqa" in self.dataset_name.lower():
            return self._process_logiqa_item(item, idx)
        
        # NLI ì²˜ë¦¬
        elif 'premise' in item and 'hypothesis' in item:
            return self._process_nli_item(item, idx)
        
        # QA ì²˜ë¦¬
        elif 'question' in item and 'answer' in item:
            return self._process_qa_item(item, idx)
        
        # ê¸°ë³¸ ì²˜ë¦¬
        else:
            return self._process_generic_item(item, idx)
    
    def _process_logiqa_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """LogiQA ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            import json
            
            # text í•„ë“œì—ì„œ JSON íŒŒì‹±
            raw_text = item.get('text', '').strip()
            if not raw_text:
                return None
            
            # JSON íŒŒì‹±
            data = json.loads(raw_text)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            context = data.get('text', '').strip()
            question = data.get('question', '').strip()
            options = data.get('options', [])
            answer = data.get('answer', 0)
            
            if not question:
                return None
            
            input_parts = ["Answer the question:"]
            if context:
                input_parts.append(f"Context: {context}")
            input_parts.append(f"Question: {question}")
            
            if options:
                # options_text = " ".join([f"{chr(65+i)}) {opt}" 
                #                        for i, opt in enumerate(options)])
                # input_parts.append(f"Options: {options_text}")
                
                # ì •ë‹µ ì²˜ë¦¬
                if isinstance(answer, int) and 0 <= answer < len(options):
                    target_text = options[answer].strip()  # ì‹¤ì œ ë‹µ í…ìŠ¤íŠ¸
                else:
                    target_text = "unknown"
            else:
                target_text = "unknown"
            
            return {
                'input_text': " ".join(input_parts),
                'target_text': target_text,
                'metadata': {'task_type': 'reasoning', 'index': idx}
            }
        except Exception as e:
            logger.warning(f"Failed to process LogiQA item {idx}: {e}")
            return None
    
    def _process_nli_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """NLI ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            premise = item.get('premise', '').strip()
            hypothesis = item.get('hypothesis', '').strip()
            label = item.get('label', 0)
            
            if not premise or not hypothesis:
                return None
            
            input_text = f"Determine relationship: Premise: {premise} Hypothesis: {hypothesis}"
            
            # ë¼ë²¨ ë§¤í•‘
            label_map = {0: "entailment", 1: "neutral", 2: "contradiction"}
            target_text = label_map.get(label, "neutral")
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'metadata': {'task_type': 'nli', 'index': idx}
            }
        except:
            return None
    
    def _process_qa_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """QA ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            context = item.get('context', '').strip()
            question = item.get('question', '').strip()
            answer = item.get('answer', '').strip()
            
            if not question:
                return None
            
            if context:
                input_text = f"Answer based on context: Context: {context} Question: {question}"
            else:
                input_text = f"Answer the question: {question}"
            
            return {
                'input_text': input_text,
                'target_text': answer,
                'metadata': {'task_type': 'qa', 'index': idx}
            }
        except:
            return None
    
    def _process_generic_item(self, item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """ì¼ë°˜ ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # í…ìŠ¤íŠ¸ í•„ë“œ ì°¾ê¸°
            text_fields = ['text', 'sentence', 'input', 'content']
            input_text = ""
            
            for field in text_fields:
                if field in item:
                    input_text = str(item[field])
                    break
            
            if not input_text:
                input_text = str(item)
            
            return {
                'input_text': f"Process: {input_text}",
                'target_text': "processed",
                'metadata': {'task_type': 'generic', 'index': idx}
            }
        except:
            return None


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_dataset(
    dataset_name: str,
    tokenizer: SCSTokenizer,
    split: str = "train",
    num_samples: int = -1,
    task_id: int = 1,
    learning_style: str = "generative",  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
    bert_config: Optional[Dict[str, Any]] = None  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
) -> BaseDataset:
    """ë°ì´í„°ì…‹ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜ - BERT ìŠ¤íƒ€ì¼ ì§€ì› ì¶”ê°€"""
    
    # 1ë‹¨ê³„: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë² ì´ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
    if "babi" in dataset_name.lower():
        base_dataset = bAbIDataset(tokenizer, task_id=task_id, split=split, num_samples=num_samples)
    elif "logiqa" in dataset_name.lower():
        base_dataset = LogiQADataset(tokenizer, split, num_samples=num_samples)
    elif "squad" in dataset_name.lower():
        base_dataset = SQuADDataset(tokenizer, split, num_samples=num_samples)
    else:
        base_dataset = MultiDataset(dataset_name, tokenizer, split, num_samples=num_samples)
    
    # 2ë‹¨ê³„: learning_styleì— ë”°ë¼ BERT ìŠ¤íƒ€ì¼ ë³€í™˜ ì ìš©
    if learning_style == "bert":
        logger.info(f"Converting to BERT style dataset with learning_style='{learning_style}'")
        
        # BERT ì„¤ì • ê¸°ë³¸ê°’
        default_bert_config = {
            'mask_probability': 0.15,
            'mask_token_id': None,  # ìë™ ê°ì§€
            'random_token_prob': 0.1,
            'unchanged_prob': 0.1,
            'min_masks': 1,
            'max_masks_ratio': 0.5,
            'special_tokens': None  # ìë™ ì„¤ì •
        }
        
        # ì‚¬ìš©ì ì„¤ì • ë³‘í•©
        if bert_config:
            default_bert_config.update(bert_config)
        
        # BERTStyleDatasetìœ¼ë¡œ ë˜í•‘ (í† í¬ë‚˜ì´ì € ì „ë‹¬)
        return BERTStyleDataset(
            base_dataset=base_dataset,
            tokenizer=tokenizer,  # í† í¬ë‚˜ì´ì € ì „ë‹¬ (ì¤‘ìš”!)
            **default_bert_config
        )
    
    else:
        # ê¸°ì¡´ generative ë°©ì‹ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return base_dataset