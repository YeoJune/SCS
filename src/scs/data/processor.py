# src/scs/data/processor.py
"""
ê°„ì†Œí™”ëœ ë²”ìš© ë°ì´í„° í”„ë¡œì„¸ì„œ
"""

from typing import List, Dict, Any, Optional
import logging

from .dataset import create_dataset
from .tokenizer import SCSTokenizer

logger = logging.getLogger(__name__)


class DataProcessor:
    """ë²”ìš© ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, tokenizer: Optional[SCSTokenizer] = None):
        self.tokenizer = tokenizer or SCSTokenizer()
    
    def create_dataset(
        self, 
        dataset_name: str, 
        split: str = "train",
        tokenizer: Optional[SCSTokenizer] = None,
        max_length: int = 256,
        num_samples: int = -1,
        task_id: int = 1,
        learning_style: str = "generative",  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
        bert_config: Optional[Dict[str, Any]] = None  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
    ):
        """ë°ì´í„°ì…‹ ìƒì„± - BERT ìŠ¤íƒ€ì¼ ì§€ì› ì¶”ê°€"""
        effective_tokenizer = tokenizer or self.tokenizer
        
        logger.info(f"Creating dataset: {dataset_name} ({split}) with learning_style='{learning_style}'")
        if num_samples > 0:
            logger.info(f"Limiting to {num_samples} samples")
        else:
            logger.info("Using full dataset")
        
        try:
            dataset = create_dataset(
                dataset_name=dataset_name,
                tokenizer=effective_tokenizer,
                split=split,
                num_samples=num_samples,
                task_id=task_id,
                learning_style=learning_style,  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„° ì „ë‹¬
                bert_config=bert_config  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„° ì „ë‹¬
            )
            
            logger.info(f"âœ… Successfully created dataset with {len(dataset)} examples")
            
            # BERT ìŠ¤íƒ€ì¼ì¸ ê²½ìš° ë§ˆìŠ¤í‚¹ í†µê³„ ì¶œë ¥
            if learning_style == "bert" and hasattr(dataset, 'get_masking_stats'):
                stats = dataset.get_masking_stats(num_samples=min(10, len(dataset)))
                logger.info(f"ğŸ“Š BERT masking stats: {stats}")
            
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ Failed to create dataset {dataset_name}: {e}")
            raise
    
    def get_supported_datasets(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” ë°ì´í„°ì…‹ ëª©ë¡"""
        return [
            "datatune/LogiQA2.0",
            "Muennighoff/babi",
            "rajpurkar/squad",
        ]
    
    def validate_dataset_config(self, dataset_name: str, split: str) -> bool:
        """ë°ì´í„°ì…‹ ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if not dataset_name or not split:
                return False
            
            # split ìœ íš¨ì„±
            valid_splits = ["train", "validation", "test", "dev"]
            if split not in valid_splits:
                logger.warning(f"Unusual split name: {split}")
            
            return True
            
        except Exception as e:
            logger.error(f"Validation failed: {e}")
            return False
