# src/scs/data/processor.py
"""
ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ
"""

from typing import List, Dict, Any
from datasets import load_dataset

from .tokenizer import SCSTokenizer


class DataProcessor:
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    
    def __init__(self, tokenizer: SCSTokenizer):
        self.tokenizer = tokenizer
    
    def process_classification(self, dataset_name: str, split: str = "train") -> List[Dict[str, Any]]:
        """ë¶„ë¥˜ íƒœìŠ¤í¬ ì²˜ë¦¬"""
        dataset = load_dataset(dataset_name, split=split)
        processed = []
        
        for item in dataset:
            if 'sentence' in item:
                input_text = item['sentence']
            elif 'text' in item:
                input_text = item['text']
            else:
                input_text = str(item)
            
            label = item.get('label', 0)
            target_text = f"<LABEL_{label}>"
            
            processed.append({
                'input': input_text,
                'target': target_text,
                'metadata': {'label': label}
            })
        
        return processed
    
    def process_qa(self, dataset_name: str, split: str = "train") -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ ë‹µë³€ íƒœìŠ¤í¬ ì²˜ë¦¬"""
        dataset = load_dataset(dataset_name, split=split)
        processed = []
        
        for item in dataset:
            context = item.get('context', '')
            question = item.get('question', item.get('query', ''))
            answer = item.get('answer', item.get('correct_answer', ''))
            
            if context:
                input_text = f"{context} [SEP] {question}"
            else:
                input_text = question
            
            processed.append({
                'input': input_text,
                'target': answer,
                'metadata': {'context': context, 'question': question}
            })
        
        return processed
    
    def process_reasoning(self, dataset_name: str, split: str = "train") -> List[Dict[str, Any]]:
        """ì¶”ë¡  íƒœìŠ¤í¬ ì²˜ë¦¬"""
        # LogiQA ê´€ë ¨ ëª¨ë“  ë³€í˜• ì²˜ë¦¬
        if "logiqa" in dataset_name.lower():
            return self._process_logiqa(dataset_name, split)
        
        dataset = load_dataset(dataset_name, split=split)
        processed = []
        
        for item in dataset:
            if 'premise' in item and 'hypothesis' in item:
                input_text = f"{item['premise']} [SEP] {item['hypothesis']}"
                label = item.get('label', 0)
                target_text = f"<ENTAIL_{label}>"
            else:
                input_text = str(item)
                target_text = "<UNK>"
            
            processed.append({
                'input': input_text,
                'target': target_text,
                'metadata': item
            })
        
        return processed
    
    def _process_logiqa(self, dataset_name: str, split: str = "train") -> List[Dict[str, Any]]:
        """LogiQA ì „ìš© ì²˜ë¦¬ - ì•ˆì „í•œ ë¡œë”© ë°©ì‹"""
        dataset = None
        
        # 1ë‹¨ê³„: ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ë¶€í„° ì‹œë„
        loading_strategies = [
            # ì „ì²´ ë°ì´í„°ì…‹ì„ trainìœ¼ë¡œ ë¡œë“œí•˜ê³  ìˆ˜ë™ ë¶„í• 
            ("manual_split", lambda: load_dataset("lucasmccabe/logiqa", trust_remote_code=False)),
            # ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„
            ("force_download", lambda: load_dataset("lucasmccabe/logiqa", download_mode="force_redownload", trust_remote_code=False)),
            # ìºì‹œ ë¬´ì‹œí•˜ê³  ìŠ¤íŠ¸ë¦¬ë°
            ("streaming", lambda: load_dataset("lucasmccabe/logiqa", streaming=True, trust_remote_code=False)),
        ]
        
        for strategy_name, load_func in loading_strategies:
            try:
                print(f"ğŸ”„ LogiQA ë¡œë”© ì‹œë„: {strategy_name}")
                raw_dataset = load_func()
                
                # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
                if hasattr(raw_dataset, 'keys'):
                    available_splits = list(raw_dataset.keys())
                    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„í• : {available_splits}")
                    
                    # ìš”ì²­ëœ splitì´ ìˆìœ¼ë©´ ì‚¬ìš©
                    if split in available_splits:
                        dataset = raw_dataset[split]
                        print(f"âœ… {split} ë¶„í•  ë¡œë“œ: {len(dataset) if hasattr(dataset, '__len__') else 'ìŠ¤íŠ¸ë¦¬ë°'}")
                        break
                    # trainë§Œ ìˆìœ¼ë©´ ìˆ˜ë™ ë¶„í• 
                    elif "train" in available_splits:
                        train_data = raw_dataset["train"]
                        if split == "train":
                            # 80%ë¥¼ í›ˆë ¨ìš©ìœ¼ë¡œ
                            split_data = train_data.train_test_split(test_size=0.2, seed=42)
                            dataset = split_data["train"]
                        else:
                            # 20%ë¥¼ ê²€ì¦/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ  
                            split_data = train_data.train_test_split(test_size=0.2, seed=42)
                            dataset = split_data["test"]
                        print(f"âœ… ìˆ˜ë™ ë¶„í•  ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ")
                        break
                else:
                    # ë‹¨ì¼ ë°ì´í„°ì…‹ì¸ ê²½ìš°
                    dataset = raw_dataset
                    print(f"âœ… ë‹¨ì¼ ë°ì´í„°ì…‹ ë¡œë“œ: {len(dataset) if hasattr(dataset, '__len__') else 'ìŠ¤íŠ¸ë¦¬ë°'}")
                    break
                    
            except Exception as e:
                print(f"âŒ {strategy_name} ì‹¤íŒ¨: {str(e)[:100]}...")
                continue
        
        if dataset is None:
            raise RuntimeError("ëª¨ë“  LogiQA ë¡œë”© ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        processed = []
        
        # ë°ì´í„° ì²˜ë¦¬ ë° ê²€ì¦
        processed_count = 0
        error_count = 0
        
        for idx, item in enumerate(dataset):
            try:
                # LogiQA í•„ë“œ ë§¤í•‘ (ìµœì‹  í˜•ì‹)
                context = item.get('context', '').strip()
                question = item.get('query', item.get('question', '')).strip()
                options = item.get('options', item.get('choices', []))
                correct_option = item.get('correct_option', item.get('answer', 0))
                
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
                if not question:
                    print(f"âš ï¸ í•­ëª© {idx}: ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŒ")
                    continue
                    
                if not options or len(options) < 2:
                    print(f"âš ï¸ í•­ëª© {idx}: ì„ íƒì§€ê°€ ë¶€ì¡±í•¨ ({len(options)}ê°œ)")
                    continue
                
                # ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
                input_parts = ["ì¶”ë¡ :"]
                if context:
                    input_parts.append(f"ìƒí™©: {context}")
                input_parts.append(f"ì§ˆë¬¸: {question}")
                
                if options and len(options) >= 2:
                    options_text = " ".join([f"{chr(65+i)}) {opt.strip()}" 
                                           for i, opt in enumerate(options)])
                    input_parts.append(f"ì„ íƒì§€: {options_text}")
                
                input_text = " ".join(input_parts)
                
                # ì •ë‹µ ì²˜ë¦¬ (A, B, C, D í˜•ì‹)
                if isinstance(correct_option, int) and 0 <= correct_option < len(options):
                    target_text = chr(65 + correct_option)  # 0->A, 1->B, etc.
                elif isinstance(correct_option, str) and correct_option.upper() in ['A', 'B', 'C', 'D']:
                    target_text = correct_option.upper()
                else:
                    print(f"âš ï¸ í•­ëª© {idx}: ì˜ëª»ëœ ì •ë‹µ í˜•ì‹ ({correct_option}), 'A'ë¡œ ì„¤ì •")
                    target_text = "A"  # ê¸°ë³¸ê°’
                
                processed.append({
                    'input': input_text,
                    'target': target_text,
                    'metadata': {
                        'context': context,
                        'question': question,
                        'options': options,
                        'correct_option': correct_option,
                        'original_index': idx
                    }
                })
                
                processed_count += 1
                
            except Exception as e:
                error_count += 1
                print(f"âŒ í•­ëª© {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}")
                if error_count > 10:  # ë„ˆë¬´ ë§ì€ ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨
                    print("âš ï¸ ì˜¤ë¥˜ê°€ ë„ˆë¬´ ë§ì•„ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                continue
        
        print(f"ğŸ“Š LogiQA ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì˜¤ë¥˜")
        
        if not processed:
            raise RuntimeError("ì²˜ë¦¬ëœ LogiQA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return processed