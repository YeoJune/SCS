# src/scs/evaluation/visualizer.py
"""
SCS ëª¨ë¸ ì‹œê°í™” ëª¨ë“ˆ (v2.0)

SCSSystemì˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì— ë§ì¶˜ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ë° ê°€ì¤‘ì¹˜ ì‹œê°í™” ê¸°ëŠ¥ ì œê³µ
"""

import torch
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np
from pathlib import Path
from typing import Any, Dict, List, Optional
import logging

logger = logging.getLogger(__name__)


def generate_visualizations(model, test_loader, output_dir: Path):
    """
    SCS ëª¨ë¸ì˜ ìŠ¤íŒŒì´í¬ íŒ¨í„´ê³¼ ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ ì‹œê°í™” ìƒì„±
    
    Args:
        model: SCSSystem ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        output_dir: ì‹œê°í™” íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    try:
        vis_dir = output_dir / "visualizations"
        vis_dir.mkdir(exist_ok=True)
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©
        first_batch = next(iter(test_loader))
        input_tokens = first_batch['input_tokens'][:1].to(model.device)  # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ
        attention_mask = first_batch.get('attention_mask')
        if attention_mask is not None:
            attention_mask = attention_mask[:1].to(model.device)
        
        # ìŠ¤íŒŒì´í¬ íŒ¨í„´ ìˆ˜ì§‘ì„ ìœ„í•œ ëª¨ë¸ ì‹¤í–‰
        model.eval()
        with torch.no_grad():
            # ìŠ¤íŒŒì´í¬ íŒ¨í„´ ìˆ˜ì§‘
            all_spike_patterns = _collect_spike_patterns(
                model, input_tokens, attention_mask
            )
        
        # ë…¸ë“œ ì´ë¦„ ì¶”ì¶œ
        node_names = list(all_spike_patterns[0].keys()) if all_spike_patterns else []
        
        if not node_names:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ìŠ¤íŒŒì´í¬ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 1. CLKë³„ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ì´ë¯¸ì§€ ìƒì„±
        _generate_spike_pattern_images(all_spike_patterns, node_names, vis_dir)
        
        # 2. ìŠ¤íŒŒì´í¬ íŒ¨í„´ GIF ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        _generate_spike_animation(all_spike_patterns, node_names, vis_dir)
        
        # 3. ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ ìƒì„±
        _generate_weight_heatmaps(model, node_names, vis_dir)
        
        # 4. ì²˜ë¦¬ ì •ë³´ ì‹œê°í™” (ìƒˆë¡œìš´ ê¸°ëŠ¥)
        _generate_processing_info_plots(all_spike_patterns, vis_dir)
        
        logger.info(f"ğŸ“ ëª¨ë“  ì‹œê°í™” íŒŒì¼ ì €ì¥ ì™„ë£Œ: {vis_dir}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        import traceback
        logger.debug(traceback.format_exc())


def _collect_spike_patterns(
    model, 
    input_tokens: torch.Tensor, 
    attention_mask: Optional[torch.Tensor]
) -> List[Dict[str, np.ndarray]]:
    """
    SCSSystem v2.0ì—ì„œ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ìˆ˜ì§‘
    
    Args:
        model: SCSSystem ëª¨ë¸
        input_tokens: [1, seq_len] ì…ë ¥ í† í°
        attention_mask: [1, seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
        
    Returns:
        all_spike_patterns: CLKë³„ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    """
    # ëª¨ë¸ ìƒíƒœ ì´ˆê¸°í™”
    model.reset_state(batch_size=1)
    
    all_spike_patterns = []
    max_clk = min(100, model.max_clk)  # ì‹œê°í™”ìš©ìœ¼ë¡œ ì œí•œ
    
    logger.info(f"ğŸ” ìŠ¤íŒŒì´í¬ íŒ¨í„´ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_clk} CLK)")
    
    for clk in range(max_clk):
        try:
            # Phase 1: ìŠ¤íŒŒì´í¬ ê³„ì‚° ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            current_spikes = model._compute_spikes()
            external_input = model._get_external_input_at_clk(
                input_tokens, clk, attention_mask
            )
            model._update_states(external_input, current_spikes)
            final_acc_spikes = current_spikes.get(model.acc_node)
            
            # í˜„ì¬ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ì €ì¥
            if current_spikes:
                spike_pattern = {}
                for node_name, spikes in current_spikes.items():
                    if spikes is not None:
                        spike_pattern[node_name] = spikes[0].cpu().numpy()  # [H, W]
                all_spike_patterns.append(spike_pattern)
            
            # Phase 2: TimingManager ì—…ë°ì´íŠ¸
            model.timing_manager.step(
                current_clk=clk,
                acc_node_spikes=final_acc_spikes,
                training=False,
                input_seq_len=input_tokens.shape[1],
                target_seq_len=input_tokens.shape[1]  # ì¶”ë¡  ëª¨ë“œ
            )
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if model.timing_manager.all_ended:
                logger.info(f"â¹ï¸ CLK {clk}ì—ì„œ ì²˜ë¦¬ ì™„ë£Œ (ì¡°ê¸° ì¢…ë£Œ)")
                break
                
        except Exception as e:
            logger.warning(f"âš ï¸ CLK {clk} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            break
    
    logger.info(f"âœ… ì´ {len(all_spike_patterns)}ê°œ CLKì˜ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_spike_patterns


def _generate_spike_pattern_images(
    all_spike_patterns: List[Dict[str, np.ndarray]], 
    node_names: List[str], 
    vis_dir: Path
):
    """CLKë³„ ìŠ¤íŒŒì´í¬ íŒ¨í„´ ì´ë¯¸ì§€ ìƒì„±"""
    spike_dir = vis_dir / "spike_patterns"
    spike_dir.mkdir(exist_ok=True)
    
    num_nodes = len(node_names)
    
    for clk, spike_pattern in enumerate(all_spike_patterns):
        fig, axes = plt.subplots(1, num_nodes, figsize=(4*num_nodes, 4))
        if num_nodes == 1:
            axes = [axes]
        
        for i, node_name in enumerate(node_names):
            if node_name in spike_pattern:
                spikes = spike_pattern[node_name]
                im = axes[i].imshow(spikes, cmap='gray', vmin=0, vmax=1)
                axes[i].set_title(f'{node_name}\nCLK {clk}')
                axes[i].set_xlabel('Width')
                axes[i].set_ylabel('Height')
                plt.colorbar(im, ax=axes[i])
            else:
                axes[i].text(0.5, 0.5, f'{node_name}\nNo Data', 
                           transform=axes[i].transAxes, ha='center', va='center')
                axes[i].set_title(f'{node_name}\nCLK {clk}')
        
        plt.tight_layout()
        plt.savefig(spike_dir / f"clk_{clk:03d}.png", dpi=100, bbox_inches='tight')
        plt.close()
    
    logger.info(f"âœ… ìŠ¤íŒŒì´í¬ íŒ¨í„´ ì´ë¯¸ì§€ {len(all_spike_patterns)}ê°œ ì €ì¥: {spike_dir}")


def _generate_spike_animation(
    all_spike_patterns: List[Dict[str, np.ndarray]], 
    node_names: List[str], 
    vis_dir: Path
):
    """ìŠ¤íŒŒì´í¬ íŒ¨í„´ GIF ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
    try:
        if not all_spike_patterns:
            logger.warning("âš ï¸ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±ì„ ìœ„í•œ ìŠ¤íŒŒì´í¬ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        num_nodes = len(node_names)
        fig, axes = plt.subplots(1, num_nodes, figsize=(4*num_nodes, 4))
        if num_nodes == 1:
            axes = [axes]
        
        # ì´ˆê¸° í”Œë¡¯ ì„¤ì •
        ims = []
        for i, node_name in enumerate(node_names):
            if node_name in all_spike_patterns[0]:
                initial_data = all_spike_patterns[0][node_name]
            else:
                initial_data = np.zeros((64, 64))  # ê¸°ë³¸ í¬ê¸°
            
            im = axes[i].imshow(initial_data, cmap='gray', vmin=0, vmax=1)
            axes[i].set_title(f'{node_name}\nCLK 0')
            axes[i].set_xlabel('Width')
            axes[i].set_ylabel('Height')
            plt.colorbar(im, ax=axes[i])
            ims.append(im)
        
        def animate(frame):
            spike_pattern = all_spike_patterns[frame]
            for i, (node_name, im) in enumerate(zip(node_names, ims)):
                if node_name in spike_pattern:
                    im.set_array(spike_pattern[node_name])
                axes[i].set_title(f'{node_name}\nCLK {frame}')
            return ims
        
        # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        anim = animation.FuncAnimation(
            fig, animate, frames=len(all_spike_patterns),
            interval=200, blit=True, repeat=True
        )
        
        # GIF ì €ì¥
        gif_path = vis_dir / "spike_animation.gif"
        anim.save(gif_path, writer='pillow', fps=5)
        plt.close()
        
        logger.info(f"ğŸ¬ ìŠ¤íŒŒì´í¬ íŒ¨í„´ GIF ìƒì„±: {gif_path}")
        
    except Exception as gif_error:
        logger.warning(f"âš ï¸ GIF ìƒì„± ì¤‘ ì˜¤ë¥˜ (ê°œë³„ ì´ë¯¸ì§€ëŠ” ì •ìƒ ì €ì¥ë¨): {gif_error}")


def _generate_weight_heatmaps(model, node_names: List[str], vis_dir: Path):
    """ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ ìƒì„±"""
    weight_dir = vis_dir / "weight_heatmaps"
    weight_dir.mkdir(exist_ok=True)
    
    num_nodes = len(node_names)
    
    # 1. ë…¸ë“œë³„ influence ê°€ì¤‘ì¹˜
    fig, axes = plt.subplots(1, num_nodes, figsize=(4*num_nodes, 4))
    if num_nodes == 1:
        axes = [axes]
    
    for i, node_name in enumerate(node_names):
        if node_name in model.nodes:
            node = model.nodes[node_name]
            influence = node.influence_strength.detach().cpu().numpy()
            
            im = axes[i].imshow(influence, cmap='RdBu_r', vmin=-2, vmax=2)
            axes[i].set_title(f'{node_name}\nInfluence Strength')
            axes[i].set_xlabel('Width')
            axes[i].set_ylabel('Height')
            plt.colorbar(im, ax=axes[i])
        else:
            axes[i].text(0.5, 0.5, f'{node_name}\nNo Data', 
                       transform=axes[i].transAxes, ha='center', va='center')
            axes[i].set_title(f'{node_name}\nInfluence Strength')
    
    plt.tight_layout()
    plt.savefig(weight_dir / "node_influence_weights.png", dpi=100, bbox_inches='tight')
    plt.close()
    
    # 2. ì¶•ì‚­ ì—°ê²° ê°€ì¤‘ì¹˜ (AxonalConnections)
    _visualize_axonal_connections(model.axonal_connections, weight_dir)
    
    logger.info(f"âœ… ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ ì €ì¥: {weight_dir}")


def _visualize_axonal_connections(axonal_connections, weight_dir: Path):
    """ì¶•ì‚­ ì—°ê²° ê°€ì¤‘ì¹˜ ì‹œê°í™” - í†µí•©ëœ GatesÃ—Transforms íˆíŠ¸ë§µ"""
    try:
        if not hasattr(axonal_connections, 'patch_gates'):
            logger.warning("AxonalConnectionsì— patch_gatesê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        patch_gates = axonal_connections.patch_gates
        patch_transforms = axonal_connections.patch_transforms
        
        # í†µí•© íˆíŠ¸ë§µ ìƒì„±
        _visualize_integrated_axonal_heatmaps(patch_gates, patch_transforms, weight_dir)
        
        # ê¸°ì¡´ í†µê³„ ì‹œê°í™”ë„ ìœ ì§€
        if patch_transforms:
            _visualize_patch_transform_stats(patch_transforms, weight_dir)
            
    except Exception as e:
        logger.warning(f"ì¶•ì‚­ ì—°ê²° ê°€ì¤‘ì¹˜ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")


def _visualize_integrated_axonal_heatmaps(
    patch_gates: Dict[str, torch.Tensor], 
    patch_transforms: Dict[str, torch.Tensor], 
    weight_dir: Path
):
    """í†µí•©ëœ ì¶•ì‚­ ì—°ê²° íˆíŠ¸ë§µ - Gatesì™€ Transformsë¥¼ íŒ¨ì¹˜ ê²©ì ìœ„ì¹˜ì— í•¨ê»˜ í‘œì‹œ"""
    try:
        if not patch_gates or not patch_transforms:
            return
        
        # ê³µí†µ ì—°ê²°ë§Œ ì²˜ë¦¬
        common_connections = set(patch_gates.keys()) & set(patch_transforms.keys())
        num_connections = min(4, len(common_connections))
        
        if num_connections > 0:
            cols = min(2, num_connections)
            rows = (num_connections + cols - 1) // cols
            
            fig, axes = plt.subplots(rows, cols, figsize=(8*cols, 8*rows))
            if rows == 1 and cols == 1:
                axes = [axes]
            elif rows == 1 or cols == 1:
                axes = axes.flatten()
            else:
                axes = axes.flatten()
            
            for i, conn_name in enumerate(list(common_connections)[:num_connections]):
                gates = patch_gates[conn_name].detach().cpu().numpy()
                transforms = patch_transforms[conn_name].detach().cpu().numpy()
                
                # transforms: [num_patches, target_size, source_size]
                num_patches, target_size, source_size = transforms.shape
                
                # íŒ¨ì¹˜ ê²©ì í¬ê¸° ê³„ì‚°
                patches_per_row = int(np.ceil(np.sqrt(num_patches)))
                patches_per_col = int(np.ceil(num_patches / patches_per_row))
                
                # í†µí•© íˆíŠ¸ë§µ í¬ê¸°: ê° íŒ¨ì¹˜ëŠ” transform + gate overlay
                cell_size = max(target_size, source_size)  # ì •ì‚¬ê°í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°
                total_height = patches_per_col * cell_size
                total_width = patches_per_row * cell_size
                
                integrated_heatmap = np.zeros((total_height, total_width))
                
                for patch_idx in range(num_patches):
                    row_idx = patch_idx // patches_per_row
                    col_idx = patch_idx % patches_per_row
                    
                    # íŒ¨ì¹˜ ìœ„ì¹˜ ê³„ì‚°
                    start_row = row_idx * cell_size
                    end_row = start_row + cell_size
                    start_col = col_idx * cell_size
                    end_col = start_col + cell_size
                    
                    # Transform í‰ê· ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                    patch_transform_mean = transforms[patch_idx].mean()
                    integrated_heatmap[start_row:end_row, start_col:end_col] = patch_transform_mean
                    
                    # Gate ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ (ê²Œì´íŠ¸ê°€ ê°•í• ìˆ˜ë¡ ë” ë°ê²Œ)
                    gate_value = gates[patch_idx]
                    integrated_heatmap[start_row:end_row, start_col:end_col] *= gate_value
                    
                    # ì¤‘ì•™ì— ì‹¤ì œ transform íŒ¨í„´ ì˜¤ë²„ë ˆì´ (ì‘ì€ ê²½ìš°ë§Œ)
                    if target_size <= cell_size and source_size <= cell_size:
                        # ì¤‘ì•™ ì •ë ¬
                        center_start_row = start_row + (cell_size - target_size) // 2
                        center_end_row = center_start_row + target_size
                        center_start_col = start_col + (cell_size - source_size) // 2
                        center_end_col = center_start_col + source_size
                        
                        # ì‹¤ì œ transform ê°’ ì ìš©
                        integrated_heatmap[center_start_row:center_end_row, 
                                         center_start_col:center_end_col] = transforms[patch_idx] * gate_value
                
                # íˆíŠ¸ë§µ í‘œì‹œ
                im = axes[i].imshow(integrated_heatmap, cmap='RdYlBu_r', aspect='auto')
                axes[i].set_title(f'{conn_name}\nIntegrated GatesÃ—Transforms\n({num_patches} patches)')
                axes[i].set_xlabel('Source Dimension')
                axes[i].set_ylabel('Target Dimension')
                
                # íŒ¨ì¹˜ ê²½ê³„ì„  ê·¸ë¦¬ê¸°
                for p in range(1, patches_per_row):
                    axes[i].axvline(x=p * cell_size - 0.5, color='black', linewidth=1, alpha=0.8)
                for p in range(1, patches_per_col):
                    axes[i].axhline(y=p * cell_size - 0.5, color='black', linewidth=1, alpha=0.8)
                
                # ê° íŒ¨ì¹˜ì— ê²Œì´íŠ¸ ê°’ í…ìŠ¤íŠ¸ í‘œì‹œ
                for patch_idx in range(num_patches):
                    row_idx = patch_idx // patches_per_row
                    col_idx = patch_idx % patches_per_row
                    
                    text_row = row_idx * cell_size + cell_size // 2
                    text_col = col_idx * cell_size + cell_size // 2
                    
                    axes[i].text(text_col, text_row, f'{gates[patch_idx]:.2f}', 
                               ha='center', va='center', fontsize=10, 
                               color='white', fontweight='bold',
                               bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7))
                
                plt.colorbar(im, ax=axes[i])
            
            # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
            for j in range(i+1, len(axes)):
                axes[j].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(weight_dir / "axonal_integrated_heatmap.png", dpi=100, bbox_inches='tight')
            plt.close()
            
            logger.info("í†µí•© ì¶•ì‚­ ì—°ê²° íˆíŠ¸ë§µ ìƒì„± ì™„ë£Œ")
            
    except Exception as e:
        logger.warning(f"í†µí•© ì¶•ì‚­ íˆíŠ¸ë§µ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")


def _visualize_patch_transform_stats(patch_transforms, weight_dir: Path):
    """Patch Transform í–‰ë ¬ì˜ í†µê³„ ì‹œê°í™”"""
    try:
        # ê° ì—°ê²°ë³„ transform í–‰ë ¬ì˜ í†µê³„ ê³„ì‚°
        conn_stats = {}
        
        for conn_name, transforms in patch_transforms.items():
            transforms_np = transforms.detach().cpu().numpy()
            
            # í†µê³„ ê³„ì‚°
            stats = {
                'mean': np.mean(transforms_np),
                'std': np.std(transforms_np),
                'min': np.min(transforms_np),
                'max': np.max(transforms_np),
                'shape': transforms_np.shape
            }
            conn_stats[conn_name] = stats
        
        # í†µê³„ ì‹œê°í™”
        if conn_stats:
            conn_names = list(conn_stats.keys())
            metrics = ['mean', 'std', 'min', 'max']
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            axes = axes.flatten()
            
            for i, metric in enumerate(metrics):
                values = [conn_stats[conn][metric] for conn in conn_names]
                bars = axes[i].bar(range(len(conn_names)), values)
                axes[i].set_title(f'Patch Transform {metric.capitalize()}')
                axes[i].set_xlabel('Connection')
                axes[i].set_ylabel(metric.capitalize())
                axes[i].set_xticks(range(len(conn_names)))
                axes[i].set_xticklabels([name.split('_to_')[0][:8] + 'â†’' + name.split('_to_')[1][:8] 
                                       for name in conn_names], rotation=45)
                
                # ê°’ í‘œì‹œ
                for bar, value in zip(bars, values):
                    axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                               f'{value:.3f}', ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            plt.savefig(weight_dir / "patch_transform_stats.png", dpi=100, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Patch Transform í†µê³„ ì‹œê°í™” ì™„ë£Œ")
            
    except Exception as e:
        logger.warning(f"âš ï¸ Patch Transform í†µê³„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")


def _generate_processing_info_plots(all_spike_patterns: List[Dict[str, np.ndarray]], vis_dir: Path):
    """ì²˜ë¦¬ ì •ë³´ ì‹œê°í™” (ìƒˆë¡œìš´ ê¸°ëŠ¥)"""
    try:
        if not all_spike_patterns:
            return
        
        info_dir = vis_dir / "processing_info"
        info_dir.mkdir(exist_ok=True)
        
        node_names = list(all_spike_patterns[0].keys())
        num_clks = len(all_spike_patterns)
        
        # 1. CLKë³„ ë…¸ë“œ í™œì„±ë„ ë³€í™”
        node_activities = {node: [] for node in node_names}
        
        for spike_pattern in all_spike_patterns:
            for node_name in node_names:
                if node_name in spike_pattern:
                    activity = np.mean(spike_pattern[node_name])
                    node_activities[node_name].append(activity)
                else:
                    node_activities[node_name].append(0.0)
        
        # í™œì„±ë„ í”Œë¡¯
        plt.figure(figsize=(12, 6))
        for node_name, activities in node_activities.items():
            plt.plot(activities, label=node_name, linewidth=2)
        
        plt.xlabel('CLK')
        plt.ylabel('Average Spike Rate')
        plt.title('Node Activity Over Time')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(info_dir / "node_activity_timeline.png", dpi=100, bbox_inches='tight')
        plt.close()
        
        # 2. ì´ ìŠ¤íŒŒì´í¬ ìˆ˜ ë³€í™”
        total_spikes = []
        for spike_pattern in all_spike_patterns:
            total = sum(np.sum(spikes) for spikes in spike_pattern.values())
            total_spikes.append(total)
        
        plt.figure(figsize=(10, 5))
        plt.plot(total_spikes, linewidth=2, color='red')
        plt.xlabel('CLK')
        plt.ylabel('Total Spikes')
        plt.title('Total Spike Count Over Time')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(info_dir / "total_spikes_timeline.png", dpi=100, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ… ì²˜ë¦¬ ì •ë³´ ì‹œê°í™” ì €ì¥: {info_dir}")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì²˜ë¦¬ ì •ë³´ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")


def generate_quick_visualization(
    model, 
    input_tokens: torch.Tensor, 
    attention_mask: Optional[torch.Tensor] = None,
    max_clks: int = 50
) -> Dict[str, Any]:
    """
    ë¹ ë¥¸ ì‹œê°í™”ìš© í•¨ìˆ˜ - íŒŒì¼ ì €ì¥ ì—†ì´ ë°ì´í„°ë§Œ ë°˜í™˜
    
    Args:
        model: SCSSystem ëª¨ë¸
        input_tokens: [B, seq_len] ì…ë ¥ í† í°
        attention_mask: [B, seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
        max_clks: ìµœëŒ€ CLK ìˆ˜
        
    Returns:
        visualization_data: ì‹œê°í™” ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    model.eval()
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‚¬ìš©
    single_input = input_tokens[:1]
    single_mask = attention_mask[:1] if attention_mask is not None else None
    
    with torch.no_grad():
        spike_patterns = _collect_spike_patterns(model, single_input, single_mask)
        
        # í™œì„±ë„ ê³„ì‚°
        if spike_patterns:
            node_names = list(spike_patterns[0].keys())
            activities = {node: [np.mean(pattern.get(node, np.zeros((1,1)))) 
                               for pattern in spike_patterns] 
                         for node in node_names}
        else:
            activities = {}
    
    return {
        'spike_patterns': spike_patterns,
        'node_activities': activities,
        'num_clks_processed': len(spike_patterns)
    }