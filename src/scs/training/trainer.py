# src/scs/training/trainer.py
"""
SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ
"""

import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from tqdm import tqdm
import logging
from pathlib import Path
from datetime import datetime

from .loss import SCSLoss
from .metric import SCSMetrics


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨"""
    epochs: int = 15
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    eval_every: int = 3
    save_every: int = 10
    early_stopping_patience: int = 20
    device: str = "cuda"
    max_clk_training: int = 250
    pad_token_id: int = 0
    use_scheduled_sampling: bool = False
    ss_start_prob: float = 1.0
    ss_end_prob: float = 0.05
    ss_decay_epochs: int = 10
    eta_min: float = 0.0
    use_curriculum_learning: bool = False
    curriculum_schedule: Optional[Dict[int, int]] = None

class SCSTrainer:
    """SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        config: TrainingConfig,
        loss_fn: Optional[SCSLoss] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        tokenizer: Optional[Any] = None,
        unfreezing_config: Optional[Dict] = None  # ìƒˆë¡œ ì¶”ê°€
    ):
        self.model = model
        self.config = config

        # ë¡œê¹…
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì†ì‹¤ í•¨ìˆ˜ (pad_token_id í¬í•¨)
        self.loss_fn = loss_fn or SCSLoss(pad_token_id=config.pad_token_id)
        
        # ì ì§„ì  í•´ì œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ìƒˆë¡œ ì¶”ê°€)
        self.unfreezing_scheduler = None
        if unfreezing_config and unfreezing_config.get('enabled', False):
            frozen_patterns = unfreezing_config.get('initial_frozen_patterns', [])
            unfreeze_schedule = unfreezing_config.get('unfreeze_schedule', {})
            self.unfreezing_scheduler = GradualUnfreezingScheduler(
                model=self.model,
                frozen_patterns=frozen_patterns,
                unfreeze_schedule=unfreeze_schedule,
                logger=self.logger
            )
        
        # ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optimizer or torch.optim.Adam(
            # unfreezing_schedulerê°€ ìˆìœ¼ë©´ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ, ì—†ìœ¼ë©´ ì „ì²´
            filter(lambda p: p.requires_grad, model.parameters()) if self.unfreezing_scheduler else model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        self.scheduler = scheduler
        self.tokenizer = tokenizer
        
        self.device = config.device
        self.model.to(self.device)
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.best_model_path = None  # ìµœê³  ëª¨ë¸ ê²½ë¡œ ì¶”ê°€

        self.current_ss_prob = self.config.ss_start_prob
    
    def _update_scheduled_sampling_prob(self):
        """í˜„ì¬ ì—í¬í¬ì— ë§ì¶° ìŠ¤ì¼€ì¤„ ìƒ˜í”Œë§ í™•ë¥ (epsilon)ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if not self.config.use_scheduled_sampling:
            self.current_ss_prob = 1.0 # ì‚¬ìš© ì•ˆ í•  ì‹œ í•­ìƒ Teacher Forcing
            return

        # ì„ í˜• ê°ì†Œ(Linear Decay) ìŠ¤ì¼€ì¤„
        decay_epochs = self.config.ss_decay_epochs
        start_prob = self.config.ss_start_prob
        end_prob = self.config.ss_end_prob
        
        if self.current_epoch >= decay_epochs:
            self.current_ss_prob = end_prob
        else:
            # í˜„ì¬ ì—í¬í¬ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ë¥ ì„ ê°ì†Œì‹œí‚´
            self.current_ss_prob = start_prob - (start_prob - end_prob) * (self.current_epoch / decay_epochs)
        
        # self.loggerê°€ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ë¡œê¹…
        if hasattr(self, 'logger'):
            self.logger.info(f"Scheduled Sampling í™•ë¥ (epsilon) ì—…ë°ì´íŠ¸: {self.current_ss_prob:.4f}")

    def _update_curriculum_max_clk(self, epoch: int):
        """ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: í˜„ì¬ ì—í¬í¬ì— ë§ëŠ” max_clk ì„¤ì •"""
        schedule = self.config.curriculum_schedule
        sorted_schedule = sorted(schedule.items())
        
        # í˜„ì¬ ì—í¬í¬ì— ì ìš©í•  max_clk ì°¾ê¸°
        current_max_clk = self.config.max_clk_training  # ê¸°ë³¸ê°’
        for start_epoch, max_clk in sorted_schedule:
            if epoch >= start_epoch:
                current_max_clk = max_clk
        
        # max_clkê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
        if current_max_clk != self.config.max_clk_training:
            old_max_clk = self.config.max_clk_training
            self.config.max_clk_training = current_max_clk
            
            # loss_fnì˜ max_clkë„ ì—…ë°ì´íŠ¸
            if hasattr(self.loss_fn, 'update_max_clk'):
                self.loss_fn.update_max_clk(current_max_clk)
            
            self.logger.info(f"ğŸ“š ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: ì—í¬í¬ {epoch}, max_clk {old_max_clk} â†’ {current_max_clk}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, List[float]]:
        """í•™ìŠµ ì‹¤í–‰"""
        
        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }
        
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ í•™ìŠµ ì‹œì‘: {self.config.epochs} ì—í¬í¬")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_path:
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch
            
            # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: max_clk ë™ì  ì¡°ì •
            if self.config.use_curriculum_learning and self.config.curriculum_schedule:
                self._update_curriculum_max_clk(epoch)
            
            # ì ì§„ì  í•´ì œ ì ìš©
            if self.unfreezing_scheduler:
                optimizer_needs_update = self.unfreezing_scheduler.step(epoch)
                if optimizer_needs_update:
                    # ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± (ìƒˆë¡œ í•´ì œëœ íŒŒë¼ë¯¸í„° í¬í•¨)
                    self.logger.info("ğŸ“ ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± - ìƒˆë¡œ í•´ì œëœ íŒŒë¼ë¯¸í„° í¬í•¨")
                    self.optimizer = torch.optim.AdamW(
                        filter(lambda p: p.requires_grad, self.model.parameters()),
                        lr=self.config.learning_rate,
                        weight_decay=self.config.weight_decay
                    )
                    # ìŠ¤ì¼€ì¤„ëŸ¬ë„ ì¬ìƒì„± (ìˆëŠ” ê²½ìš°)
                    if self.scheduler:
                        eta_min = getattr(self.config, 'eta_min', 0.0)
                        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                            self.optimizer, 
                            T_max=self.config.epochs - epoch,
                            eta_min=eta_min
                        )

            self._update_scheduled_sampling_prob()
            
            # í•™ìŠµ
            train_metrics = self._train_epoch(train_loader)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ì—í¬í¬ë§ˆë‹¤ í˜¸ì¶œ - í‘œì¤€ì ì¸ CosineAnnealingLR ì‚¬ìš©ë²•)
            if self.scheduler:
                self.scheduler.step()
            
            # ê²€ì¦
            if val_loader and epoch % self.config.eval_every == 0:
                val_metrics = self._validate_epoch(val_loader)
                history['val_loss'].append(val_metrics['loss'])
                history['val_accuracy'].append(val_metrics['accuracy'])
                
                # ìµœê³  ëª¨ë¸ ì €ì¥ (validation loss ê¸°ì¤€)
                if save_path and val_metrics['loss'] < self.best_loss:
                    self.best_loss = val_metrics['loss']
                    self.best_model_path = self._save_best_model(save_path, epoch, val_metrics['loss'])
                    self.patience_counter = 0
                    self.logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                else:
                    self.patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self._should_early_stop(val_metrics['loss']):
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch}")
                    break
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if save_path and epoch % self.config.save_every == 0:
                self._save_checkpoint(save_path, epoch)
            
            # ë¡œê¹…
            self._log_progress(epoch, train_metrics, 
                             val_metrics if val_loader and epoch % self.config.eval_every == 0 else None)
        
        # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ìµœê³  ëª¨ë¸ì´ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥
        if save_path and self.best_model_path is None:
            self.best_model_path = self._save_best_model(save_path, self.current_epoch, self.best_loss)
            self.logger.info(f"ìµœì¢… ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥: {self.best_model_path}")
        
        return history
    
    def _save_best_model(self, save_path: str, epoch: int, loss: float) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        best_model_path = f"{save_path}/best_model.pt"
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ì „ì²´ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
            'eta_min': self.config.eta_min,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss,
            'training_config_dict': training_config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
        torch.save(checkpoint, best_model_path)
        return best_model_path

    def _save_checkpoint(self, save_path: str, epoch: int):
        """ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (í™•ì¥ëœ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            # Scheduled Sampling ì •ë³´ ì¶”ê°€
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
            'eta_min': self.config.eta_min,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'training_config_dict': training_config_dict,  # í•™ìŠµ ì„¤ì •
            'model_config': getattr(self.model, 'config', None),  # ëª¨ë¸ ìì²´ ì„¤ì •
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),  # ì €ì¥ ì‹œê°„
            'current_ss_prob': getattr(self, 'current_ss_prob', None)  # í˜„ì¬ SS í™•ë¥  ì €ì¥
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, f"{save_path}/checkpoint_epoch_{epoch}.pt")
    
    def _log_progress(self, epoch: int, train_metrics: Dict[str, float], val_metrics: Optional[Dict[str, float]] = None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        log_msg = f"ì—í¬í¬ {epoch}: í›ˆë ¨ ì†ì‹¤={train_metrics['loss']:.4f}, í›ˆë ¨ ì •í™•ë„={train_metrics['accuracy']:.4f}"
        
        if val_metrics:
            log_msg += f", ê²€ì¦ ì†ì‹¤={val_metrics['loss']:.4f}, ê²€ì¦ ì •í™•ë„={val_metrics['accuracy']:.4f}"
            if val_metrics['loss'] < self.best_loss:
                log_msg += " â­"
        
        self.logger.info(log_msg)
    
    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ë°°ì¹˜ í•™ìŠµ"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch in progress_bar:
            # ë°°ì¹˜ ì²˜ë¦¬ (for ë£¨í”„ ì œê±°!)
            batch_loss, batch_metrics = self._train_batch(batch)
            
            total_loss += batch_loss
            total_accuracy += batch_metrics['accuracy']
            num_batches += 1
            
            progress_bar.set_postfix({
                'loss': f"{batch_loss:.4f}",
                'acc': f"{batch_metrics['accuracy']:.4f}"
            })
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _train_batch(self, batch: Dict[str, torch.Tensor]) -> tuple:
        """ì§„ì •í•œ ë°°ì¹˜ í•™ìŠµ - GPU ë³‘ë ¬ ì²˜ë¦¬ í™œìš©"""
        # 1. ë°ì´í„° ì¤€ë¹„ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_tokens = batch['input_tokens'].to(self.device)
        target_tokens = batch['target_tokens'].to(self.device) 
        attention_mask = batch['attention_mask'].to(self.device)
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # 3. Forward Pass (ëª¨ë¸ì— ë°°ì¹˜ ì „ì²´ë¥¼ ì „ë‹¬)
        # ëª¨ë¸ì˜ forwardëŠ” ë‚´ë¶€ì ìœ¼ë¡œ CLK ë£¨í”„ë¥¼ ëŒê³  ìµœì¢… ë¡œì§“ [B, seq_len, vocab_size]ë¥¼ ë°˜í™˜
        output_logits, processing_info = self.model(
            input_schedule=input_tokens,
            max_clk=self.config.max_clk_training,  # YAML ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ê³ ì • CLK
            training=True,
            target_schedule=target_tokens,
            attention_mask=attention_mask,
            ss_prob=self.current_ss_prob
        )
        
        # 4. ì†ì‹¤ ê³„ì‚° (ìˆ˜ì •ëœ loss_fn ì‚¬ìš©)
        loss = self.loss_fn(output_logits, target_tokens, processing_info)
        
        # 5. Backward Pass (ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        loss.backward()
        
        # 6. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.config.gradient_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_norm
            )
        
        # 7. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        self.optimizer.step()
        # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì—í¬í¬ë§ˆë‹¤ í˜¸ì¶œ (í‘œì¤€ì ì¸ CosineAnnealingLR ì‚¬ìš©ë²•)
        
        # 8. ë©”íŠ¸ë¦­ ê³„ì‚° (ì •í™•ë„)
        with torch.no_grad():
            accuracy = SCSMetrics.accuracy(
                output_logits, 
                target_tokens, 
                pad_token_id=self.config.pad_token_id
            )
            
        return loss.item(), {'accuracy': accuracy}
    
    
    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """ê²€ì¦ - Teacher Forcingìœ¼ë¡œ ê³µì •í•œ ë¹„êµ"""
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask
                    # ss_prob íŒŒë¼ë¯¸í„° ì œê±° - inference ëª¨ë“œì—ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
                )
                
                batch_loss = self.loss_fn(output_logits, target_tokens, processing_info)
                batch_accuracy = SCSMetrics.accuracy(
                    output_logits, 
                    target_tokens, 
                    pad_token_id=self.config.pad_token_id
                )
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _should_early_stop(self, val_loss: float) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ íŒë‹¨"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.config.early_stopping_patience
        
    def evaluate(self, test_loader: DataLoader, save_examples: int = 10) -> Dict[str, Any]:
        """
        ê¸°ì¡´ evaluate ë©”ì„œë“œ ìˆ˜ì • - ë°°ì¹˜ ì²˜ë¦¬ í›„ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„í•´
        """
        self.model.eval()
        
        all_sample_results = []
        saved_examples = []
        total_samples = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                batch_size = batch['input_tokens'].shape[0]
                
                # ğŸš€ ë°°ì¹˜ ì „ì²´ë¥¼ ëª¨ë¸ì— í•œ ë²ˆì— ì „ë‹¬ (ê¸°ì¡´ _evaluate_single_sample_inference ëŒ€ì‹ )
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ ì‹¤í–‰
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask
                )
                
                # ë°°ì¹˜ ê²°ê³¼ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„í•´
                for sample_idx in range(batch_size):
                    sample_result = self._extract_sample_result(
                        batch, output_logits, processing_info, sample_idx, total_samples
                    )
                    
                    all_sample_results.append(sample_result)
                    total_samples += 1
                    
                    if len(saved_examples) < save_examples:
                        saved_examples.append(sample_result)

        # ... ê¸°ì¡´ ê²°ê³¼ ì§‘ê³„ ì½”ë“œ ê·¸ëŒ€ë¡œ ...
        print(f"\n=== ì „ì²´ {total_samples}ê°œ ìƒ˜í”Œ ê²°ê³¼ ì§‘ê³„ ===")
        
        total_accuracy = sum(result['accuracy'] for result in all_sample_results) / len(all_sample_results)
        losses = [result['loss'] for result in all_sample_results if result['loss'] is not None]
        avg_loss = sum(losses) / len(losses) if losses else 0.0
        convergence_rate = sum(result['convergence_achieved'] for result in all_sample_results) / len(all_sample_results)
        avg_processing_clk = sum(result['processing_clk'] for result in all_sample_results if isinstance(result['processing_clk'], (int, float))) / len(all_sample_results)
        avg_tokens_generated = sum(result['tokens_generated'] for result in all_sample_results if isinstance(result['tokens_generated'], (int, float))) / len(all_sample_results)
        processing_efficiency = max(0.0, 1.0 - (avg_processing_clk / self.config.max_clk_training))
        comprehensive_score = (
            0.4 * convergence_rate +
            0.3 * processing_efficiency +
            0.2 * min(1.0, (avg_tokens_generated / 10.0)) +
            0.1 * total_accuracy
        )
        
        results = {
            'test_accuracy': total_accuracy,
            'test_loss': avg_loss,
            'comprehensive_score': comprehensive_score,
            'convergence_rate': convergence_rate,
            'processing_efficiency': processing_efficiency,
            'examples': saved_examples,
            'num_examples_saved': len(saved_examples),
            'total_batches_evaluated': len(set(result.get('batch_idx', 0) for result in all_sample_results))
        }
        
        print(f"ìµœì¢… ê²°ê³¼: ì •í™•ë„={total_accuracy:.4f}, ì¢…í•©ì ìˆ˜={comprehensive_score:.4f}")
        return results

    def _extract_sample_result(
        self,
        batch: Dict[str, torch.Tensor],
        output_logits: torch.Tensor,  # [B, seq_len, vocab_size]
        processing_info: Dict[str, Any],
        sample_idx: int,
        global_idx: int
    ) -> Dict[str, Any]:
        """ë°°ì¹˜ ê²°ê³¼ì—ì„œ ê°œë³„ ìƒ˜í”Œ ê²°ê³¼ ì¶”ì¶œ (ê¸°ì¡´ _evaluate_single_sample_inference ëŒ€ì²´)"""
        try:
            # í…ìŠ¤íŠ¸ ë³µì›
            input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
            target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            
            # ìƒì„± ê²°ê³¼ ì¶”ì¶œ
            if output_logits.shape[1] > 0 and sample_idx < output_logits.shape[0]:
                generated_tokens = output_logits[sample_idx].argmax(dim=-1)
                generated_text = self._decode_tokens_to_text(generated_tokens)
            else:
                generated_tokens = torch.tensor([], dtype=torch.long)
                generated_text = "[ë¹ˆ ì¶œë ¥]"
            
            # ê°œë³„ ìƒ˜í”Œ ì •í™•ë„ ê³„ì‚°
            if output_logits.shape[1] > 0 and sample_idx < output_logits.shape[0]:
                try:
                    from scs.training.metric import SCSMetrics
                    accuracy = SCSMetrics.accuracy(
                        output_logits[sample_idx:sample_idx+1],
                        batch['target_tokens'][sample_idx:sample_idx+1].to(output_logits.device),
                        pad_token_id=self.config.pad_token_id
                    )
                except:
                    accuracy = self._calculate_sequence_accuracy_fallback(
                        generated_tokens, batch['target_tokens'][sample_idx]
                    )
            else:
                accuracy = 0.0
            
            # ì†ì‹¤ ê³„ì‚° (ë°°ì¹˜ í‰ê·  ì‚¬ìš©)
            loss = None
            if hasattr(self, 'loss_fn') and self.loss_fn is not None:
                try:
                    sample_target = batch['target_tokens'][sample_idx:sample_idx+1].to(output_logits.device)
                    loss = self.loss_fn(
                        output_logits[sample_idx:sample_idx+1], 
                        sample_target, 
                        processing_info
                    ).item()
                except:
                    loss = None
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': generated_text,
                'accuracy': accuracy,
                'processing_clk': processing_info.get('processing_clk', 'unknown'),
                'tokens_generated': processing_info.get('tokens_generated', 'unknown'),
                'convergence_achieved': processing_info.get('convergence_achieved', False),
                'batch_accuracy': accuracy,
                'generation_method': 'batch_inference',
                'loss': loss,
                'global_index': global_idx,
                'batch_idx': global_idx // batch['input_tokens'].shape[0],
            }
            
        except Exception as e:
            print(f"  ìƒ˜í”Œ {global_idx} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            try:
                input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
                target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            except:
                input_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
                target_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': "[ì¶”ì¶œ ì‹¤íŒ¨]",
                'accuracy': 0.0,
                'loss': None,
                'processing_clk': self.config.max_clk_training,
                'tokens_generated': 0,
                'convergence_achieved': False,
                'batch_accuracy': 0.0,
                'generation_method': 'error',
                'global_index': global_idx,
                'batch_idx': 0,
            }

    def _calculate_sequence_accuracy_fallback(self, generated_tokens: torch.Tensor, target_tokens: torch.Tensor) -> float:
        """ì‹œí€€ìŠ¤ ì •í™•ë„ ê³„ì‚° í´ë°± (SCSMetrics ì‚¬ìš© ì‹¤íŒ¨ ì‹œ)
        
        Args:
            generated_tokens: ìƒì„±ëœ í† í° ì‹œí€€ìŠ¤ [seq_len]
            target_tokens: ì •ë‹µ í† í° ì‹œí€€ìŠ¤ [seq_len]
            
        Returns:
            í† í°ë³„ ì •í™•ë„ (0.0 ~ 1.0)
        """
        try:
            # íŒ¨ë”© í† í° ì œê±°
            pad_token_id = self.config.pad_token_id
            
            # ì •ë‹µì—ì„œ ìœ íš¨í•œ í† í°ë§Œ ì¶”ì¶œ
            valid_target = target_tokens[target_tokens != pad_token_id]
            
            if len(valid_target) == 0:
                return 0.0
            
            # ìƒì„±ëœ í† í°ì„ ì •ë‹µ ê¸¸ì´ì— ë§ì¶¤
            if len(generated_tokens) >= len(valid_target):
                trimmed_generated = generated_tokens[:len(valid_target)]
            else:
                # ìƒì„±ì´ ë¶€ì¡±í•˜ë©´ íŒ¨ë”©ìœ¼ë¡œ ì±„ì›€
                padding = torch.full((len(valid_target) - len(generated_tokens),), pad_token_id, dtype=generated_tokens.dtype)
                trimmed_generated = torch.cat([generated_tokens, padding])
            
            # í† í°ë³„ ì •í™•ë„ ê³„ì‚°
            correct = (trimmed_generated == valid_target).float()
            accuracy = correct.mean().item()
            
            return accuracy
            
        except Exception as e:
            print(f"í´ë°± ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _decode_tokens_to_text(self, tokens: torch.Tensor) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if self.tokenizer is None:
            return f"tokens: {tokens.tolist()}"
        
        try:
            # íŒ¨ë”© í† í° ì œê±°
            if hasattr(self.tokenizer, 'tokenizer'):
                pad_token_id = self.tokenizer.tokenizer.pad_token_id
            else:
                pad_token_id = self.config.pad_token_id
                
            # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
            valid_tokens = tokens[tokens != pad_token_id]
            
            # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”©
            if hasattr(self.tokenizer, 'decode'):
                return self.tokenizer.decode(valid_tokens.tolist())
            elif hasattr(self.tokenizer, 'tokenizer'):
                return self.tokenizer.tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=True)
            else:
                return f"tokens: {valid_tokens.tolist()}"
                
        except Exception as e:
            return f"decode_error: {tokens.tolist()}"

class GradualUnfreezingScheduler:
    """ì ì§„ì  ì–¸í”„ë¦¬ì§• ìŠ¤ì¼€ì¤„ëŸ¬ - ë™ê²° íŒ¨í„´ ê¸°ë°˜"""
    
    def __init__(self, model, frozen_patterns: List[str], unfreeze_schedule: Dict[int, List[str]], logger=None):
        """
        Args:
            model: SCS ëª¨ë¸
            frozen_patterns: ì´ˆê¸°ì— ë™ê²°í•  íŒŒë¼ë¯¸í„° íŒ¨í„´ë“¤
            unfreeze_schedule: {epoch: [patterns]} í˜•íƒœì˜ í•´ì œ ìŠ¤ì¼€ì¤„
            logger: ë¡œê¹… ê°ì²´
        """
        self.model = model
        self.frozen_patterns = frozen_patterns
        self.unfreeze_schedule = unfreeze_schedule
        self.logger = logger
        self.current_epoch = -1
        self.unfrozen_patterns = set()  # ì´ë¯¸ í•´ì œëœ íŒ¨í„´ë“¤ ì¶”ì 
        
        # ì§€ì •ëœ íŒ¨í„´ë§Œ ë™ê²°
        if self.frozen_patterns:
            self._freeze_by_patterns(self.frozen_patterns)
            
        if self.logger:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            self.logger.info(f"ğŸ”’ ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒíƒœ: {trainable_params:,}/{total_params:,} í•™ìŠµ ê°€ëŠ¥")
    
    def _freeze_by_patterns(self, patterns: List[str]):
        """íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ ë™ê²°"""
        frozen_count = 0
        for name, param in self.model.named_parameters():
            for pattern in patterns:
                if name.startswith(pattern):
                    param.requires_grad = False
                    frozen_count += param.numel()
                    if self.logger:
                        self.logger.info(f"ğŸ”’ ë™ê²°: {name} ({param.numel():,} íŒŒë¼ë¯¸í„°)")
                    break
        
        if self.logger and frozen_count > 0:
            self.logger.info(f"ì´ {frozen_count:,}ê°œ íŒŒë¼ë¯¸í„° ë™ê²° ì™„ë£Œ")
    
    def _unfreeze_by_patterns(self, patterns: List[str]) -> bool:
        """íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ í•´ì œ"""
        newly_unfrozen = False
        unfrozen_count = 0
        
        for pattern in patterns:
            if pattern in self.unfrozen_patterns:
                continue  # ì´ë¯¸ í•´ì œëœ íŒ¨í„´ì€ ìŠ¤í‚µ
                
            for name, param in self.model.named_parameters():
                if name.startswith(pattern) and not param.requires_grad:
                    param.requires_grad = True
                    unfrozen_count += param.numel()
                    newly_unfrozen = True
                    if self.logger:
                        self.logger.info(f"ğŸ”“ í•´ì œ: {name} ({param.numel():,} íŒŒë¼ë¯¸í„°)")
            
            self.unfrozen_patterns.add(pattern)
        
        if self.logger and unfrozen_count > 0:
            self.logger.info(f"ì´ {unfrozen_count:,}ê°œ íŒŒë¼ë¯¸í„° í•´ì œ ì™„ë£Œ")
        
        return newly_unfrozen
    
    def step(self, epoch: int) -> bool:
        """
        ì—í¬í¬ ì§„í–‰ ì‹œ í˜¸ì¶œ. ìƒˆë¡œìš´ íŒ¨í„´ì´ í•´ì œë˜ë©´ True ë°˜í™˜
        
        Args:
            epoch: í˜„ì¬ ì—í¬í¬
            
        Returns:
            bool: ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„±ì´ í•„ìš”í•œì§€ ì—¬ë¶€
        """
        if epoch == self.current_epoch:
            return False
            
        self.current_epoch = epoch
        
        if epoch in self.unfreeze_schedule:
            if self.logger:
                self.logger.info(f"ğŸ“… ì—í¬í¬ {epoch}: ì ì§„ì  í•´ì œ ì‹¤í–‰")
            
            patterns_to_unfreeze = self.unfreeze_schedule[epoch]
            newly_unfrozen = self._unfreeze_by_patterns(patterns_to_unfreeze)
            
            if newly_unfrozen:
                # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í†µê³„ ì¶œë ¥
                total_params = sum(p.numel() for p in self.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                
                if self.logger:
                    self.logger.info(f"ğŸ“Š í˜„ì¬ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}/{total_params:,} "
                                   f"({100*trainable_params/total_params:.1f}%)")
                
                return True  # ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± í•„ìš”
        
        return False
    
    def get_unfrozen_patterns(self) -> List[str]:
        """í˜„ì¬ê¹Œì§€ í•´ì œëœ íŒ¨í„´ ëª©ë¡ ë°˜í™˜"""
        return list(self.unfrozen_patterns)
    
    def get_training_statistics(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'trainable_ratio': trainable_params / total_params if total_params > 0 else 0,
            'unfrozen_patterns': list(self.unfrozen_patterns),
            'frozen_patterns': self.frozen_patterns,
            'current_epoch': self.current_epoch
        }