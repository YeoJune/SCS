# src/scs/training/trainer.py
"""
SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ
"""

import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from tqdm import tqdm
import logging
from pathlib import Path
from datetime import datetime

from .loss import SCSLoss
from .metric import SCSMetrics


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨"""
    epochs: int = 15
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    eval_every: int = 3
    save_every: int = 10
    early_stopping_patience: int = 20
    device: str = "cuda"
    max_clk_training: int = 250
    pad_token_id: int = 0
    use_scheduled_sampling: bool = False
    ss_start_prob: float = 1.0
    ss_end_prob: float = 0.05
    ss_decay_epochs: int = 10

class SCSTrainer:
    """SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        config: TrainingConfig,
        loss_fn: Optional[SCSLoss] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        tokenizer: Optional[Any] = None
    ):
        self.model = model
        self.config = config
        
        # ì†ì‹¤ í•¨ìˆ˜ (pad_token_id í¬í•¨)
        self.loss_fn = loss_fn or SCSLoss(pad_token_id=config.pad_token_id)
        
        # ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optimizer or torch.optim.Adam(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        self.scheduler = scheduler
        self.tokenizer = tokenizer
        
        self.device = config.device
        self.model.to(self.device)
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.best_model_path = None  # ìµœê³  ëª¨ë¸ ê²½ë¡œ ì¶”ê°€

        self.current_ss_prob = self.config.ss_start_prob
        
        # ë¡œê¹…
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _update_scheduled_sampling_prob(self):
        """í˜„ì¬ ì—í¬í¬ì— ë§ì¶° ìŠ¤ì¼€ì¤„ ìƒ˜í”Œë§ í™•ë¥ (epsilon)ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if not self.config.use_scheduled_sampling:
            self.current_ss_prob = 1.0 # ì‚¬ìš© ì•ˆ í•  ì‹œ í•­ìƒ Teacher Forcing
            return

        # ì„ í˜• ê°ì†Œ(Linear Decay) ìŠ¤ì¼€ì¤„
        decay_epochs = self.config.ss_decay_epochs
        start_prob = self.config.ss_start_prob
        end_prob = self.config.ss_end_prob
        
        if self.current_epoch >= decay_epochs:
            self.current_ss_prob = end_prob
        else:
            # í˜„ì¬ ì—í¬í¬ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ë¥ ì„ ê°ì†Œì‹œí‚´
            self.current_ss_prob = start_prob - (start_prob - end_prob) * (self.current_epoch / decay_epochs)
        
        # self.loggerê°€ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ë¡œê¹…
        if hasattr(self, 'logger'):
            self.logger.info(f"Scheduled Sampling í™•ë¥ (epsilon) ì—…ë°ì´íŠ¸: {self.current_ss_prob:.4f}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, List[float]]:
        """í•™ìŠµ ì‹¤í–‰"""
        
        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }
        
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ í•™ìŠµ ì‹œì‘: {self.config.epochs} ì—í¬í¬")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_path:
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch

            self._update_scheduled_sampling_prob()
            
            # í•™ìŠµ
            train_metrics = self._train_epoch(train_loader)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # ê²€ì¦
            if val_loader and epoch % self.config.eval_every == 0:
                val_metrics = self._validate_epoch(val_loader)
                history['val_loss'].append(val_metrics['loss'])
                history['val_accuracy'].append(val_metrics['accuracy'])
                
                # ìµœê³  ëª¨ë¸ ì €ì¥ (validation loss ê¸°ì¤€)
                if save_path and val_metrics['loss'] < self.best_loss:
                    self.best_loss = val_metrics['loss']
                    self.best_model_path = self._save_best_model(save_path, epoch, val_metrics['loss'])
                    self.patience_counter = 0
                    self.logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                else:
                    self.patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self._should_early_stop(val_metrics['loss']):
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch}")
                    break
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if save_path and epoch % self.config.save_every == 0:
                self._save_checkpoint(save_path, epoch)
            
            # ë¡œê¹…
            self._log_progress(epoch, train_metrics, 
                             val_metrics if val_loader and epoch % self.config.eval_every == 0 else None)
        
        # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ìµœê³  ëª¨ë¸ì´ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥
        if save_path and self.best_model_path is None:
            self.best_model_path = self._save_best_model(save_path, self.current_epoch, self.best_loss)
            self.logger.info(f"ìµœì¢… ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥: {self.best_model_path}")
        
        return history
    
    def _save_best_model(self, save_path: str, epoch: int, loss: float) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        best_model_path = f"{save_path}/best_model.pt"
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ì „ì²´ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss,
            'training_config_dict': training_config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
        torch.save(checkpoint, best_model_path)
        return best_model_path

    def _save_checkpoint(self, save_path: str, epoch: int):
        """ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (í™•ì¥ëœ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            # Scheduled Sampling ì •ë³´ ì¶”ê°€
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'training_config_dict': training_config_dict,  # í•™ìŠµ ì„¤ì •
            'model_config': getattr(self.model, 'config', None),  # ëª¨ë¸ ìì²´ ì„¤ì •
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),  # ì €ì¥ ì‹œê°„
            'current_ss_prob': getattr(self, 'current_ss_prob', None)  # í˜„ì¬ SS í™•ë¥  ì €ì¥
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, f"{save_path}/checkpoint_epoch_{epoch}.pt")
    
    def _log_progress(self, epoch: int, train_metrics: Dict[str, float], val_metrics: Optional[Dict[str, float]] = None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        log_msg = f"ì—í¬í¬ {epoch}: í›ˆë ¨ ì†ì‹¤={train_metrics['loss']:.4f}, í›ˆë ¨ ì •í™•ë„={train_metrics['accuracy']:.4f}"
        
        if val_metrics:
            log_msg += f", ê²€ì¦ ì†ì‹¤={val_metrics['loss']:.4f}, ê²€ì¦ ì •í™•ë„={val_metrics['accuracy']:.4f}"
            if val_metrics['loss'] < self.best_loss:
                log_msg += " â­"
        
        self.logger.info(log_msg)
    
    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ë°°ì¹˜ í•™ìŠµ"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch in progress_bar:
            # ë°°ì¹˜ ì²˜ë¦¬ (for ë£¨í”„ ì œê±°!)
            batch_loss, batch_metrics = self._train_batch(batch)
            
            total_loss += batch_loss
            total_accuracy += batch_metrics['accuracy']
            num_batches += 1
            
            progress_bar.set_postfix({
                'loss': f"{batch_loss:.4f}",
                'acc': f"{batch_metrics['accuracy']:.4f}"
            })
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _train_batch(self, batch: Dict[str, torch.Tensor]) -> tuple:
        """ì§„ì •í•œ ë°°ì¹˜ í•™ìŠµ - GPU ë³‘ë ¬ ì²˜ë¦¬ í™œìš©"""
        # 1. ë°ì´í„° ì¤€ë¹„ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_tokens = batch['input_tokens'].to(self.device)
        target_tokens = batch['target_tokens'].to(self.device) 
        attention_mask = batch['attention_mask'].to(self.device)
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # 3. Forward Pass (ëª¨ë¸ì— ë°°ì¹˜ ì „ì²´ë¥¼ ì „ë‹¬)
        # ëª¨ë¸ì˜ forwardëŠ” ë‚´ë¶€ì ìœ¼ë¡œ CLK ë£¨í”„ë¥¼ ëŒê³  ìµœì¢… ë¡œì§“ [B, seq_len, vocab_size]ë¥¼ ë°˜í™˜
        input_seq_len = input_tokens.shape[1]
        target_seq_len = target_tokens.shape[1]
        latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
        target_start_clk = min(input_seq_len, latest_possible_start)

        output_logits, processing_info = self.model(
            input_schedule=input_tokens,
            max_clk=self.config.max_clk_training,  # YAML ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ê³ ì • CLK
            training=True,
            target_schedule=target_tokens,
            attention_mask=attention_mask,
            target_start_clk=target_start_clk,
            ss_prob=self.current_ss_prob
        )
        
        # 4. ì†ì‹¤ ê³„ì‚° (ìˆ˜ì •ëœ loss_fn ì‚¬ìš©)
        loss = self.loss_fn(output_logits, target_tokens, processing_info)
        
        # 5. Backward Pass (ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        loss.backward()
        
        # 6. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.config.gradient_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_norm
            )
        
        # 7. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()
            
        # 8. ë©”íŠ¸ë¦­ ê³„ì‚° (ì •í™•ë„)
        with torch.no_grad():
            accuracy = SCSMetrics.accuracy(
                output_logits, 
                target_tokens, 
                pad_token_id=self.config.pad_token_id
            )
            
        return loss.item(), {'accuracy': accuracy}
    
    
    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """ê²€ì¦ - Teacher Forcingìœ¼ë¡œ ê³µì •í•œ ë¹„êµ"""
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                input_seq_len = input_tokens.shape[1]
                target_seq_len = target_tokens.shape[1]
                latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
                target_start_clk = min(input_seq_len, latest_possible_start)

                # Teacher Forcing ì‚¬ìš©ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask,
                    target_start_clk=target_start_clk  # **ìƒˆë¡œ ì¶”ê°€**
                )
                
                batch_loss = self.loss_fn(output_logits, target_tokens, processing_info)
                batch_accuracy = SCSMetrics.accuracy(
                    output_logits, 
                    target_tokens, 
                    pad_token_id=self.config.pad_token_id
                )
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _should_early_stop(self, val_loss: float) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ íŒë‹¨"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.config.early_stopping_patience
        
    def evaluate(self, test_loader: DataLoader, save_examples: int = 10) -> Dict[str, Any]:
        """
        ì„¤ê³„ ì›ì¹™:
        1. ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ê°œë³„ ìƒ˜í”Œ ì²˜ë¦¬
        2. ëª¨ë“  í‰ê°€ë¥¼ ì‹¤ì œ ì¶”ë¡  ëª¨ë“œë¡œë§Œ ìˆ˜í–‰  
        3. ë°°ì¹˜ëŠ” ê°œë³„ ê²°ê³¼ì˜ ë‹¨ìˆœ ì§‘ê³„
        
        Args:
            test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
            save_examples: ì €ì¥í•  ì˜ˆì‹œ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        
        Returns:
            í‰ê°€ ê²°ê³¼ + ì˜ˆì‹œ ë°ì´í„°
        """
        self.model.eval()
        
        # ê°œë³„ ìƒ˜í”Œ ê²°ê³¼ ëˆ„ì ìš©
        all_sample_results = []
        saved_examples = []
        
        total_samples = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                batch_size = batch['input_tokens'].shape[0]
                
                # =====================================
                # ë°°ì¹˜ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„í•´í•˜ì—¬ ê°ê° ìˆœìˆ˜ ì¶”ë¡ 
                # =====================================
                for sample_idx in range(batch_size):
                    sample_result = self._evaluate_single_sample_inference(
                        batch, sample_idx, total_samples
                    )
                    
                    all_sample_results.append(sample_result)
                    total_samples += 1
                    
                    # ì˜ˆì‹œ ì €ì¥ (ì´ˆê¸° ëª‡ ê°œë§Œ)
                    if len(saved_examples) < save_examples:
                        saved_examples.append(sample_result)

        print(f"\n=== ì „ì²´ {total_samples}ê°œ ìƒ˜í”Œ ê²°ê³¼ ì§‘ê³„ ===")
        
        # ì •í™•ë„ ê³„ì‚°
        total_accuracy = sum(result['accuracy'] for result in all_sample_results) / len(all_sample_results)
        
        # ì†ì‹¤ ê³„ì‚° (ìˆëŠ” ê²½ìš°)
        losses = [result['loss'] for result in all_sample_results if result['loss'] is not None]
        avg_loss = sum(losses) / len(losses) if losses else 0.0
        
        # ê¸°íƒ€ ë©”íŠ¸ë¦­ ê³„ì‚° (ê¸°ì¡´ SCSMetrics ë°©ì‹ê³¼ ë™ì¼)
        convergence_rate = sum(result['convergence_achieved'] for result in all_sample_results) / len(all_sample_results)
        avg_processing_clk = sum(result['processing_clk'] for result in all_sample_results if isinstance(result['processing_clk'], (int, float))) / len(all_sample_results)
        avg_tokens_generated = sum(result['tokens_generated'] for result in all_sample_results if isinstance(result['tokens_generated'], (int, float))) / len(all_sample_results)
        
        # ì²˜ë¦¬ íš¨ìœ¨ì„± (ê¸°ì¡´ SCSMetricsì™€ ë™ì¼)
        processing_efficiency = max(0.0, 1.0 - (avg_processing_clk / self.config.max_clk_training))
        
        # ì¢…í•© ì ìˆ˜ (ê¸°ì¡´ SCSMetricsì™€ ë™ì¼)
        comprehensive_score = (
            0.4 * convergence_rate +
            0.3 * processing_efficiency +
            0.2 * min(1.0, (avg_tokens_generated / 10.0)) +  # spike_rate ëŒ€ì‹ 
            0.1 * total_accuracy
        )
        
        results = {
            # âœ… ê¸°ì¡´ê³¼ ë™ì¼í•œ ë©”íŠ¸ë¦­ ì´ë¦„ë“¤
            'test_accuracy': total_accuracy,
            'test_loss': avg_loss,
            'comprehensive_score': comprehensive_score,
            'convergence_rate': convergence_rate,
            'processing_efficiency': processing_efficiency,
            
            # ì˜ˆì‹œ ë°ì´í„°
            'examples': saved_examples,
            'num_examples_saved': len(saved_examples),
            'total_batches_evaluated': len(set(result.get('batch_idx', 0) for result in all_sample_results))
        }
        
        print(f"ìµœì¢… ê²°ê³¼: ì •í™•ë„={total_accuracy:.4f}, ì¢…í•©ì ìˆ˜={comprehensive_score:.4f}")
        
        return results

    def _evaluate_single_sample_inference(self, batch: Dict[str, torch.Tensor], sample_idx: int, global_idx: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ìƒ˜í”Œì„ ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ë¡  í‰ê°€
        
        Args:
            batch: ì›ë³¸ ë°°ì¹˜
            sample_idx: ë°°ì¹˜ ë‚´ ìƒ˜í”Œ ì¸ë±ìŠ¤  
            global_idx: ì „ì²´ ìƒ˜í”Œ ì¸ë±ìŠ¤
            
        Returns:
            ê°œë³„ ìƒ˜í”Œ í‰ê°€ ê²°ê³¼
        """
        try:
            device = self.config.device
            
            # =====================================
            # 1. ê°œë³„ ìƒ˜í”Œ ì¶”ì¶œ ë° ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
            # =====================================
            single_input = batch['input_tokens'][sample_idx:sample_idx+1].to(device)  # [1, seq_len]
            single_target = batch['target_tokens'][sample_idx:sample_idx+1].to(device)  # [1, seq_len]
            single_mask = batch['attention_mask'][sample_idx:sample_idx+1].to(device) if 'attention_mask' in batch else None
            
            # í…ìŠ¤íŠ¸ ë³µì›
            input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
            target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            
            # =====================================
            # 2. ê¸°ì¡´ evaluateì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„° ê³„ì‚°
            # =====================================
            input_seq_len = single_input.shape[1]
            target_seq_len = single_target.shape[1]
            latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
            target_start_clk = min(input_seq_len, latest_possible_start)
            
            # =====================================
            # 3. âœ… ê¸°ì¡´ê³¼ ì™„ì „íˆ ë™ì¼í•œ ëª¨ë¸ í˜¸ì¶œ
            # =====================================
            output_logits, processing_info = self.model(
                input_schedule=single_input,
                max_clk=self.config.max_clk_training,
                training=False,  # ê¸°ì¡´ê³¼ ë™ì¼
                target_schedule=single_target,
                attention_mask=single_mask,
                target_start_clk=target_start_clk,  # âœ… ê¸°ì¡´ê³¼ ë™ì¼
                ss_prob=1.0  # âœ… ê¸°ì¡´ê³¼ ë™ì¼ (ê¸°ë³¸ê°’)
            )
            
            # =====================================
            # 4. ìƒì„± ê²°ê³¼ ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
            # =====================================
            if output_logits.shape[1] > 0:
                # ê¸°ì¡´ê³¼ ë™ì¼: argmax ì‚¬ìš©
                generated_tokens = output_logits[0].argmax(dim=-1)  # [seq_len]
                generated_text = self._decode_tokens_to_text(generated_tokens)
            else:
                generated_tokens = torch.tensor([], dtype=torch.long)
                generated_text = "[ë¹ˆ ì¶œë ¥]"
            
            # =====================================
            # 5. ì •í™•ë„ ê³„ì‚° (ê¸°ì¡´ SCSMetricsì™€ ë™ì¼ ë°©ì‹)
            # =====================================
            # ê¸°ì¡´ì²˜ëŸ¼ output_logits vs targetìœ¼ë¡œ ê³„ì‚°
            try:
                from scs.training.metric import SCSMetrics
                accuracy = SCSMetrics.accuracy(
                    output_logits.unsqueeze(0) if output_logits.dim() == 2 else output_logits,
                    single_target,
                    pad_token_id=self.config.pad_token_id
                )
            except:
                # í´ë°±: ì§ì ‘ ê³„ì‚°
                accuracy = self._calculate_sequence_accuracy_fallback(generated_tokens, batch['target_tokens'][sample_idx])
            
            # =====================================
            # 6. ì†ì‹¤ ê³„ì‚° (train_batchì™€ ì¼ê´€ì„± ë§ì¶¤)
            # =====================================
            loss = None
            if output_logits.shape[1] > 0 and single_target.shape[1] > 0:
                try:
                    # ê¸°ì¡´ê³¼ ë™ì¼í•œ loss ê³„ì‚° (train_batchì™€ ë™ì¼í•œ ë°©ì‹)
                    if hasattr(self, 'loss_fn') and self.loss_fn is not None:
                        # train_batchì™€ ë™ì¼: unsqueeze ì œê±°, .item() ì¶”ê°€
                        loss = self.loss_fn(output_logits, single_target, processing_info).item()
                    else:
                        # í´ë°±: CrossEntropyLoss
                        min_len = min(output_logits.shape[1], single_target.shape[1])
                        trimmed_logits = output_logits[:min_len, :].unsqueeze(0)  # [1, min_len, vocab]
                        trimmed_target = single_target[:, :min_len]               # [1, min_len]
                        
                        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)
                        loss = loss_fn(trimmed_logits.view(-1, trimmed_logits.shape[-1]), trimmed_target.view(-1)).item()
                except Exception as e:
                    print(f"  ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨ (ìƒ˜í”Œ {global_idx}): {e}")
                    loss = None
            
            # =====================================
            # 7. ê²°ê³¼ êµ¬ì„± (ê¸°ì¡´ ì˜ˆì‹œì™€ ë™ì¼ í¬ë§·)
            # =====================================
            result = {
                # ê¸°ì¡´ _extract_examples_from_batchì™€ ë™ì¼í•œ í•„ë“œë“¤
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': generated_text,
                'accuracy': accuracy,
                'processing_clk': processing_info.get('processing_clk', 'unknown'),
                'tokens_generated': processing_info.get('tokens_generated', 'unknown'),
                'convergence_achieved': processing_info.get('convergence_achieved', False),
                'batch_accuracy': accuracy,  # ê°œë³„ ìƒ˜í”Œì´ë¯€ë¡œ ë™ì¼
                'generation_method': 'pure_inference',
                
                # ì¶”ê°€ ì •ë³´
                'loss': loss,
                'global_index': global_idx,
                'batch_idx': global_idx // batch['input_tokens'].shape[0],  # ëŒ€ëµì ì¸ ë°°ì¹˜ ì¸ë±ìŠ¤
            }
            
            return result
            
        except Exception as e:
            print(f"  ìƒ˜í”Œ {global_idx} í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # í´ë°± ê²°ê³¼
            try:
                input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
                target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            except:
                input_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
                target_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': "[í‰ê°€ ì‹¤íŒ¨]",
                'accuracy': 0.0,
                'loss': None,
                'processing_clk': self.config.max_clk_training,
                'tokens_generated': 0,
                'convergence_achieved': False,
                'batch_accuracy': 0.0,
                'generation_method': 'error',
                'global_index': global_idx,
                'batch_idx': 0,
            }

    def _calculate_sequence_accuracy_fallback(self, generated_tokens: torch.Tensor, target_tokens: torch.Tensor) -> float:
        """ì‹œí€€ìŠ¤ ì •í™•ë„ ê³„ì‚° í´ë°± (SCSMetrics ì‚¬ìš© ì‹¤íŒ¨ ì‹œ)
        
        Args:
            generated_tokens: ìƒì„±ëœ í† í° ì‹œí€€ìŠ¤ [seq_len]
            target_tokens: ì •ë‹µ í† í° ì‹œí€€ìŠ¤ [seq_len]
            
        Returns:
            í† í°ë³„ ì •í™•ë„ (0.0 ~ 1.0)
        """
        try:
            # íŒ¨ë”© í† í° ì œê±°
            pad_token_id = self.config.pad_token_id
            
            # ì •ë‹µì—ì„œ ìœ íš¨í•œ í† í°ë§Œ ì¶”ì¶œ
            valid_target = target_tokens[target_tokens != pad_token_id]
            
            if len(valid_target) == 0:
                return 0.0
            
            # ìƒì„±ëœ í† í°ì„ ì •ë‹µ ê¸¸ì´ì— ë§ì¶¤
            if len(generated_tokens) >= len(valid_target):
                trimmed_generated = generated_tokens[:len(valid_target)]
            else:
                # ìƒì„±ì´ ë¶€ì¡±í•˜ë©´ íŒ¨ë”©ìœ¼ë¡œ ì±„ì›€
                padding = torch.full((len(valid_target) - len(generated_tokens),), pad_token_id, dtype=generated_tokens.dtype)
                trimmed_generated = torch.cat([generated_tokens, padding])
            
            # í† í°ë³„ ì •í™•ë„ ê³„ì‚°
            correct = (trimmed_generated == valid_target).float()
            accuracy = correct.mean().item()
            
            return accuracy
            
        except Exception as e:
            print(f"í´ë°± ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _decode_tokens_to_text(self, tokens: torch.Tensor) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if self.tokenizer is None:
            return f"tokens: {tokens.tolist()}"
        
        try:
            # íŒ¨ë”© í† í° ì œê±°
            if hasattr(self.tokenizer, 'tokenizer'):
                pad_token_id = self.tokenizer.tokenizer.pad_token_id
            else:
                pad_token_id = self.config.pad_token_id
                
            # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
            valid_tokens = tokens[tokens != pad_token_id]
            
            # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”©
            if hasattr(self.tokenizer, 'decode'):
                return self.tokenizer.decode(valid_tokens.tolist())
            elif hasattr(self.tokenizer, 'tokenizer'):
                return self.tokenizer.tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=True)
            else:
                return f"tokens: {valid_tokens.tolist()}"
                
        except Exception as e:
            return f"decode_error: {tokens.tolist()}"

class GradualUnfreezingScheduler:
    """ì ì§„ì  ì–¸í”„ë¦¬ì§• ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, model, unfreezing_schedule: Dict[int, List[str]]):
        self.model = model
        self.schedule = unfreezing_schedule
        
        # ì´ˆê¸°ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„° ê³ ì •
        for param in self.model.parameters():
            param.requires_grad = False
    
    def step(self, epoch: int):
        """ì—í¬í¬ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì–¸í”„ë¦¬ì§•"""
        if epoch in self.schedule:
            modules_to_unfreeze = self.schedule[epoch]
            for module_name in modules_to_unfreeze:
                module = getattr(self.model, module_name, None)
                if module:
                    for param in module.parameters():
                        param.requires_grad = True