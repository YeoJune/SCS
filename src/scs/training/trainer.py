# src/scs/training/trainer.py
"""
SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ
"""

import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from tqdm import tqdm
import logging
from pathlib import Path
from datetime import datetime

from .loss import SCSLoss
from .metric import SCSMetrics


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨"""
    epochs: int = 15
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    eval_every: int = 3
    save_every: int = 10
    early_stopping_patience: int = 20
    device: str = "cuda"
    max_clk_training: int = 250
    pad_token_id: int = 0
    use_scheduled_sampling: bool = False
    ss_start_prob: float = 1.0
    ss_end_prob: float = 0.05
    ss_decay_epochs: int = 10

class SCSTrainer:
    """SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        config: TrainingConfig,
        loss_fn: Optional[SCSLoss] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        tokenizer: Optional[Any] = None
    ):
        self.model = model
        self.config = config
        
        # ì†ì‹¤ í•¨ìˆ˜ (pad_token_id í¬í•¨)
        self.loss_fn = loss_fn or SCSLoss(pad_token_id=config.pad_token_id)
        
        # ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optimizer or torch.optim.Adam(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        self.scheduler = scheduler
        self.tokenizer = tokenizer
        
        self.device = config.device
        self.model.to(self.device)
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.best_model_path = None  # ìµœê³  ëª¨ë¸ ê²½ë¡œ ì¶”ê°€

        self.current_ss_prob = self.config.ss_start_prob
        
        # ë¡œê¹…
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _update_scheduled_sampling_prob(self):
        """í˜„ì¬ ì—í¬í¬ì— ë§ì¶° ìŠ¤ì¼€ì¤„ ìƒ˜í”Œë§ í™•ë¥ (epsilon)ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if not self.config.use_scheduled_sampling:
            self.current_ss_prob = 1.0 # ì‚¬ìš© ì•ˆ í•  ì‹œ í•­ìƒ Teacher Forcing
            return

        # ì„ í˜• ê°ì†Œ(Linear Decay) ìŠ¤ì¼€ì¤„
        decay_epochs = self.config.ss_decay_epochs
        start_prob = self.config.ss_start_prob
        end_prob = self.config.ss_end_prob
        
        if self.current_epoch >= decay_epochs:
            self.current_ss_prob = end_prob
        else:
            # í˜„ì¬ ì—í¬í¬ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ë¥ ì„ ê°ì†Œì‹œí‚´
            self.current_ss_prob = start_prob - (start_prob - end_prob) * (self.current_epoch / decay_epochs)
        
        # self.loggerê°€ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ë¡œê¹…
        if hasattr(self, 'logger'):
            self.logger.info(f"Scheduled Sampling í™•ë¥ (epsilon) ì—…ë°ì´íŠ¸: {self.current_ss_prob:.4f}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, List[float]]:
        """í•™ìŠµ ì‹¤í–‰"""
        
        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }
        
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ í•™ìŠµ ì‹œì‘: {self.config.epochs} ì—í¬í¬")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_path:
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch

            self._update_scheduled_sampling_prob()
            
            # í•™ìŠµ
            train_metrics = self._train_epoch(train_loader)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # ê²€ì¦
            if val_loader and epoch % self.config.eval_every == 0:
                val_metrics = self._validate_epoch(val_loader)
                history['val_loss'].append(val_metrics['loss'])
                history['val_accuracy'].append(val_metrics['accuracy'])
                
                # ìµœê³  ëª¨ë¸ ì €ì¥ (validation loss ê¸°ì¤€)
                if save_path and val_metrics['loss'] < self.best_loss:
                    self.best_loss = val_metrics['loss']
                    self.best_model_path = self._save_best_model(save_path, epoch, val_metrics['loss'])
                    self.patience_counter = 0
                    self.logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                else:
                    self.patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self._should_early_stop(val_metrics['loss']):
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch}")
                    break
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if save_path and epoch % self.config.save_every == 0:
                self._save_checkpoint(save_path, epoch)
            
            # ë¡œê¹…
            self._log_progress(epoch, train_metrics, 
                             val_metrics if val_loader and epoch % self.config.eval_every == 0 else None)
        
        # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ìµœê³  ëª¨ë¸ì´ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥
        if save_path and self.best_model_path is None:
            self.best_model_path = self._save_best_model(save_path, self.current_epoch, self.best_loss)
            self.logger.info(f"ìµœì¢… ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥: {self.best_model_path}")
        
        return history
    
    def _save_best_model(self, save_path: str, epoch: int, loss: float) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        best_model_path = f"{save_path}/best_model.pt"
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ì „ì²´ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss,
            'training_config_dict': training_config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
        torch.save(checkpoint, best_model_path)
        return best_model_path

    def _save_checkpoint(self, save_path: str, epoch: int):
        """ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (í™•ì¥ëœ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            # Scheduled Sampling ì •ë³´ ì¶”ê°€
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'training_config_dict': training_config_dict,  # í•™ìŠµ ì„¤ì •
            'model_config': getattr(self.model, 'config', None),  # ëª¨ë¸ ìì²´ ì„¤ì •
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),  # ì €ì¥ ì‹œê°„
            'current_ss_prob': getattr(self, 'current_ss_prob', None)  # í˜„ì¬ SS í™•ë¥  ì €ì¥
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, f"{save_path}/checkpoint_epoch_{epoch}.pt")
    
    def _log_progress(self, epoch: int, train_metrics: Dict[str, float], val_metrics: Optional[Dict[str, float]] = None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        log_msg = f"ì—í¬í¬ {epoch}: í›ˆë ¨ ì†ì‹¤={train_metrics['loss']:.4f}, í›ˆë ¨ ì •í™•ë„={train_metrics['accuracy']:.4f}"
        
        if val_metrics:
            log_msg += f", ê²€ì¦ ì†ì‹¤={val_metrics['loss']:.4f}, ê²€ì¦ ì •í™•ë„={val_metrics['accuracy']:.4f}"
            if val_metrics['loss'] < self.best_loss:
                log_msg += " â­"
        
        self.logger.info(log_msg)
    
    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ë°°ì¹˜ í•™ìŠµ"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch in progress_bar:
            # ë°°ì¹˜ ì²˜ë¦¬ (for ë£¨í”„ ì œê±°!)
            batch_loss, batch_metrics = self._train_batch(batch)
            
            total_loss += batch_loss
            total_accuracy += batch_metrics['accuracy']
            num_batches += 1
            
            progress_bar.set_postfix({
                'loss': f"{batch_loss:.4f}",
                'acc': f"{batch_metrics['accuracy']:.4f}"
            })
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _train_batch(self, batch: Dict[str, torch.Tensor]) -> tuple:
        """ì§„ì •í•œ ë°°ì¹˜ í•™ìŠµ - GPU ë³‘ë ¬ ì²˜ë¦¬ í™œìš©"""
        # 1. ë°ì´í„° ì¤€ë¹„ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_tokens = batch['input_tokens'].to(self.device)
        target_tokens = batch['target_tokens'].to(self.device) 
        attention_mask = batch['attention_mask'].to(self.device)
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # 3. Forward Pass (ëª¨ë¸ì— ë°°ì¹˜ ì „ì²´ë¥¼ ì „ë‹¬)
        # ëª¨ë¸ì˜ forwardëŠ” ë‚´ë¶€ì ìœ¼ë¡œ CLK ë£¨í”„ë¥¼ ëŒê³  ìµœì¢… ë¡œì§“ [B, seq_len, vocab_size]ë¥¼ ë°˜í™˜
        input_seq_len = input_tokens.shape[1]
        target_seq_len = target_tokens.shape[1]
        latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
        target_start_clk = min(input_seq_len, latest_possible_start)

        output_logits, processing_info = self.model(
            input_schedule=input_tokens,
            max_clk=self.config.max_clk_training,  # YAML ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ê³ ì • CLK
            training=True,
            target_schedule=target_tokens,
            attention_mask=attention_mask,
            target_start_clk=target_start_clk,
            ss_prob=self.current_ss_prob
        )
        
        # 4. ì†ì‹¤ ê³„ì‚° (ìˆ˜ì •ëœ loss_fn ì‚¬ìš©)
        loss = self.loss_fn(output_logits, target_tokens, processing_info)
        
        # 5. Backward Pass (ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        loss.backward()
        
        # 6. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.config.gradient_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_norm
            )
        
        # 7. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()
            
        # 8. ë©”íŠ¸ë¦­ ê³„ì‚° (ì •í™•ë„)
        with torch.no_grad():
            accuracy = SCSMetrics.accuracy(
                output_logits, 
                target_tokens, 
                pad_token_id=self.config.pad_token_id
            )
            
        return loss.item(), {'accuracy': accuracy}
    
    
    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """ê²€ì¦ - Teacher Forcingìœ¼ë¡œ ê³µì •í•œ ë¹„êµ"""
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                input_seq_len = input_tokens.shape[1]
                target_seq_len = target_tokens.shape[1]
                latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
                target_start_clk = min(input_seq_len, latest_possible_start)

                # Teacher Forcing ì‚¬ìš©ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask,
                    target_start_clk=target_start_clk  # **ìƒˆë¡œ ì¶”ê°€**
                )
                
                batch_loss = self.loss_fn(output_logits, target_tokens, processing_info)
                batch_accuracy = SCSMetrics.accuracy(
                    output_logits, 
                    target_tokens, 
                    pad_token_id=self.config.pad_token_id
                )
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _should_early_stop(self, val_loss: float) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ íŒë‹¨"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.config.early_stopping_patience
    
    def evaluate(self, test_loader: DataLoader, save_examples: int = 10) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ í‰ê°€ (ë°°ì¹˜ ì¼ê´€ì„± ë³´ì¥ + ì˜ˆì‹œ ì €ì¥)
        
        Args:
            test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
            save_examples: ì €ì¥í•  ì˜ˆì‹œ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        
        Returns:
            í‰ê°€ ê²°ê³¼ + ì˜ˆì‹œ ë°ì´í„°
        """
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        total_comprehensive = 0.0
        total_convergence = 0.0
        total_efficiency = 0.0
        num_batches = 0
        
        # ì˜ˆì‹œ ë°ì´í„° ìˆ˜ì§‘ìš©
        saved_examples = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                # ë°°ì¹˜ ì „ì²´ë¥¼ í•œë²ˆì— ì²˜ë¦¬
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                input_seq_len = input_tokens.shape[1]
                target_seq_len = target_tokens.shape[1]
                latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
                target_start_clk = min(input_seq_len, latest_possible_start)

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ ì‹¤í–‰ (í•­ìƒ ë°°ì¹˜ ì¶œë ¥ ë³´ì¥)
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask,
                    target_start_clk=target_start_clk  # **ìƒˆë¡œ ì¶”ê°€**
                )
                
                # ì¶œë ¥ì€ í•­ìƒ [B, seq_len, vocab_size] í˜•íƒœë¡œ ë³´ì¥ë¨
                assert output_logits.dim() == 3, f"ì˜ˆìƒ ì°¨ì› [B, seq_len, vocab_size], ì‹¤ì œ: {output_logits.shape}"
                
                # ë°°ì¹˜ ë‹¨ìœ„ ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
                batch_loss = self.loss_fn(output_logits, target_tokens, processing_info)
                batch_accuracy = SCSMetrics.accuracy(
                    output_logits, 
                    target_tokens, 
                    pad_token_id=self.config.pad_token_id
                )
                
                # ìƒì„¸ ë©”íŠ¸ë¦­ë“¤ë„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³„ì‚°
                batch_comprehensive = SCSMetrics.comprehensive_score(processing_info)
                batch_convergence = SCSMetrics.convergence_rate(processing_info)
                batch_efficiency = SCSMetrics.processing_efficiency(processing_info)
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                total_comprehensive += batch_comprehensive
                total_convergence += batch_convergence
                total_efficiency += batch_efficiency
                num_batches += 1
                
                # --- ì˜ˆì‹œ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ---
                if len(saved_examples) < save_examples:
                    examples_to_save = self._extract_examples_from_batch(
                        batch, output_logits, processing_info, batch_accuracy
                    )
                    saved_examples.extend(examples_to_save)
                    
                    # ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ì €ì¥ ì¤‘ë‹¨
                    if len(saved_examples) >= save_examples:
                        saved_examples = saved_examples[:save_examples]
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        results = {
            # ê¸°ì¡´ ë©”íŠ¸ë¦­ë“¤
            'test_loss': total_loss / num_batches,
            'test_accuracy': total_accuracy / num_batches,
            'comprehensive_score': total_comprehensive / num_batches,
            'convergence_rate': total_convergence / num_batches,
            'processing_efficiency': total_efficiency / num_batches,
            
            # ìƒˆë¡œ ì¶”ê°€: ì˜ˆì‹œ ë°ì´í„°
            'examples': saved_examples,
            'num_examples_saved': len(saved_examples),
            'total_batches_evaluated': num_batches
        }
        
        return results
    
    def _generate_text_for_sample(self, input_tokens: torch.Tensor, attention_mask: torch.Tensor = None) -> str:
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•´ ì‹¤ì œ ì¶”ë¡ ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        self.model.eval()
        with torch.no_grad():
            try:
                device = self.config.device
                input_tokens = input_tokens.to(device)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)
                
                # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens.unsqueeze(0),  # [1, seq_len]
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=None,
                    attention_mask=attention_mask.unsqueeze(0) if attention_mask is not None else None
                )
                
                if output_logits.shape[1] > 0:
                    # ì˜¨ë„ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¶”ê°€
                    temperature = 0.8
                    if temperature > 0:
                        scaled_logits = output_logits[0] / temperature  # [seq_len, vocab_size]
                        probs = torch.softmax(scaled_logits, dim=-1)
                        
                        generated_tokens = []
                        for pos in range(scaled_logits.shape[0]):
                            # Top-k ìƒ˜í”Œë§
                            k = 20
                            top_probs, top_indices = probs[pos].topk(k)
                            top_probs = top_probs / top_probs.sum()
                            
                            sampled_idx = torch.multinomial(top_probs, 1).item()
                            sampled_token = top_indices[sampled_idx].item()
                            generated_tokens.append(sampled_token)
                        
                        generated_tokens = torch.tensor(generated_tokens, device=device)
                    else:
                        generated_tokens = output_logits[0].argmax(dim=-1)
                    
                    return self._decode_tokens_to_text(generated_tokens)
                else:
                    return "[ë¹ˆ ì¶œë ¥]"
                    
            except Exception as e:
                print(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
                return "[ì¶”ë¡  ì˜¤ë¥˜]"

    def _extract_examples_from_batch(self, batch, output_logits, processing_info, batch_accuracy):
        """ë°°ì¹˜ì—ì„œ ì˜ˆì‹œ ë°ì´í„° ì¶”ì¶œ - ì‹¤ì œ ì¶”ë¡  ëª¨ë“œ ì‚¬ìš©"""
        examples = []
        batch_size = batch['input_tokens'].shape[0]
        
        device = self.config.device
        
        for i in range(batch_size):
            try:
                input_text = self._decode_tokens_to_text(batch['input_tokens'][i])
                target_text = self._decode_tokens_to_text(batch['target_tokens'][i])
                
                # *** ì‹¤ì œ ì¶”ë¡ ìœ¼ë¡œ ìƒì„± ***
                input_tokens_cpu = batch['input_tokens'][i]  # CPUì—ì„œ ìœ ì§€
                attention_mask_cpu = batch['attention_mask'][i] if 'attention_mask' in batch else None
                
                generated_text = self._generate_text_for_sample(
                    input_tokens_cpu,  # _generate_text_for_sample ë‚´ë¶€ì—ì„œ deviceë¡œ ì´ë™
                    attention_mask_cpu
                )
                
                # ì •í™•ë„ëŠ” ì›ë˜ Teacher Forcing ê²°ê³¼ ì‚¬ìš©
                individual_accuracy = self._calculate_individual_accuracy(
                    output_logits[i:i+1], 
                    batch['target_tokens'][i:i+1]
                )
                
                example_info = {
                    'input_text': input_text,
                    'target_text': target_text,
                    'generated_text': generated_text,  # ì‹¤ì œ ì¶”ë¡  ê²°ê³¼!
                    'accuracy': individual_accuracy,
                    'processing_clk': processing_info.get('processing_clk', 'unknown'),
                    'tokens_generated': processing_info.get('tokens_generated', 'unknown'),
                    'convergence_achieved': processing_info.get('convergence_achieved', False),
                    'batch_accuracy': batch_accuracy,
                    'generation_method': 'real_inference'
                }
                examples.append(example_info)
                
            except Exception as e:
                self.logger.warning(f"ì˜ˆì‹œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ (ë°°ì¹˜ {i}): {e}")
                # í´ë°±: ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                try:
                    input_text = self._decode_tokens_to_text(batch['input_tokens'][i])
                    target_text = self._decode_tokens_to_text(batch['target_tokens'][i])
                    
                    example_info = {
                        'input_text': input_text,
                        'target_text': target_text,
                        'generated_text': "[ì¶”ë¡  ì‹¤íŒ¨]",
                        'accuracy': 0.0,
                        'processing_clk': 'error',
                        'tokens_generated': 0,
                        'convergence_achieved': False,
                        'batch_accuracy': batch_accuracy,
                        'generation_method': 'error'
                    }
                    examples.append(example_info)
                except:
                    continue
        
        return examples

    def _decode_tokens_to_text(self, tokens: torch.Tensor) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if self.tokenizer is None:
            return f"tokens: {tokens.tolist()}"
        
        try:
            # íŒ¨ë”© í† í° ì œê±°
            if hasattr(self.tokenizer, 'tokenizer'):
                pad_token_id = self.tokenizer.tokenizer.pad_token_id
            else:
                pad_token_id = self.config.pad_token_id
                
            # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
            valid_tokens = tokens[tokens != pad_token_id]
            
            # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”©
            if hasattr(self.tokenizer, 'decode'):
                return self.tokenizer.decode(valid_tokens.tolist())
            elif hasattr(self.tokenizer, 'tokenizer'):
                return self.tokenizer.tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=True)
            else:
                return f"tokens: {valid_tokens.tolist()}"
                
        except Exception as e:
            self.logger.warning(f"í† í° ë””ì½”ë”© ì‹¤íŒ¨: {e}")
            return f"decode_error: {tokens.tolist()}"

    def _calculate_individual_accuracy(
        self, 
        output_logits: torch.Tensor,  # [1, seq_len, vocab_size]
        target_tokens: torch.Tensor   # [1, seq_len]
    ) -> float:
        """ê°œë³„ ìƒ˜í”Œì˜ ì •í™•ë„ ê³„ì‚°"""
        try:
            return SCSMetrics.accuracy(
                output_logits, 
                target_tokens, 
                pad_token_id=self.config.pad_token_id
            )
        except Exception:
            return 0.0

class GradualUnfreezingScheduler:
    """ì ì§„ì  ì–¸í”„ë¦¬ì§• ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, model, unfreezing_schedule: Dict[int, List[str]]):
        self.model = model
        self.schedule = unfreezing_schedule
        
        # ì´ˆê¸°ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„° ê³ ì •
        for param in self.model.parameters():
            param.requires_grad = False
    
    def step(self, epoch: int):
        """ì—í¬í¬ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì–¸í”„ë¦¬ì§•"""
        if epoch in self.schedule:
            modules_to_unfreeze = self.schedule[epoch]
            for module_name in modules_to_unfreeze:
                module = getattr(self.model, module_name, None)
                if module:
                    for param in module.parameters():
                        param.requires_grad = True