# src/scs/training/trainer.py
"""
SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ
"""

import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from tqdm import tqdm
import logging
from pathlib import Path
from datetime import datetime

from .loss import SCSLoss
from .metric import SCSMetrics


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨"""
    epochs: int = 15
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    eval_every: int = 3
    save_every: int = 10
    early_stopping_patience: int = 20
    device: str = "cuda"
    max_clk_training: int = 250
    pad_token_id: int = 0
    use_scheduled_sampling: bool = False
    ss_start_prob: float = 1.0
    ss_end_prob: float = 0.05
    ss_decay_epochs: int = 10

class SCSTrainer:
    """SCS ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        config: TrainingConfig,
        loss_fn: Optional[SCSLoss] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        tokenizer: Optional[Any] = None
    ):
        self.model = model
        self.config = config
        
        # ì†ì‹¤ í•¨ìˆ˜ (pad_token_id í¬í•¨)
        self.loss_fn = loss_fn or SCSLoss(pad_token_id=config.pad_token_id)
        
        # ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optimizer or torch.optim.Adam(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        self.scheduler = scheduler
        self.tokenizer = tokenizer
        
        self.device = config.device
        self.model.to(self.device)
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.best_model_path = None  # ìµœê³  ëª¨ë¸ ê²½ë¡œ ì¶”ê°€

        self.current_ss_prob = self.config.ss_start_prob
        
        # ë¡œê¹…
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _update_scheduled_sampling_prob(self):
        """í˜„ì¬ ì—í¬í¬ì— ë§ì¶° ìŠ¤ì¼€ì¤„ ìƒ˜í”Œë§ í™•ë¥ (epsilon)ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if not self.config.use_scheduled_sampling:
            self.current_ss_prob = 1.0 # ì‚¬ìš© ì•ˆ í•  ì‹œ í•­ìƒ Teacher Forcing
            return

        # ì„ í˜• ê°ì†Œ(Linear Decay) ìŠ¤ì¼€ì¤„
        decay_epochs = self.config.ss_decay_epochs
        start_prob = self.config.ss_start_prob
        end_prob = self.config.ss_end_prob
        
        if self.current_epoch >= decay_epochs:
            self.current_ss_prob = end_prob
        else:
            # í˜„ì¬ ì—í¬í¬ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ í™•ë¥ ì„ ê°ì†Œì‹œí‚´
            self.current_ss_prob = start_prob - (start_prob - end_prob) * (self.current_epoch / decay_epochs)
        
        # self.loggerê°€ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ë¡œê¹…
        if hasattr(self, 'logger'):
            self.logger.info(f"Scheduled Sampling í™•ë¥ (epsilon) ì—…ë°ì´íŠ¸: {self.current_ss_prob:.4f}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, List[float]]:
        """í•™ìŠµ ì‹¤í–‰"""
        
        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }
        
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ í•™ìŠµ ì‹œì‘: {self.config.epochs} ì—í¬í¬")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_path:
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch

            self._update_scheduled_sampling_prob()
            
            # í•™ìŠµ
            train_metrics = self._train_epoch(train_loader)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # ê²€ì¦
            if val_loader and epoch % self.config.eval_every == 0:
                val_metrics = self._validate_epoch(val_loader)
                history['val_loss'].append(val_metrics['loss'])
                history['val_accuracy'].append(val_metrics['accuracy'])
                
                # ìµœê³  ëª¨ë¸ ì €ì¥ (validation loss ê¸°ì¤€)
                if save_path and val_metrics['loss'] < self.best_loss:
                    self.best_loss = val_metrics['loss']
                    self.best_model_path = self._save_best_model(save_path, epoch, val_metrics['loss'])
                    self.patience_counter = 0
                    self.logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                else:
                    self.patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self._should_early_stop(val_metrics['loss']):
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch}")
                    break
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if save_path and epoch % self.config.save_every == 0:
                self._save_checkpoint(save_path, epoch)
            
            # ë¡œê¹…
            self._log_progress(epoch, train_metrics, 
                             val_metrics if val_loader and epoch % self.config.eval_every == 0 else None)
        
        # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ìµœê³  ëª¨ë¸ì´ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥
        if save_path and self.best_model_path is None:
            self.best_model_path = self._save_best_model(save_path, self.current_epoch, self.best_loss)
            self.logger.info(f"ìµœì¢… ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥: {self.best_model_path}")
        
        return history
    
    def _save_best_model(self, save_path: str, epoch: int, loss: float) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        best_model_path = f"{save_path}/best_model.pt"
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ì „ì²´ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss,
            'training_config_dict': training_config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
        torch.save(checkpoint, best_model_path)
        return best_model_path

    def _save_checkpoint(self, save_path: str, epoch: int):
        """ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„¤ì • ì •ë³´ í¬í•¨)"""
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # TrainingConfigë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (í™•ì¥ëœ í•„ë“œ í¬í•¨)
        training_config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            # Scheduled Sampling ì •ë³´ ì¶”ê°€
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'training_config_dict': training_config_dict,  # í•™ìŠµ ì„¤ì •
            'model_config': getattr(self.model, 'config', None),  # ëª¨ë¸ ìì²´ ì„¤ì •
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),  # ì €ì¥ ì‹œê°„
            'current_ss_prob': getattr(self, 'current_ss_prob', None)  # í˜„ì¬ SS í™•ë¥  ì €ì¥
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, f"{save_path}/checkpoint_epoch_{epoch}.pt")
    
    def _log_progress(self, epoch: int, train_metrics: Dict[str, float], val_metrics: Optional[Dict[str, float]] = None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        log_msg = f"ì—í¬í¬ {epoch}: í›ˆë ¨ ì†ì‹¤={train_metrics['loss']:.4f}, í›ˆë ¨ ì •í™•ë„={train_metrics['accuracy']:.4f}"
        
        if val_metrics:
            log_msg += f", ê²€ì¦ ì†ì‹¤={val_metrics['loss']:.4f}, ê²€ì¦ ì •í™•ë„={val_metrics['accuracy']:.4f}"
            if val_metrics['loss'] < self.best_loss:
                log_msg += " â­"
        
        self.logger.info(log_msg)
    
    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ë°°ì¹˜ í•™ìŠµ"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch in progress_bar:
            # ë°°ì¹˜ ì²˜ë¦¬ (for ë£¨í”„ ì œê±°!)
            batch_loss, batch_metrics = self._train_batch(batch)
            
            total_loss += batch_loss
            total_accuracy += batch_metrics['accuracy']
            num_batches += 1
            
            progress_bar.set_postfix({
                'loss': f"{batch_loss:.4f}",
                'acc': f"{batch_metrics['accuracy']:.4f}"
            })
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _train_batch(self, batch: Dict[str, torch.Tensor]) -> tuple:
        """ì§„ì •í•œ ë°°ì¹˜ í•™ìŠµ - GPU ë³‘ë ¬ ì²˜ë¦¬ í™œìš©"""
        # 1. ë°ì´í„° ì¤€ë¹„ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_tokens = batch['input_tokens'].to(self.device)
        target_tokens = batch['target_tokens'].to(self.device) 
        attention_mask = batch['attention_mask'].to(self.device)
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # 3. Forward Pass (ëª¨ë¸ì— ë°°ì¹˜ ì „ì²´ë¥¼ ì „ë‹¬)
        # ëª¨ë¸ì˜ forwardëŠ” ë‚´ë¶€ì ìœ¼ë¡œ CLK ë£¨í”„ë¥¼ ëŒê³  ìµœì¢… ë¡œì§“ [B, seq_len, vocab_size]ë¥¼ ë°˜í™˜
        input_seq_len = input_tokens.shape[1]
        target_seq_len = target_tokens.shape[1]
        latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
        target_start_clk = min(input_seq_len, latest_possible_start)

        output_logits, processing_info = self.model(
            input_schedule=input_tokens,
            max_clk=self.config.max_clk_training,  # YAML ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ê³ ì • CLK
            training=True,
            target_schedule=target_tokens,
            attention_mask=attention_mask,
            target_start_clk=target_start_clk,
            ss_prob=self.current_ss_prob
        )
        
        # 4. ì†ì‹¤ ê³„ì‚° (ìˆ˜ì •ëœ loss_fn ì‚¬ìš©)
        loss = self.loss_fn(output_logits, target_tokens, processing_info)
        
        # 5. Backward Pass (ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        loss.backward()
        
        # 6. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        if self.config.gradient_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_norm
            )
        
        # 7. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()
            
        # 8. ë©”íŠ¸ë¦­ ê³„ì‚° (ì •í™•ë„)
        with torch.no_grad():
            accuracy = SCSMetrics.accuracy(
                output_logits, 
                target_tokens, 
                pad_token_id=self.config.pad_token_id
            )
            
        return loss.item(), {'accuracy': accuracy}
    
    
    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """ê²€ì¦ - Teacher Forcingìœ¼ë¡œ ê³µì •í•œ ë¹„êµ"""
        self.model.eval()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                input_seq_len = input_tokens.shape[1]
                target_seq_len = target_tokens.shape[1]
                latest_possible_start = max(0, self.config.max_clk_training - target_seq_len - 1)
                target_start_clk = min(input_seq_len, latest_possible_start)

                # Teacher Forcing ì‚¬ìš©ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ
                output_logits, processing_info = self.model(
                    input_schedule=input_tokens,
                    max_clk=self.config.max_clk_training,
                    training=False,
                    target_schedule=target_tokens,
                    attention_mask=attention_mask,
                    target_start_clk=target_start_clk  # **ìƒˆë¡œ ì¶”ê°€**
                )
                
                batch_loss = self.loss_fn(output_logits, target_tokens, processing_info)
                batch_accuracy = SCSMetrics.accuracy(
                    output_logits, 
                    target_tokens, 
                    pad_token_id=self.config.pad_token_id
                )
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _should_early_stop(self, val_loss: float) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ íŒë‹¨"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.config.early_stopping_patience
    def evaluate(self, test_loader: DataLoader, save_examples: int = 10) -> Dict[str, Any]:
        """ì™„ì „íˆ ì¬ì„¤ê³„ëœ í‰ê°€ ì‹œìŠ¤í…œ - ìˆœìˆ˜ ì¶”ë¡  ëª¨ë“œë§Œ ì‚¬ìš©
        
        ì„¤ê³„ ì›ì¹™:
        1. ëª¨ë“  í‰ê°€ë¥¼ ì‹¤ì œ ì¶”ë¡  ëª¨ë“œë¡œë§Œ ìˆ˜í–‰
        2. ê°œë³„ ìƒ˜í”Œì„ 1-ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì™„ì „í•œ ì¼ê´€ì„± í™•ë³´
        3. ë°°ì¹˜ ë‹¨ìœ„ëŠ” ë‹¨ìˆœíˆ ê°œë³„ ê²°ê³¼ì˜ ì§‘ê³„
        
        Args:
            test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
            save_examples: ì €ì¥í•  ì˜ˆì‹œ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        
        Returns:
            í‰ê°€ ê²°ê³¼ + ì˜ˆì‹œ ë°ì´í„°
        """
        self.model.eval()
        
        # ê°œë³„ ìƒ˜í”Œ ê²°ê³¼ ëˆ„ì ìš©
        all_sample_results = []
        saved_examples = []
        
        total_samples = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                batch_size = batch['input_tokens'].shape[0]
                print(f"\n=== ë°°ì¹˜ {batch_idx} (í¬ê¸°: {batch_size}) ì²˜ë¦¬ ì¤‘ ===")
                
                # =====================================
                # ë°°ì¹˜ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„í•´í•˜ì—¬ ê°ê° ìˆœìˆ˜ ì¶”ë¡ 
                # =====================================
                for sample_idx in range(batch_size):
                    sample_result = self._evaluate_single_sample_inference(
                        batch, sample_idx, total_samples
                    )
                    
                    all_sample_results.append(sample_result)
                    total_samples += 1
                    
                    # ì˜ˆì‹œ ì €ì¥ (ì´ˆê¸° ëª‡ ê°œë§Œ)
                    if len(saved_examples) < save_examples:
                        saved_examples.append(sample_result)
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    if sample_idx < 3 or total_samples % 10 == 0:
                        print(f"  ìƒ˜í”Œ {total_samples}: ì •í™•ë„={sample_result['accuracy']:.3f}, "
                            f"ìƒì„±='{sample_result['generated_text'][:30]}...', "
                            f"ì •ë‹µ='{sample_result['target_text'][:30]}...'")
        
        # =====================================
        # ì „ì²´ ê²°ê³¼ ì§‘ê³„
        # =====================================
        print(f"\n=== ì „ì²´ {total_samples}ê°œ ìƒ˜í”Œ ê²°ê³¼ ì§‘ê³„ ===")
        
        # ì •í™•ë„ ê³„ì‚°
        total_accuracy = sum(result['accuracy'] for result in all_sample_results) / len(all_sample_results)
        
        # ì†ì‹¤ ê³„ì‚° (ìˆëŠ” ê²½ìš°)
        losses = [result['loss'] for result in all_sample_results if result['loss'] is not None]
        avg_loss = sum(losses) / len(losses) if losses else 0.0
        
        # ê¸°íƒ€ ë©”íŠ¸ë¦­ ê³„ì‚°
        convergence_rate = sum(result['convergence_achieved'] for result in all_sample_results) / len(all_sample_results)
        avg_processing_clk = sum(result['processing_clk'] for result in all_sample_results if isinstance(result['processing_clk'], (int, float))) / len(all_sample_results)
        avg_tokens_generated = sum(result['tokens_generated'] for result in all_sample_results if isinstance(result['tokens_generated'], (int, float))) / len(all_sample_results)
        
        # ì²˜ë¦¬ íš¨ìœ¨ì„± (CLK ê¸°ë°˜)
        processing_efficiency = max(0.0, 1.0 - (avg_processing_clk / self.config.max_clk_training))
        
        # ì¢…í•© ì ìˆ˜
        comprehensive_score = (
            0.6 * total_accuracy +  # ì •í™•ë„ 60%
            0.2 * convergence_rate +  # ìˆ˜ë ´ìœ¨ 20%
            0.2 * processing_efficiency  # íš¨ìœ¨ì„± 20%
        )
        
        results = {
            # í•µì‹¬ ë©”íŠ¸ë¦­
            'test_accuracy': total_accuracy,
            'test_loss': avg_loss,
            'comprehensive_score': comprehensive_score,
            'convergence_rate': convergence_rate,
            'processing_efficiency': processing_efficiency,
            
            # ì¶”ê°€ í†µê³„
            'avg_processing_clk': avg_processing_clk,
            'avg_tokens_generated': avg_tokens_generated,
            'total_samples_evaluated': total_samples,
            
            # ì˜ˆì‹œ ë°ì´í„°
            'examples': saved_examples,
            'num_examples_saved': len(saved_examples),
            
            # ë””ë²„ê¹…ìš© ì „ì²´ ê²°ê³¼ (ì˜µì…˜)
            'all_results': all_sample_results if total_samples <= 50 else None  # 50ê°œ ì´í•˜ë§Œ ì €ì¥
        }
        
        print(f"ìµœì¢… ê²°ê³¼: ì •í™•ë„={total_accuracy:.4f}, ì¢…í•©ì ìˆ˜={comprehensive_score:.4f}")
        
        return results

    def _evaluate_single_sample_inference(self, batch: Dict[str, torch.Tensor], sample_idx: int, global_idx: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ìƒ˜í”Œì„ ìˆœìˆ˜ ì¶”ë¡  ëª¨ë“œë¡œ í‰ê°€
        
        Args:
            batch: ì›ë³¸ ë°°ì¹˜
            sample_idx: ë°°ì¹˜ ë‚´ ìƒ˜í”Œ ì¸ë±ìŠ¤
            global_idx: ì „ì²´ ìƒ˜í”Œ ì¸ë±ìŠ¤
            
        Returns:
            ê°œë³„ ìƒ˜í”Œ í‰ê°€ ê²°ê³¼
        """
        try:
            device = self.config.device
            
            # =====================================
            # 1. ê°œë³„ ìƒ˜í”Œ ì¶”ì¶œ ë° ì¤€ë¹„
            # =====================================
            single_input = batch['input_tokens'][sample_idx:sample_idx+1].to(device)  # [1, seq_len]
            single_target = batch['target_tokens'][sample_idx:sample_idx+1].to(device)  # [1, seq_len]
            single_mask = batch['attention_mask'][sample_idx:sample_idx+1].to(device) if 'attention_mask' in batch else None
            
            # í…ìŠ¤íŠ¸ ë³µì›
            input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
            target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            
            # =====================================
            # 2. ìˆœìˆ˜ ì¶”ë¡  ì‹¤í–‰ (1-ë°°ì¹˜)
            # =====================================
            output_logits, processing_info = self.model(
                input_schedule=single_input,
                max_clk=self.config.max_clk_training,
                training=False,  # ì¶”ë¡  ëª¨ë“œ
                target_schedule=None,  # íƒ€ê²Ÿ ì—†ìŒ (ìˆœìˆ˜ ì¶”ë¡ )
                attention_mask=single_mask
            )
            
            # =====================================
            # 3. ìƒì„± ê²°ê³¼ ì¶”ì¶œ
            # =====================================
            if output_logits.shape[1] > 0:
                # ê°€ì¥ í™•ë¥  ë†’ì€ í† í°ë“¤ ì„ íƒ (deterministic)
                generated_tokens = output_logits[0].argmax(dim=-1)  # [seq_len]
                generated_text = self._decode_tokens_to_text(generated_tokens)
            else:
                generated_tokens = torch.tensor([], dtype=torch.long)
                generated_text = "[ë¹ˆ ì¶œë ¥]"
            
            # =====================================
            # 4. ì •í™•ë„ ê³„ì‚° (generated vs target)
            # =====================================
            accuracy = self._calculate_sequence_accuracy(generated_tokens, batch['target_tokens'][sample_idx])
            
            # =====================================
            # 5. ì†ì‹¤ ê³„ì‚° (ì„ íƒì )
            # =====================================
            loss = None
            if output_logits.shape[1] > 0 and single_target.shape[1] > 0:
                try:
                    # ê¸¸ì´ ë§ì¶¤
                    min_len = min(output_logits.shape[1], single_target.shape[1])
                    trimmed_logits = output_logits[:, :min_len, :]  # [1, min_len, vocab]
                    trimmed_target = single_target[:, :min_len]     # [1, min_len]
                    
                    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)
                    loss = loss_fn(trimmed_logits.view(-1, trimmed_logits.shape[-1]), trimmed_target.view(-1)).item()
                except Exception as e:
                    print(f"  ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨ (ìƒ˜í”Œ {global_idx}): {e}")
                    loss = None
            
            # =====================================
            # 6. ê²°ê³¼ êµ¬ì„±
            # =====================================
            result = {
                # ê¸°ë³¸ ì •ë³´
                'global_index': global_idx,
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': generated_text,
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                'accuracy': accuracy,
                'loss': loss,
                
                # ì²˜ë¦¬ ì •ë³´
                'processing_clk': processing_info.get('processing_clk', self.config.max_clk_training),
                'tokens_generated': processing_info.get('tokens_generated', len(generated_tokens)),
                'convergence_achieved': processing_info.get('convergence_achieved', False),
                'output_started': processing_info.get('output_started', output_logits.shape[1] > 0),
                
                # ë©”íƒ€ ì •ë³´
                'generation_method': 'pure_inference',
                'model_mode': 'inference'
            }
            
            return result
            
        except Exception as e:
            print(f"  ìƒ˜í”Œ {global_idx} í‰ê°€ ì‹¤íŒ¨: {e}")
            
            # í´ë°± ê²°ê³¼
            try:
                input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
                target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            except:
                input_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
                target_text = "[ë””ì½”ë”© ì‹¤íŒ¨]"
            
            return {
                'global_index': global_idx,
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': "[í‰ê°€ ì‹¤íŒ¨]",
                'accuracy': 0.0,
                'loss': None,
                'processing_clk': self.config.max_clk_training,
                'tokens_generated': 0,
                'convergence_achieved': False,
                'output_started': False,
                'generation_method': 'error',
                'model_mode': 'error'
            }

    def _calculate_sequence_accuracy(self, generated_tokens: torch.Tensor, target_tokens: torch.Tensor) -> float:
        """ì‹œí€€ìŠ¤ ì •í™•ë„ ê³„ì‚° (íŒ¨ë”© ì œì™¸)
        
        Args:
            generated_tokens: ìƒì„±ëœ í† í° ì‹œí€€ìŠ¤ [seq_len]
            target_tokens: ì •ë‹µ í† í° ì‹œí€€ìŠ¤ [seq_len]
            
        Returns:
            í† í°ë³„ ì •í™•ë„ (0.0 ~ 1.0)
        """
        try:
            # íŒ¨ë”© í† í° ì œê±°
            pad_token_id = self.config.pad_token_id
            
            # ì •ë‹µì—ì„œ ìœ íš¨í•œ í† í°ë§Œ ì¶”ì¶œ
            valid_target = target_tokens[target_tokens != pad_token_id]
            
            if len(valid_target) == 0:
                return 0.0
            
            # ìƒì„±ëœ í† í°ì„ ì •ë‹µ ê¸¸ì´ì— ë§ì¶¤
            if len(generated_tokens) >= len(valid_target):
                trimmed_generated = generated_tokens[:len(valid_target)]
            else:
                # ìƒì„±ì´ ë¶€ì¡±í•˜ë©´ íŒ¨ë”©ìœ¼ë¡œ ì±„ì›€
                padding = torch.full((len(valid_target) - len(generated_tokens),), pad_token_id, dtype=generated_tokens.dtype)
                trimmed_generated = torch.cat([generated_tokens, padding])
            
            # í† í°ë³„ ì •í™•ë„ ê³„ì‚°
            correct = (trimmed_generated == valid_target).float()
            accuracy = correct.mean().item()
            
            return accuracy
            
        except Exception as e:
            print(f"ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _decode_tokens_to_text(self, tokens: torch.Tensor) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if self.tokenizer is None:
            return f"tokens: {tokens.tolist()}"
        
        try:
            # íŒ¨ë”© í† í° ì œê±°
            if hasattr(self.tokenizer, 'tokenizer'):
                pad_token_id = self.tokenizer.tokenizer.pad_token_id
            else:
                pad_token_id = self.config.pad_token_id
                
            # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
            valid_tokens = tokens[tokens != pad_token_id]
            
            # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”©
            if hasattr(self.tokenizer, 'decode'):
                return self.tokenizer.decode(valid_tokens.tolist())
            elif hasattr(self.tokenizer, 'tokenizer'):
                return self.tokenizer.tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=True)
            else:
                return f"tokens: {valid_tokens.tolist()}"
                
        except Exception as e:
            return f"decode_error: {tokens.tolist()}"

class GradualUnfreezingScheduler:
    """ì ì§„ì  ì–¸í”„ë¦¬ì§• ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, model, unfreezing_schedule: Dict[int, List[str]]):
        self.model = model
        self.schedule = unfreezing_schedule
        
        # ì´ˆê¸°ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„° ê³ ì •
        for param in self.model.parameters():
            param.requires_grad = False
    
    def step(self, epoch: int):
        """ì—í¬í¬ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì–¸í”„ë¦¬ì§•"""
        if epoch in self.schedule:
            modules_to_unfreeze = self.schedule[epoch]
            for module_name in modules_to_unfreeze:
                module = getattr(self.model, module_name, None)
                if module:
                    for param in module.parameters():
                        param.requires_grad = True