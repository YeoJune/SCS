# src/scs/training/trainer.py (ê°„ì†Œí™”ëœ ë²„ì „)
"""
SCS ê°„ì†Œí™”ëœ í•™ìŠµ ì‹œìŠ¤í…œ - Systemì˜ ì™„ì „í•œ ì‹œí€€ìŠ¤ ì²˜ë¦¬ í™œìš©
"""

import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
from tqdm import tqdm
import logging
from pathlib import Path
from datetime import datetime

from .loss import SCSLoss
from ..evaluation.metrics import SCSMetrics
from ..config.schemas import LearningConfig

class SCSTrainer:
    """SCS ê°„ì†Œí™”ëœ í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model: torch.nn.Module,
        config: LearningConfig,
        loss_fn: Optional[SCSLoss] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        tokenizer: Optional[Any] = None,
        unfreezing_config: Optional[Dict] = None
    ):
        self.model = model
        self.config = config

        # ë¡œê¹…
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.loss_fn = loss_fn or SCSLoss(pad_token_id=config.pad_token_id)
        
        # ì ì§„ì  í•´ì œ ìŠ¤ì¼€ì¤„ëŸ¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        self.unfreezing_scheduler = None
        if unfreezing_config and unfreezing_config.get('enabled', False):
            frozen_patterns = unfreezing_config.get('initial_frozen_patterns', [])
            unfreeze_schedule = unfreezing_config.get('unfreeze_schedule', {})
            self.unfreezing_scheduler = GradualUnfreezingScheduler(
                model=self.model,
                frozen_patterns=frozen_patterns,
                unfreeze_schedule=unfreeze_schedule,
                logger=self.logger
            )
        
        # ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optimizer or torch.optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()) if self.unfreezing_scheduler else model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        self.scheduler = scheduler
        self.tokenizer = tokenizer
        
        self.device = config.device
        self.model.to(self.device)
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.best_model_path = None
        self.current_ss_prob = self.config.ss_start_prob
    
    def _update_scheduled_sampling_prob(self):
        """ìŠ¤ì¼€ì¤„ ìƒ˜í”Œë§ í™•ë¥  ì—…ë°ì´íŠ¸"""
        if not self.config.use_scheduled_sampling:
            self.current_ss_prob = 1.0
            return

        decay_epochs = self.config.ss_decay_epochs
        start_prob = self.config.ss_start_prob
        end_prob = self.config.ss_end_prob
        
        if self.current_epoch >= decay_epochs:
            self.current_ss_prob = end_prob
        else:
            self.current_ss_prob = start_prob - (start_prob - end_prob) * (self.current_epoch / decay_epochs)
        
        if hasattr(self, 'logger'):
            self.logger.info(f"Scheduled Sampling í™•ë¥  ì—…ë°ì´íŠ¸: {self.current_ss_prob:.4f}")

    def _update_curriculum_max_clk(self, epoch: int):
        """ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: max_clk ë™ì  ì¡°ì •"""
        schedule = self.config.curriculum_schedule
        sorted_schedule = sorted(schedule.items())
        
        current_max_clk = self.config.max_clk_training
        for start_epoch, max_clk in sorted_schedule:
            if epoch >= start_epoch:
                current_max_clk = max_clk
        
        if current_max_clk != self.config.max_clk_training:
            old_max_clk = self.config.max_clk_training
            self.config.max_clk_training = current_max_clk
            
            # ëª¨ë¸ì˜ max_clk ì—…ë°ì´íŠ¸
            self.model.set_max_clk(current_max_clk)
            
            # loss_fnì˜ max_clkë„ ì—…ë°ì´íŠ¸
            if hasattr(self.loss_fn, 'update_max_clk'):
                self.loss_fn.update_max_clk(current_max_clk)
            
            self.logger.info(f"ğŸ“š ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: ì—í¬í¬ {epoch}, max_clk {old_max_clk} â†’ {current_max_clk}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, List[float]]:
        """í•™ìŠµ ì‹¤í–‰"""
        
        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': []
        }
        
        self.logger.info(f"ê°„ì†Œí™”ëœ í•™ìŠµ ì‹œì‘: {self.config.epochs} ì—í¬í¬")
        
        if save_path:
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch
            
            # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
            if self.config.use_curriculum_learning and self.config.curriculum_schedule:
                self._update_curriculum_max_clk(epoch)
            
            # ì ì§„ì  í•´ì œ
            if self.unfreezing_scheduler:
                optimizer_needs_update = self.unfreezing_scheduler.step(epoch)
                if optimizer_needs_update:
                    self.logger.info("ğŸ“ ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„±")
                    self.optimizer = torch.optim.AdamW(
                        filter(lambda p: p.requires_grad, self.model.parameters()),
                        lr=self.config.learning_rate,
                        weight_decay=self.config.weight_decay
                    )
                    if self.scheduler:
                        eta_min = getattr(self.config, 'eta_min', 0.0)
                        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                            self.optimizer, 
                            T_max=self.config.epochs - epoch,
                            eta_min=eta_min
                        )

            self._update_scheduled_sampling_prob()
            
            # í•™ìŠµ
            train_metrics = self._train_epoch(train_loader)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            if self.scheduler:
                self.scheduler.step()
            
            # ê²€ì¦
            if val_loader and epoch % self.config.eval_every == 0:
                val_metrics = self._validate_epoch(val_loader)
                history['val_loss'].append(val_metrics['loss'])
                history['val_accuracy'].append(val_metrics['accuracy'])
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if save_path and val_metrics['loss'] < self.best_loss:
                    self.best_loss = val_metrics['loss']
                    self.best_model_path = self._save_best_model(save_path, epoch, val_metrics['loss'])
                    self.patience_counter = 0
                    self.logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                else:
                    self.patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ
                if self._should_early_stop(val_metrics['loss']):
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch}")
                    break
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸
            if save_path and epoch % self.config.save_every == 0:
                self._save_checkpoint(save_path, epoch)
            
            # ë¡œê¹…
            self._log_progress(epoch, train_metrics, 
                             val_metrics if val_loader and epoch % self.config.eval_every == 0 else None)
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if save_path and self.best_model_path is None:
            self.best_model_path = self._save_best_model(save_path, self.current_epoch, self.best_loss)
            self.logger.info(f"ìµœì¢… ëª¨ë¸ì„ ìµœê³  ëª¨ë¸ë¡œ ì €ì¥: {self.best_model_path}")
        
        return history
    
    def _train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ - ê°„ì†Œí™”ë¨"""
        self.model.train()
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch in progress_bar:
            batch_loss, batch_metrics = self._train_batch(batch)
            
            total_loss += batch_loss
            total_accuracy += batch_metrics['accuracy']
            num_batches += 1
            
            progress_bar.set_postfix({
                'loss': f"{batch_loss:.4f}",
                'acc': f"{batch_metrics['accuracy']:.4f}"
            })
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches
        }
    
    def _train_batch(self, batch: Dict[str, torch.Tensor]) -> tuple:
        """ë°°ì¹˜ í•™ìŠµ - ë””ë²„ê¹… ì½”ë“œê°€ ì¶”ê°€ëœ ë²„ì „"""
        
        # --- ë””ë²„ê¹… ì„¤ì • ì‹œì‘ ---
        DEBUG_GRADIENTS = True 
        grad_values = {}

        def save_grad(name):
            def hook(grad):
                if grad is not None:
                    grad_values[name] = grad.detach().clone()
                else:
                    grad_values[name] = None
            return hook
        # --- ë””ë²„ê¹… ì„¤ì • ë ---

        # ë°ì´í„° ì¤€ë¹„
        input_tokens = batch['input_tokens'].to(self.device)
        target_tokens = batch['target_tokens'].to(self.device) 
        attention_mask = batch['attention_mask'].to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self.optimizer.zero_grad()
        
        # ğŸš€ í•µì‹¬: ì‹œìŠ¤í…œì´ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬!
        result = self.model(
            input_tokens=input_tokens,
            target_tokens=target_tokens,
            attention_mask=attention_mask,
            training=True,
            scheduled_sampling_prob=self.current_ss_prob
        )
        
        # ì†ì‹¤ ê³„ì‚°
        output_logits = result['output_logits']
        processing_info = result['processing_info']
        
        if output_logits.shape[1] > 0:
            # íƒ€ê²Ÿê³¼ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶¤
            target_subset = target_tokens[:, :output_logits.shape[1]]
            
            # --- ë””ë²„ê¹…: ì†ì‹¤ í•­ ë¶„ë¦¬ ê³„ì‚° ---
            loss_fn = self.loss_fn
            base_loss = loss_fn._compute_base_loss(output_logits, target_subset, processing_info, output_logits.shape[-1])
            pruning_loss = loss_fn._compute_axon_pruning_loss(processing_info, output_logits.device)
            
            loss = base_loss + pruning_loss # + ë‹¤ë¥¸ ì†ì‹¤ë“¤...
            # --- ë””ë²„ê¹… ë ---
            
        else:
            loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            base_loss = torch.tensor(0.0)
            pruning_loss = torch.tensor(0.0)

        # --- ë””ë²„ê¹…: Hook ë“±ë¡ ---
        if DEBUG_GRADIENTS and loss.requires_grad:
            # axonal_connectionsì˜ ì²« ë²ˆì§¸ ì—°ê²°ì— ëŒ€í•œ íŒŒë¼ë¯¸í„°ì— hookì„ ë“±ë¡
            if self.model.axonal_connections.patch_gates:
                first_conn_key = next(iter(self.model.axonal_connections.patch_gates))
                
                gate_param = self.model.axonal_connections.patch_gates[first_conn_key]
                gate_param.register_hook(save_grad('patch_gates_grad'))

                transform_param = self.model.axonal_connections.patch_transforms[first_conn_key]
                transform_param.register_hook(save_grad('patch_transforms_grad'))
        # --- ë””ë²„ê¹… ë ---

        # ì—­ì „íŒŒ
        loss.backward()

        # --- ë””ë²„ê¹…: ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ì¶œë ¥ ---
        if DEBUG_GRADIENTS:
            print("\n" + "="*50)
            print("Gradient Debug Information")
            print("="*50)
            print(f"Loss -> Base: {base_loss.item():.6f}, Pruning: {pruning_loss.item():.6f}, Total: {loss.item():.6f}")

            for name, grad_tensor in grad_values.items():
                if grad_tensor is not None:
                    print(f"--- Grad for {name} ---")
                    print(f"  Shape: {grad_tensor.shape}")
                    print(f"  Mean:  {grad_tensor.mean().item():.6e}")
                    print(f"  Std:   {grad_tensor.std().item():.6e}")
                    print(f"  Max:   {grad_tensor.max().item():.6e}")
                    print(f"  Min:   {grad_tensor.min().item():.6e}")
                    print(f"  Is NaN: {torch.isnan(grad_tensor).any().item()}")
                else:
                    print(f"--- Grad for {name}: IS NONE --- <--- CRITICAL ERROR!")
            
            print("="*50 + "\n")
            
            
        if self.config.gradient_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_norm)
        self.optimizer.step()
        
        # ì •í™•ë„ ê³„ì‚°
        with torch.no_grad():
            if output_logits.shape[1] > 0:
                target_subset = target_tokens[:, :output_logits.shape[1]]
                accuracy = SCSMetrics.accuracy(output_logits, target_subset, pad_token_id=self.config.pad_token_id)
            else:
                accuracy = 0.0
                
        return loss.item(), {'accuracy': accuracy}

    def _validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """ê²€ì¦ - ê°„ì†Œí™”ë¨"""
        self.model.eval()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                # ğŸš€ ì‹œìŠ¤í…œì´ ëª¨ë“  ì¶”ë¡  ì²˜ë¦¬!
                result = self.model(
                    input_tokens=input_tokens,
                    target_tokens=target_tokens,  # ê¸¸ì´ ì°¸ì¡°ìš©
                    attention_mask=attention_mask,
                    training=False,
                    scheduled_sampling_prob=0.0  # ì™„ì „ auto-regressive
                )
                
                # ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°
                output_logits = result['output_logits']
                processing_info = result['processing_info']
                
                if output_logits.shape[1] > 0:
                    target_subset = target_tokens[:, :output_logits.shape[1]]
                    batch_loss = self.loss_fn(output_logits, target_subset, processing_info)
                    batch_accuracy = SCSMetrics.accuracy(output_logits, target_subset, pad_token_id=self.config.pad_token_id)
                else:
                    batch_loss = torch.tensor(float('inf'))
                    batch_accuracy = 0.0
                
                total_loss += batch_loss.item()
                total_accuracy += batch_accuracy
                num_batches += 1
        
        return {'loss': total_loss / num_batches, 'accuracy': total_accuracy / num_batches}
    
    def evaluate(self, test_loader: DataLoader, save_examples: int = 10) -> Dict[str, Any]:
        """í‰ê°€ - ëŒ€í­ ê°„ì†Œí™”ë¨"""
        self.model.eval()
        
        all_sample_results = []
        saved_examples = []
        total_samples = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                input_tokens = batch['input_tokens'].to(self.device)
                target_tokens = batch['target_tokens'].to(self.device)
                attention_mask = batch.get('attention_mask')
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                batch_size = input_tokens.shape[0]
                
                # ğŸš€ ì‹œìŠ¤í…œì´ ì™„ì „í•œ ì¶”ë¡  ì²˜ë¦¬!
                result = self.model(
                    input_tokens=input_tokens,
                    target_tokens=None,  # ì¶”ë¡ ì‹œì—ëŠ” None
                    attention_mask=attention_mask,
                    training=False,
                    scheduled_sampling_prob=0.0,  # ì™„ì „ auto-regressive
                    max_output_length=target_tokens.shape[1]  # íƒ€ê²Ÿ ê¸¸ì´ íŒíŠ¸
                )
                
                # ë°°ì¹˜ ê²°ê³¼ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„í•´
                for sample_idx in range(batch_size):
                    sample_result = self._extract_sample_from_result(
                        batch, result, sample_idx, total_samples
                    )
                    
                    all_sample_results.append(sample_result)
                    total_samples += 1
                    
                    if len(saved_examples) < save_examples:
                        saved_examples.append(sample_result)
        
        return self._aggregate_evaluation_results(all_sample_results, saved_examples, total_samples)

    def _extract_sample_from_result(
        self, 
        batch: Dict[str, torch.Tensor], 
        result: Dict[str, Any], 
        sample_idx: int, 
        global_idx: int
    ) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê²°ê³¼ì—ì„œ ê°œë³„ ìƒ˜í”Œ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            # í…ìŠ¤íŠ¸ ë³µì›
            input_text = self._decode_tokens_to_text(batch['input_tokens'][sample_idx])
            target_text = self._decode_tokens_to_text(batch['target_tokens'][sample_idx])
            
            # ìƒì„± ê²°ê³¼ ì¶”ì¶œ
            generated_tokens = result['generated_tokens'][sample_idx]
            generated_text = self._decode_tokens_to_text(generated_tokens) if generated_tokens.numel() > 0 else "[ë¹ˆ ì¶œë ¥]"
            
            # ì •í™•ë„ ê³„ì‚°
            output_logits = result['output_logits'][sample_idx:sample_idx+1]
            target_tokens = batch['target_tokens'][sample_idx:sample_idx+1].to(output_logits.device)
            
            if output_logits.shape[1] > 0:
                accuracy = SCSMetrics.accuracy(
                    output_logits,
                    target_tokens[:, :output_logits.shape[1]],
                    pad_token_id=self.config.pad_token_id
                )
            else:
                accuracy = 0.0
            
            processing_info = result['processing_info']
            
            return {
                'input_text': input_text,
                'target_text': target_text,
                'generated_text': generated_text,
                'accuracy': accuracy,
                'processing_clk': processing_info['processing_clk'],
                'tokens_generated': processing_info['tokens_generated'],
                'convergence_achieved': processing_info['convergence_achieved'],
                'generation_method': 'system_complete_processing',
                'global_index': global_idx,
            }
            
        except Exception as e:
            return {
                'input_text': "[ì¶”ì¶œ ì‹¤íŒ¨]",
                'target_text': "[ì¶”ì¶œ ì‹¤íŒ¨]",
                'generated_text': "[ì¶”ë¡  ì‹¤íŒ¨]",
                'accuracy': 0.0,
                'processing_clk': self.config.max_clk_training,
                'tokens_generated': 0,
                'convergence_achieved': False,
                'generation_method': 'error',
                'global_index': global_idx,
                'error': str(e)
            }
    
    def _aggregate_evaluation_results(
        self, 
        all_results: List[Dict[str, Any]], 
        saved_examples: List[Dict[str, Any]], 
        total_samples: int
    ) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ì§‘ê³„"""
        print(f"\n=== ì „ì²´ {total_samples}ê°œ ìƒ˜í”Œ ê²°ê³¼ ì§‘ê³„ ===")
        
        total_accuracy = sum(result['accuracy'] for result in all_results) / len(all_results)
        convergence_rate = sum(result['convergence_achieved'] for result in all_results) / len(all_results)
        avg_processing_clk = sum(result['processing_clk'] for result in all_results if isinstance(result['processing_clk'], (int, float))) / len(all_results)
        avg_tokens_generated = sum(result['tokens_generated'] for result in all_results if isinstance(result['tokens_generated'], (int, float))) / len(all_results)
        processing_efficiency = max(0.0, 1.0 - (avg_processing_clk / self.config.max_clk_training))
        comprehensive_score = (
            0.4 * convergence_rate +
            0.3 * processing_efficiency +
            0.2 * min(1.0, (avg_tokens_generated / 10.0)) +
            0.1 * total_accuracy
        )
        
        results = {
            'test_accuracy': total_accuracy,
            'comprehensive_score': comprehensive_score,
            'convergence_rate': convergence_rate,
            'processing_efficiency': processing_efficiency,
            'examples': saved_examples,
            'num_examples_saved': len(saved_examples),
            'total_samples_evaluated': total_samples
        }
        
        print(f"ìµœì¢… ê²°ê³¼: ì •í™•ë„={total_accuracy:.4f}, ì¢…í•©ì ìˆ˜={comprehensive_score:.4f}")
        return results
    
    def _should_early_stop(self, val_loss: float) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ íŒë‹¨"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.config.early_stopping_patience
    
    def _save_best_model(self, save_path: str, epoch: int, loss: float) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_model_path = f"{save_path}/best_model.pt"
        
        config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
            'eta_min': self.config.eta_min,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss,
            'config_dict': config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
        torch.save(checkpoint, best_model_path)
        return best_model_path

    def _save_checkpoint(self, save_path: str, epoch: int):
        """ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        config_dict = {
            'epochs': self.config.epochs,
            'learning_rate': self.config.learning_rate,
            'weight_decay': self.config.weight_decay,
            'gradient_clip_norm': self.config.gradient_clip_norm,
            'eval_every': self.config.eval_every,
            'save_every': self.config.save_every,
            'early_stopping_patience': self.config.early_stopping_patience,
            'device': self.config.device,
            'max_clk_training': self.config.max_clk_training,
            'pad_token_id': self.config.pad_token_id,
            'use_scheduled_sampling': self.config.use_scheduled_sampling,
            'ss_start_prob': self.config.ss_start_prob,
            'ss_end_prob': self.config.ss_end_prob,
            'ss_decay_epochs': self.config.ss_decay_epochs,
            'eta_min': self.config.eta_min,
        }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'config_dict': config_dict,
            'model_config': getattr(self.model, 'config', None),
            'tokenizer_vocab_size': getattr(self.tokenizer, 'vocab_size', None) if self.tokenizer else None,
            'save_timestamp': datetime.now().isoformat(),
            'current_ss_prob': getattr(self, 'current_ss_prob', None)
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, f"{save_path}/checkpoint_epoch_{epoch}.pt")
    
    def _log_progress(self, epoch: int, train_metrics: Dict[str, float], val_metrics: Optional[Dict[str, float]] = None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        log_msg = f"ì—í¬í¬ {epoch}: í›ˆë ¨ ì†ì‹¤={train_metrics['loss']:.4f}, í›ˆë ¨ ì •í™•ë„={train_metrics['accuracy']:.4f}"
        
        if val_metrics:
            log_msg += f", ê²€ì¦ ì†ì‹¤={val_metrics['loss']:.4f}, ê²€ì¦ ì •í™•ë„={val_metrics['accuracy']:.4f}"
            if val_metrics['loss'] < self.best_loss:
                log_msg += " â­"
        
        self.logger.info(log_msg)

    def _decode_tokens_to_text(self, tokens: torch.Tensor) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if self.tokenizer is None:
            return f"tokens: {tokens.tolist()}"
        
        try:
            # íŒ¨ë”© í† í° ì œê±°
            if hasattr(self.tokenizer, 'tokenizer'):
                pad_token_id = self.tokenizer.tokenizer.pad_token_id
            else:
                pad_token_id = self.config.pad_token_id
                
            # íŒ¨ë”©ì´ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
            valid_tokens = tokens[tokens != pad_token_id]
            
            # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”©
            if hasattr(self.tokenizer, 'decode'):
                return self.tokenizer.decode(valid_tokens.tolist())
            elif hasattr(self.tokenizer, 'tokenizer'):
                return self.tokenizer.tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=True)
            else:
                return f"tokens: {valid_tokens.tolist()}"
                
        except Exception as e:
            return f"decode_error: {tokens.tolist()}"


class GradualUnfreezingScheduler:
    """ì ì§„ì  ì–¸í”„ë¦¬ì§• ìŠ¤ì¼€ì¤„ëŸ¬ - ë™ê²° íŒ¨í„´ ê¸°ë°˜"""
    
    def __init__(self, model, frozen_patterns: List[str], unfreeze_schedule: Dict[int, List[str]], logger=None):
        """
        Args:
            model: SCS ëª¨ë¸
            frozen_patterns: ì´ˆê¸°ì— ë™ê²°í•  íŒŒë¼ë¯¸í„° íŒ¨í„´ë“¤
            unfreeze_schedule: {epoch: [patterns]} í˜•íƒœì˜ í•´ì œ ìŠ¤ì¼€ì¤„
            logger: ë¡œê¹… ê°ì²´
        """
        self.model = model
        self.frozen_patterns = frozen_patterns
        self.unfreeze_schedule = unfreeze_schedule
        self.logger = logger
        self.current_epoch = -1
        self.unfrozen_patterns = set()  # ì´ë¯¸ í•´ì œëœ íŒ¨í„´ë“¤ ì¶”ì 
        
        # ì§€ì •ëœ íŒ¨í„´ë§Œ ë™ê²°
        if self.frozen_patterns:
            self._freeze_by_patterns(self.frozen_patterns)
            
        if self.logger:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            self.logger.info(f"ğŸ”’ ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒíƒœ: {trainable_params:,}/{total_params:,} í•™ìŠµ ê°€ëŠ¥")
    
    def _freeze_by_patterns(self, patterns: List[str]):
        """íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ ë™ê²°"""
        frozen_count = 0
        for name, param in self.model.named_parameters():
            for pattern in patterns:
                if name.startswith(pattern):
                    param.requires_grad = False
                    frozen_count += param.numel()
                    if self.logger:
                        self.logger.info(f"ğŸ”’ ë™ê²°: {name} ({param.numel():,} íŒŒë¼ë¯¸í„°)")
                    break
        
        if self.logger and frozen_count > 0:
            self.logger.info(f"ì´ {frozen_count:,}ê°œ íŒŒë¼ë¯¸í„° ë™ê²° ì™„ë£Œ")
    
    def _unfreeze_by_patterns(self, patterns: List[str]) -> bool:
        """íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ í•´ì œ"""
        newly_unfrozen = False
        unfrozen_count = 0
        
        for pattern in patterns:
            if pattern in self.unfrozen_patterns:
                continue  # ì´ë¯¸ í•´ì œëœ íŒ¨í„´ì€ ìŠ¤í‚µ
                
            for name, param in self.model.named_parameters():
                if name.startswith(pattern) and not param.requires_grad:
                    param.requires_grad = True
                    unfrozen_count += param.numel()
                    newly_unfrozen = True
                    if self.logger:
                        self.logger.info(f"ğŸ”“ í•´ì œ: {name} ({param.numel():,} íŒŒë¼ë¯¸í„°)")
            
            self.unfrozen_patterns.add(pattern)
        
        if self.logger and unfrozen_count > 0:
            self.logger.info(f"ì´ {unfrozen_count:,}ê°œ íŒŒë¼ë¯¸í„° í•´ì œ ì™„ë£Œ")
        
        return newly_unfrozen
    
    def step(self, epoch: int) -> bool:
        """
        ì—í¬í¬ ì§„í–‰ ì‹œ í˜¸ì¶œ. ìƒˆë¡œìš´ íŒ¨í„´ì´ í•´ì œë˜ë©´ True ë°˜í™˜
        
        Args:
            epoch: í˜„ì¬ ì—í¬í¬
            
        Returns:
            bool: ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„±ì´ í•„ìš”í•œì§€ ì—¬ë¶€
        """
        if epoch == self.current_epoch:
            return False
            
        self.current_epoch = epoch
        
        if epoch in self.unfreeze_schedule:
            if self.logger:
                self.logger.info(f"ğŸ“… ì—í¬í¬ {epoch}: ì ì§„ì  í•´ì œ ì‹¤í–‰")
            
            patterns_to_unfreeze = self.unfreeze_schedule[epoch]
            newly_unfrozen = self._unfreeze_by_patterns(patterns_to_unfreeze)
            
            if newly_unfrozen:
                # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í†µê³„ ì¶œë ¥
                total_params = sum(p.numel() for p in self.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                
                if self.logger:
                    self.logger.info(f"ğŸ“Š í˜„ì¬ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}/{total_params:,} "
                                   f"({100*trainable_params/total_params:.1f}%)")
                
                return True  # ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± í•„ìš”
        
        return False
    