# src/scs/utils/tensorboard_logger.py
"""
SCS TensorBoard ë¡œê±°

SCS ì‹œìŠ¤í…œ ì „ìš© TensorBoard ë¡œê¹… ê¸°ëŠ¥ ì œê³µ
"""

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œë„ ì‘ë™
from typing import Dict, Any, Optional, List
from pathlib import Path
import subprocess
import time
import webbrowser
import warnings

class SCSTensorBoardLogger:
    """SCS ì „ìš© TensorBoard ë¡œê±°"""
    
    def __init__(self, log_dir: Path, config: Optional[Dict[str, Any]] = None):
        """
        TensorBoard ë¡œê±° ì´ˆê¸°í™”
        
        Args:
            log_dir: TensorBoard ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            config: TensorBoard ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì • ì´ˆê¸°í™”
        self.config = config or {}
        self.log_interval = self.config.get('log_interval', {
            'scalars': 1,
            'histograms': 100,
            'images': 500,
            'spikes': 50
        })
        self.max_images_per_batch = self.config.get('max_images_per_batch', 4)
        self.histogram_freq = self.config.get('histogram_freq', 100)
        
        # TensorBoard Writer ì´ˆê¸°í™”
        self.writer = SummaryWriter(self.log_dir)
        
        # ì¹´ìš´í„°ë“¤
        self.global_step = 0
        self.epoch = 0
        self.batch_counter = 0
        self.clk_counter = 0
        
        # TensorBoard ì„œë²„ í”„ë¡œì„¸ìŠ¤
        self.tb_process = None
        
        # ìë™ ì‹¤í–‰
        if self.config.get('auto_launch', False):
            self.launch_tensorboard(self.config.get('port', 6006))
    
    def set_epoch(self, epoch: int):
        """í˜„ì¬ ì—í¬í¬ ì„¤ì •"""
        self.epoch = epoch
    
    def should_log(self, log_type: str) -> bool:
        """ë¡œê¹… ì—¬ë¶€ ê²°ì •"""
        if log_type == "scalars":
            return self.batch_counter % self.log_interval.get("scalars", 1) == 0
        elif log_type == "histograms":
            return self.batch_counter % self.log_interval.get("histograms", 100) == 0
        elif log_type == "images":
            return self.batch_counter % self.log_interval.get("images", 500) == 0
        elif log_type == "spikes":
            return self.clk_counter % self.log_interval.get("spikes", 50) == 0
        return False
    
    def log_training_step(self, metrics: Dict[str, Any], loss: float):
        """í•™ìŠµ ìŠ¤í… ë¡œê¹…"""
        if self.should_log("scalars"):
            # ê¸°ë³¸ ì†ì‹¤
            self.writer.add_scalar("Training/Loss", loss, self.global_step)
            
            # ê¸°íƒ€ ë©”íŠ¸ë¦­
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    self.writer.add_scalar(f"Training/{key}", value, self.global_step)
                elif isinstance(value, torch.Tensor) and value.numel() == 1:
                    self.writer.add_scalar(f"Training/{key}", value.item(), self.global_step)
        
        self.global_step += 1
        self.batch_counter += 1
    
    def log_validation_step(self, metrics: Dict[str, Any]):
        """ê²€ì¦ ìŠ¤í… ë¡œê¹…"""
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                self.writer.add_scalar(f"Validation/{key}", value, self.epoch)
            elif isinstance(value, torch.Tensor) and value.numel() == 1:
                self.writer.add_scalar(f"Validation/{key}", value.item(), self.epoch)
    
    def log_model_weights(self, model: nn.Module, suffix: str = ""):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ íˆìŠ¤í† ê·¸ë¨ ë¡œê¹…"""
        if not self.should_log("histograms"):
            return
        
        for name, param in model.named_parameters():
            if param.requires_grad and param.numel() > 0:
                self.writer.add_histogram(f"Weights{suffix}/{name}", param.detach().cpu(), self.epoch)
                
                if param.grad is not None:
                    self.writer.add_histogram(f"Gradients{suffix}/{name}", param.grad.detach().cpu(), self.epoch)
    
    def log_spike_patterns(self, spike_patterns: Dict[str, torch.Tensor], clk: int):
        """ìŠ¤íŒŒì´í¬ íŒ¨í„´ ì´ë¯¸ì§€ ë¡œê¹…"""
        if not self.should_log("spikes"):
            return
        
        # ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ
        node_names = list(spike_patterns.keys())[:self.max_images_per_batch]
        
        for node_name in node_names:
            spikes = spike_patterns[node_name]
            if spikes is None or spikes.numel() == 0:
                continue
            
            try:
                # [B, H, W] -> [H, W] (ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ)
                if spikes.dim() == 3:
                    spike_img = spikes[0].detach().cpu().numpy()
                elif spikes.dim() == 2:
                    spike_img = spikes.detach().cpu().numpy()
                else:
                    continue
                
                # ì´ë¯¸ì§€ ì •ê·œí™” (0-1 ë²”ìœ„)
                if spike_img.max() > spike_img.min():
                    spike_img = (spike_img - spike_img.min()) / (spike_img.max() - spike_img.min())
                else:
                    spike_img = np.zeros_like(spike_img)
                
                # TensorBoardì— ì´ë¯¸ì§€ ì¶”ê°€
                self.writer.add_image(
                    f"Spikes/{node_name}",
                    spike_img,
                    global_step=clk,
                    dataformats='HW'
                )
            except Exception as e:
                warnings.warn(f"ìŠ¤íŒŒì´í¬ íŒ¨í„´ ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ({node_name}): {e}")
        
        self.clk_counter += 1
    
    def log_processing_info(self, processing_info: Dict[str, Any]):
        """ì²˜ë¦¬ ì •ë³´ ë¡œê¹…"""
        # ì²˜ë¦¬ CLK ìˆ˜
        if 'processing_clk' in processing_info:
            self.writer.add_scalar("Processing/CLK_Count", processing_info['processing_clk'], self.epoch)
        
        # ìˆ˜ë ´ ì—¬ë¶€
        if 'convergence_achieved' in processing_info:
            convergence = 1.0 if processing_info['convergence_achieved'] else 0.0
            self.writer.add_scalar("Processing/Convergence", convergence, self.epoch)
        
        # ìƒì„±ëœ í† í° ìˆ˜
        if 'tokens_generated' in processing_info:
            self.writer.add_scalar("Processing/Tokens_Generated", processing_info['tokens_generated'], self.epoch)
        
        # ACC í™œë™ë„
        if 'final_acc_activity' in processing_info:
            self.writer.add_scalar("Processing/ACC_Activity", processing_info['final_acc_activity'], self.epoch)
        
        # ë°°ì¹˜ í¬ê¸°
        if 'batch_size' in processing_info:
            self.writer.add_scalar("Processing/Batch_Size", processing_info['batch_size'], self.epoch)
    
    def log_loss_components(self, loss_components: Dict[str, float]):
        """ì†ì‹¤ êµ¬ì„±ìš”ì†Œ ë¡œê¹…"""
        for component, value in loss_components.items():
            if isinstance(value, (int, float)):
                self.writer.add_scalar(f"Loss_Components/{component}", value, self.global_step)
            elif isinstance(value, torch.Tensor) and value.numel() == 1:
                self.writer.add_scalar(f"Loss_Components/{component}", value.item(), self.global_step)
    
    def log_hyperparameters(self, config_dict: Dict[str, Any], metrics: Dict[str, float]):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        # í‰ë©´í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        hparams = self._flatten_config(config_dict)
        
        # ìŠ¤ì¹¼ë¼ ê°’ë§Œ í•„í„°ë§
        filtered_hparams = {}
        for key, value in hparams.items():
            if isinstance(value, (int, float, str, bool)):
                # ë¬¸ìì—´ ê¸¸ì´ ì œí•œ (TensorBoard ì œí•œ)
                if isinstance(value, str) and len(value) > 100:
                    value = value[:97] + "..."
                filtered_hparams[key] = value
        
        # ë©”íŠ¸ë¦­ë„ ìŠ¤ì¹¼ë¼ ê°’ë§Œ í•„í„°ë§
        filtered_metrics = {}
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                filtered_metrics[key] = value
            elif isinstance(value, torch.Tensor) and value.numel() == 1:
                filtered_metrics[key] = value.item()
        
        if filtered_hparams and filtered_metrics:
            try:
                self.writer.add_hparams(filtered_hparams, filtered_metrics)
            except Exception as e:
                warnings.warn(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
    
    def log_learning_rate(self, lr: float):
        """í•™ìŠµë¥  ë¡œê¹…"""
        self.writer.add_scalar("Training/Learning_Rate", lr, self.global_step)
    
    def log_custom_scalar(self, tag: str, value: float, step: Optional[int] = None):
        """ì»¤ìŠ¤í…€ ìŠ¤ì¹¼ë¼ ë¡œê¹…"""
        step = step if step is not None else self.global_step
        self.writer.add_scalar(tag, value, step)
    
    def log_custom_histogram(self, tag: str, values: torch.Tensor, step: Optional[int] = None):
        """ì»¤ìŠ¤í…€ íˆìŠ¤í† ê·¸ë¨ ë¡œê¹…"""
        step = step if step is not None else self.epoch
        if values.numel() > 0:
            self.writer.add_histogram(tag, values.detach().cpu(), step)
    
    def _flatten_config(self, config: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
        """ì¤‘ì²©ëœ ì„¤ì •ì„ í‰ë©´í™”"""
        flattened = {}
        
        for key, value in config.items():
            full_key = f"{prefix}/{key}" if prefix else key
            
            if isinstance(value, dict):
                flattened.update(self._flatten_config(value, full_key))
            elif isinstance(value, (int, float, str, bool)):
                flattened[full_key] = value
            elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], (int, float)):
                # ìˆ«ì ë¦¬ìŠ¤íŠ¸ëŠ” í‰ê· ê°’ìœ¼ë¡œ ë³€í™˜
                flattened[f"{full_key}_mean"] = sum(value) / len(value)
        
        return flattened
    
    def launch_tensorboard(self, port: int = 6006, auto_open: bool = True) -> bool:
        """TensorBoard ì„œë²„ ì‹œì‘"""
        try:
            cmd = [
                "tensorboard", 
                "--logdir", str(self.log_dir.parent), 
                "--port", str(port), 
                "--host", "0.0.0.0"
            ]
            
            self.tb_process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
            time.sleep(3)
            
            # ë¸Œë¼ìš°ì € ìë™ ì—´ê¸°
            if auto_open:
                try:
                    webbrowser.open(f"http://localhost:{port}")
                except Exception:
                    pass  # ë¸Œë¼ìš°ì € ì—´ê¸° ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
            
            print(f"ğŸ“Š TensorBoard ì„œë²„ ì‹œì‘ë¨: http://localhost:{port}")
            return True
            
        except FileNotFoundError:
            print("âš ï¸ TensorBoardê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install tensorboard'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return False
        except Exception as e:
            print(f"âš ï¸ TensorBoard ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def close(self):
        """ë¡œê±° ë° TensorBoard ì„œë²„ ì¢…ë£Œ"""
        if self.writer:
            self.writer.close()
        
        if self.tb_process:
            try:
                self.tb_process.terminate()
                self.tb_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.tb_process.kill()
            except Exception:
                pass  # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
    
    def __enter__(self):
        """Context manager ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        self.close()